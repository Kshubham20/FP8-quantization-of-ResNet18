{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbNOGx1lYCL1"
      },
      "source": [
        "# **Module imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi6OygwNP0uF",
        "outputId": "8c2591b1-3bfe-4262-c39f-07c2c5f42544"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7faaa41c7870>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing necessary modules\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import argparse\n",
        "from scipy.linalg import hadamard\n",
        "from torch.autograd import Function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        " \n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.init as init\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "# Set up warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    action='ignore',\n",
        "    category=DeprecationWarning,\n",
        "    module=r'.*'\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    action='default',\n",
        "    module=r'torch.ao.quantization'\n",
        ")\n",
        "\n",
        "# Specify random seed for repeatable results\n",
        "torch.manual_seed(191009)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CUDA_VISIBLE_DEVICE = 3\n",
        "if torch.cuda.is_available(): \n",
        " dev = \"cuda:0\" \n",
        "else: \n",
        " dev = \"cpu\" \n",
        "device = torch.device(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkONLvH1Uw-D"
      },
      "source": [
        "# **Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eWfkzfBxU2U0"
      },
      "outputs": [],
      "source": [
        "def ZerO_Init_on_matrix(matrix_tensor):\n",
        "    # Algorithm 1 in the paper.\n",
        "    \n",
        "    m = matrix_tensor.size(0)\n",
        "    n = matrix_tensor.size(1)\n",
        "    \n",
        "    if m <= n:\n",
        "        init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
        "    elif m > n:\n",
        "        clog_m = math.ceil(math.log2(m))\n",
        "        p = 2**(clog_m)\n",
        "        init_matrix = torch.nn.init.eye_(torch.empty(m, p)) @ (torch.tensor(hadamard(p)).float()/(2**(clog_m/2))) @ torch.nn.init.eye_(torch.empty(p, n))\n",
        "    \n",
        "    return init_matrix\n",
        "\n",
        "def Identity_Init_on_matrix(matrix_tensor):\n",
        "    # Definition 1 in the paper\n",
        "    # See https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.eye_ for details. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible, the same as partial identity matrix.\n",
        "    \n",
        "    m = matrix_tensor.size(0)\n",
        "    n = matrix_tensor.size(1)\n",
        "    \n",
        "    init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
        "    \n",
        "    return init_matrix\n",
        "\n",
        "\n",
        "def init_sub_identity_conv1x1(weight):\n",
        "    tensor = weight.data\n",
        "    out_dim = tensor.size()[0]\n",
        "    in_dim = tensor.size()[1]\n",
        "    ori_dim = tensor.size()\n",
        "    assert tensor.size()[2] == 1 and tensor.size()[3] == 1\n",
        "    if out_dim<in_dim:\n",
        "        i = torch.eye(out_dim).type_as(tensor)\n",
        "        j = torch.zeros(out_dim,(in_dim-out_dim)).type_as(tensor)\n",
        "        k = torch.cat((i,j),1)\n",
        "    elif out_dim>in_dim:\n",
        "        i = torch.eye(in_dim).type_as(tensor)\n",
        "        j = torch.zeros((out_dim-in_dim),in_dim).type_as(tensor)\n",
        "        k = torch.cat((i,j),0)\n",
        "    else:\n",
        "        k = torch.eye(out_dim).type_as(tensor)\n",
        "    k.unsqueeze_(2)\n",
        "    k.unsqueeze_(3)\n",
        "    assert k.size() == ori_dim\n",
        "    \n",
        "    weight.data = k\n",
        "\n",
        "class Hadamard_Transform(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(Hadamard_Transform, self).__init__()\n",
        "        if dim_in != dim_out:\n",
        "            raise RuntimeError('orthogonal transform not supports dim_in != dim_out currently')\n",
        "        hadamard_matrix = hadamard(dim_in)\n",
        "        hadamard_matrix = torch.Tensor(hadamard_matrix)\n",
        "\n",
        "        n = int(np.log2(dim_in))\n",
        "        normalized_hadamard_matrix = hadamard_matrix / (2**(n / 2))\n",
        "\n",
        "        self.hadamard_matrix = nn.Parameter(normalized_hadamard_matrix, requires_grad=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input is a B x C x N x M\n",
        "        \n",
        "        return torch.matmul(x.permute(0,2,3,1), self.hadamard_matrix).permute(0,3,1,2)\n",
        "class SkipConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, scale=1):\n",
        "        super(SkipConnection, self).__init__()\n",
        "        self.scale = scale\n",
        "    def _shortcut(self, input):\n",
        "        #needs to be implemented\n",
        "\n",
        "        return input\n",
        "\n",
        "    def forward(self, x):\n",
        "        # with torch.no_grad():\n",
        "        identity = self._shortcut(x)\n",
        "        return identity * self.scale\n",
        "\n",
        "class ChannelPaddingSkip(SkipConnection):\n",
        "\n",
        "    def __init__(self, num_expand_channels_left, num_expand_channels_right, scale=1):\n",
        "        super(ChannelPaddingSkip, self).__init__(scale)\n",
        "        self.num_expand_channels_left = num_expand_channels_left\n",
        "        self.num_expand_channels_right = num_expand_channels_right\n",
        "    \n",
        "    def _shortcut(self, input):\n",
        "        # input is (N, C, H, M)\n",
        "        # and return is (N, C + num_left + num_right, H, M)\n",
        "        \n",
        "        return F.pad(input, (0, 0, 0, 0, self.num_expand_channels_left, self.num_expand_channels_right) , \"constant\", 0) \n",
        "class Zero_Relu(Function):\n",
        "        \n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        output = input.clamp(min=0)\n",
        "        return output    \n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input < 0] = 0\n",
        "        return grad_input\n",
        "    \n",
        "zero_relu = Zero_Relu.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRN2yf_UZMDq"
      },
      "source": [
        "# **Suplementary Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bVyGLlRCZUUD"
      },
      "outputs": [],
      "source": [
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "# def init_params(net):\n",
        "#     '''Init layer parameters.'''\n",
        "#     for m in net.modules():\n",
        "#         if isinstance(m, nn.Conv2d):\n",
        "#             init.kaiming_normal(m.weight, mode='fan_out')\n",
        "#             if m.bias:\n",
        "#                 init.constant(m.bias, 0)\n",
        "#         elif isinstance(m, nn.BatchNorm2d):\n",
        "#             init.constant(m.weight, 1)\n",
        "#             init.constant(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Linear):\n",
        "#             init.normal(m.weight, std=1e-3)\n",
        "#             if m.bias:\n",
        "#                 init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "# term_width = int(term_width)\n",
        "term_width=80\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeEA5kk8ZRj6"
      },
      "source": [
        "# **Cifar10 dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4cU9zHpaBgB",
        "outputId": "319d34cf-bebb-4d72-a118-58294b70ab0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IeyW1bfaWvL"
      },
      "source": [
        "# **Train/Test function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ejOVdtAJ7rka"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training\n",
        "def train(epoch,model): \n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print('Train Loss:',(train_loss/(batch_idx+1)), 'Acc: ',100.*correct/total,'correct',correct,'total',total)\n",
        "\n",
        "\n",
        "def test(epoch,model):\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        print('Test Loss:',(test_loss/(batch_idx+1)),'Acc:',100.*correct/total,'Correct',correct,'total',total)\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        # print('Saving..')\n",
        "        # state = {\n",
        "        #     'net': net.state_dict(),\n",
        "        #     'acc': acc,\n",
        "        #     'epoch': epoch,\n",
        "        # }\n",
        "        # if not os.path.isdir('checkpoint'):\n",
        "        #     os.mkdir('checkpoint')\n",
        "        # torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "    print(best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97MjwuJqY3QP"
      },
      "source": [
        "# **Resnet18 model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3uwZMTmRQRL"
      },
      "source": [
        "# PreAct Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZNYb2rq0dQKn"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.relu = zero_relu\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                Hadamard_Transform(self.expansion*planes,self.expansion*planes))\n",
        "            self.shortcut[0].type_name = 'conv1x1'\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(self.relu(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out += shortcut\n",
        "\n",
        "        identity = out\n",
        "        out = self.conv2(self.bn2(self.relu(out)))\n",
        "        out += identity\n",
        "\n",
        "        return out\n",
        "\n",
        "class PreActBlockNonBN(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlockNonBN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.relu = zero_relu\n",
        "        self.bias1a = nn.Parameter(torch.zeros(1))\n",
        "        self.bias1b = nn.Parameter(torch.zeros(1))\n",
        "        self.bias2a = nn.Parameter(torch.zeros(1))\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "        self.bias2b = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                Hadamard_Transform(self.expansion*planes,self.expansion*planes))\n",
        "            self.shortcut[0].type_name = 'conv1x1'\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(x)\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out + self.bias1a)\n",
        "        out += shortcut + self.bias1b\n",
        "\n",
        "        identity = out\n",
        "        out = self.conv2(self.relu(out) + self.bias2a)\n",
        "        out = out * self.scale + self.bias2b\n",
        "        out += identity\n",
        "\n",
        "        return out\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, init='ZerO', BN_enable=True):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.num_layers = num_blocks[0]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.BN_enable = BN_enable\n",
        "        if self.BN_enable:\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.bn2 = nn.BatchNorm2d(512*block.expansion)\n",
        "        else:\n",
        "            self.bias1 = nn.Parameter(torch.zeros(1))\n",
        "            self.bias2 = nn.Parameter(torch.zeros(1))\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        self.first_transform = nn.Sequential(ChannelPaddingSkip(0,61, scale=1), Hadamard_Transform(64,64))\n",
        "        \n",
        "        self.relu = zero_relu\n",
        "        \n",
        "        if init == 'ZerO':\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.constant_(m.weight, 0)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "                if isinstance(m, PreActBlock) or isinstance(m, PreActBlockNonBN):\n",
        "                    # initialize every conv layer as zero\n",
        "                    nn.init.constant_(m.conv1.weight, 0)\n",
        "                    nn.init.constant_(m.conv2.weight, 0)\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    # initialize first conv layer as zero\n",
        "                    if hasattr(m,'type_name'):\n",
        "                        if 'conv1x1' in m.type_name:\n",
        "                            # nn.init.constant_(m.weight, 0)\n",
        "                            init_sub_identity_conv1x1(m.weight)\n",
        "                            print('sub identity init a conv1x1')\n",
        "                    else:\n",
        "                        nn.init.constant_(m.weight, 0)\n",
        "        elif init == 'Kaiming':\n",
        "            #initialize in a standard way\n",
        "            pass \n",
        "        elif init == 'Xavier':\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.xavier_normal_(m.weight)\n",
        "                if isinstance(m, PreActBlock) or isinstance(m, PreActBlockNonBN):\n",
        "                    # initialize every conv layer as zero\n",
        "                    nn.init.xavier_normal_(m.conv1.weight)\n",
        "                    nn.init.xavier_normal_(m.conv2.weight)\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    # initialize first conv layer as zero\n",
        "                    if hasattr(m,'type_name'):\n",
        "                        if 'conv1x1' in m.type_name:\n",
        "                            # nn.init.constant_(m.weight, 0)\n",
        "                            nn.init.xavier_normal_(m.weight)\n",
        "                    else:\n",
        "                        nn.init.xavier_normal_(m.weight)\n",
        "        elif init == 'Fixup':\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, PreActBlock) or isinstance(m, PreActBlockNonBN):\n",
        "                    nn.init.normal_(m.conv1.weight, mean=0, std=np.sqrt(2 / (m.conv1.weight.shape[0] * np.prod(m.conv1.weight.shape[2:]))) * self.num_layers ** (-0.5))\n",
        "                    nn.init.constant_(m.conv2.weight, 0)\n",
        "                elif isinstance(m, nn.Linear):\n",
        "                    nn.init.constant_(m.weight, 0)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "                elif isinstance(m, nn.Conv2d):\n",
        "                    # initialize first conv layer as zero\n",
        "                    if hasattr(m,'type_name'):\n",
        "                        nn.init.constant_(m.weight, 0)\n",
        "\n",
        "        # check initialization status\n",
        "        for name, param in self.named_parameters():\n",
        "            unique_values = torch.unique(param.data)\n",
        "            if len(unique_values) > 2 and 'downsample' not in name and 'ortho_transform' not in name:\n",
        "                print('!!!!!!!!!!!!!!!!following is not initialized as zero or one!!!!!!!!!!!!!!!!')\n",
        "                print(name)\n",
        "                print(unique_values)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x) + self.first_transform(x)\n",
        "        out = self.relu(out)\n",
        "        if self.BN_enable:\n",
        "            out = self.bn1(out)\n",
        "        else:\n",
        "            # simply replace the BN in the same position, may not be the optimal choice\n",
        "            out = out + self.bias1\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.relu(out)\n",
        "        if self.BN_enable:\n",
        "            out = self.bn2(out)\n",
        "        else:\n",
        "            out = out + self.bias2\n",
        "\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def identity_resnet18():\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2])\n",
        "\n",
        "def identity_resnet18_non_bn():\n",
        "    return PreActResNet(PreActBlockNonBN, [2,2,2,2], BN_enable=False)\n",
        "\n",
        "def identity_resnet18_non_bn_kaiming():\n",
        "    return PreActResNet(PreActBlockNonBN, [2,2,2,2], BN_enable=False, init='Kaiming')\n",
        "\n",
        "def identity_resnet18_non_bn_xavier():\n",
        "    return PreActResNet(PreActBlockNonBN, [2,2,2,2], BN_enable=False, init='Xavier')\n",
        "\n",
        "def identity_resnet18_non_bn_fixup():\n",
        "    return PreActResNet(PreActBlockNonBN, [2,2,2,2], BN_enable=False, init='Fixup')\n",
        "\n",
        "def identity_resnet18_kaiming():\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], init='Kaiming')\n",
        "\n",
        "def identity_resnet18_xavier():\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], init='Xavier')\n",
        "\n",
        "def identity_resnet18_fixup():\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], init='Fixup')\n",
        "\n",
        "def identity_resnet34():\n",
        "    return PreActResNet(PreActBlock, [4,4,4,4])\n",
        "\n",
        "def identity_resnet66():\n",
        "    return PreActResNet(PreActBlock, [8,8,8,8])\n",
        "\n",
        "def identity_resnet66_non_bn():\n",
        "    return PreActResNet(PreActBlockNonBN, [8,8,8,8], BN_enable=False)\n",
        "\n",
        "def identity_resnet66_non_bn_kaiming():\n",
        "    return PreActResNet(PreActBlockNonBN, [8,8,8,8], BN_enable=False, init='Kaiming')\n",
        "\n",
        "def identity_resnet66_non_bn_xavier():\n",
        "    return PreActResNet(PreActBlockNonBN, [8,8,8,8], BN_enable=False, init='Xavier')\n",
        "\n",
        "def identity_resnet66_non_bn_fixup():\n",
        "    return PreActResNet(PreActBlockNonBN, [8,8,8,8], BN_enable=False, init='Fixup')\n",
        "\n",
        "def identity_resnet126():\n",
        "    return PreActResNet(PreActBlock, [16,16,16,16])\n",
        "\n",
        "def identity_resnet126_non_bn():\n",
        "    return PreActResNet(PreActBlockNonBN, [16,16,16,16], BN_enable=False)\n",
        "\n",
        "def identity_resnet126_non_bn_kaiming():\n",
        "    return PreActResNet(PreActBlockNonBN, [16,16,16,16], BN_enable=False, init='Kaiming')\n",
        "\n",
        "def identity_resnet126_non_bn_xavier():\n",
        "    return PreActResNet(PreActBlockNonBN, [16,16,16,16], BN_enable=False, init='Xavier')\n",
        "\n",
        "def identity_resnet126_non_bn_fixup():\n",
        "    return PreActResNet(PreActBlockNonBN, [16,16,16,16], BN_enable=False, init='Fixup')\n",
        "\n",
        "def identity_resnet250():\n",
        "    return PreActResNet(PreActBlock, [32,32,32,32])\n",
        "\n",
        "def identity_resnet250_non_bn():\n",
        "    return PreActResNet(PreActBlockNonBN, [32,32,32,32], BN_enable=False)\n",
        "\n",
        "def identity_resnet250_non_bn_kaiming():\n",
        "    return PreActResNet(PreActBlockNonBN, [32,32,32,32], BN_enable=False, init='Kaiming')\n",
        "\n",
        "def identity_resnet250_non_bn_xavier():\n",
        "    return PreActResNet(PreActBlockNonBN, [32,32,32,32], BN_enable=False, init='Xavier')\n",
        "\n",
        "def identity_resnet250_non_bn_fixup():\n",
        "    return PreActResNet(PreActBlockNonBN, [32,32,32,32], BN_enable=False, init='Fixup')\n",
        "\n",
        "def identity_resnet250_kaiming():\n",
        "    return PreActResNet(PreActBlock, [32,32,32,32], init='Kaiming')\n",
        "\n",
        "def identity_resnet498():\n",
        "    return PreActResNet(PreActBlock, [64,64,64,64])\n",
        "\n",
        "def identity_resnet498_non_bn():\n",
        "    return PreActResNet(PreActBlockNonBN, [64,64,64,64], BN_enable=False)\n",
        "\n",
        "def identity_resnet498_non_bn_kaiming():\n",
        "    return PreActResNet(PreActBlockNonBN, [64,64,64,64], BN_enable=False, init='Kaiming')\n",
        "\n",
        "def identity_resnet498_non_bn_xavier():\n",
        "    return PreActResNet(PreActBlockNonBN, [64,64,64,64], BN_enable=False, init='Xavier')\n",
        "\n",
        "def identity_resnet498_non_bn_fixup():\n",
        "    return PreActResNet(PreActBlockNonBN, [64,64,64,64], BN_enable=False, init='Fixup')\n",
        "\n",
        "def identity_resnet498_kaiming():\n",
        "    return PreActResNet(PreActBlock, [64,64,64,64], init='Kaiming')\n",
        "\n",
        "def identity_resnet994():\n",
        "    return PreActResNet(PreActBlock, [128,128,128,128])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CINOLLBCRV89"
      },
      "source": [
        "# ResNet Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLah4HOg6yUw"
      },
      "outputs": [],
      "source": [
        "from numpy.core.shape_base import block\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        # self.relu = zero_relu\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes),\n",
        "                Hadamard_Transform(self.expansion*planes,self.expansion*planes)\n",
        "            )\n",
        "            self.shortcut[0].type_name= 'conv1x1'\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks,init, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.init=init \n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        self.first_transform = nn.Sequential(ChannelPaddingSkip(0,61, scale=1), Hadamard_Transform(64,64))\n",
        "        # self.first_transform = nn.Sequential( Hadamard_Transform(64,64))\n",
        "\n",
        "        # self.apply(self._init_weights)\n",
        "        # self.relu = zero_relu\n",
        "\n",
        "        if self.init == 'ZerO':\n",
        "          for m in self.modules():\n",
        "            if isinstance(m,nn.Linear):\n",
        "              nn.init.constant_(m.weight,0)\n",
        "              nn.init.constant_(m.bias,0)\n",
        "            if isinstance(m,BasicBlock):\n",
        "              nn.init.constant_(m.conv1.weight,0)\n",
        "              nn.init.constant_(m.conv2.weight,0)\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "               if hasattr(m,'type_name'):\n",
        "                   if 'conv1x1' in m.type_name:\n",
        "                      init_sub_identity_conv1x1(m.weight)\n",
        "                      print('sub identity init a conv1x1')\n",
        "               else:\n",
        "                      nn.init.constant_(m.weight,0)\n",
        "            \n",
        "        \n",
        "        elif self.init == 'Random':\n",
        "          pass\n",
        "        # check initialization status\n",
        "        for name, param in self.named_parameters():\n",
        "            unique_values = torch.unique(param.data)\n",
        "            if len(unique_values) > 2 and 'downsample' not in name and 'ortho_transform' not in name:\n",
        "                print('!!!!!!!!!!!!!!!!following is not initialized as zero or one!!!!!!!!!!!!!!!!')\n",
        "                print(name)\n",
        "                print(unique_values)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # out=self.first_transform(x)\n",
        "        # out = F.relu(self.bn1(self.conv1(out)))\n",
        "        out=self.conv1(x)+self.first_transform(x)\n",
        "        out=F.relu(self.bn1(out))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18_ZerO():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], init='ZerO')\n",
        "def ResNet18_Random():\n",
        "  return ResNet(BasicBlock,[2,2,2,2],init='Random')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol0xVLm6CbHA"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJw6ScNdb31X"
      },
      "source": [
        "# **Building normal  resnet model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buQh4TWX7y9r",
        "outputId": "a21d073a-62ad-465d-ab78-18deda569242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Building model..\n",
            "sub identity init a conv1x1\n",
            "sub identity init a conv1x1\n",
            "sub identity init a conv1x1\n",
            "\n",
            "Epoch: 0\n",
            " [================================================================>]  Step: 86ms | Tot: 50s664ms | Loss: 1.735 | Acc: 36.446% (18223/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 1.7352758527106946 Acc:  36.446 correct 18223 total 50000\n",
            " [================================================================>]  Step: 42ms | Tot: 4s919ms | Loss: 1.535 | Acc: 43.700% (4370/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 1.5353835332393646 Acc: 43.7 Correct 4370 total 10000\n",
            "43.7\n",
            "\n",
            "Epoch: 1\n",
            " [================================================================>]  Step: 92ms | Tot: 50s132ms | Loss: 1.363 | Acc: 50.148% (25074/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 1.3634341619813535 Acc:  50.148 correct 25074 total 50000\n",
            " [================================================================>]  Step: 54ms | Tot: 5s796ms | Loss: 1.220 | Acc: 56.160% (5616/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 1.2196313798427583 Acc: 56.16 Correct 5616 total 10000\n",
            "56.16\n",
            "\n",
            "Epoch: 2\n",
            " [================================================================>]  Step: 86ms | Tot: 50s627ms | Loss: 1.100 | Acc: 60.560% (30280/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 1.099568054041899 Acc:  60.56 correct 30280 total 50000\n",
            " [================================================================>]  Step: 42ms | Tot: 5s56ms | Loss: 0.994 | Acc: 64.110% (6411/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.9942542421817779 Acc: 64.11 Correct 6411 total 10000\n",
            "64.11\n",
            "\n",
            "Epoch: 3\n",
            " [================================================================>]  Step: 87ms | Tot: 50s652ms | Loss: 0.872 | Acc: 69.278% (34639/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.8719585711693825 Acc:  69.278 correct 34639 total 50000\n",
            " [================================================================>]  Step: 44ms | Tot: 5s608ms | Loss: 0.818 | Acc: 71.390% (7139/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.8182187420129776 Acc: 71.39 Correct 7139 total 10000\n",
            "71.39\n",
            "\n",
            "Epoch: 4\n",
            " [================================================================>]  Step: 83ms | Tot: 50s328ms | Loss: 0.732 | Acc: 74.456% (37228/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.7319169805177947 Acc:  74.456 correct 37228 total 50000\n",
            " [================================================================>]  Step: 54ms | Tot: 5s159ms | Loss: 0.683 | Acc: 76.290% (7629/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6827771493792534 Acc: 76.29 Correct 7629 total 10000\n",
            "76.29\n",
            "\n",
            "Epoch: 5\n",
            " [================================================================>]  Step: 86ms | Tot: 50s85ms | Loss: 0.641 | Acc: 77.760% (38880/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.641233514565641 Acc:  77.76 correct 38880 total 50000\n",
            " [================================================================>]  Step: 45ms | Tot: 5s115ms | Loss: 0.731 | Acc: 74.810% (7481/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.7311046376824379 Acc: 74.81 Correct 7481 total 10000\n",
            "76.29\n",
            "\n",
            "Epoch: 6\n",
            " [================================================================>]  Step: 80ms | Tot: 50s261ms | Loss: 0.588 | Acc: 79.616% (39808/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.5876257841087058 Acc:  79.616 correct 39808 total 50000\n",
            " [================================================================>]  Step: 44ms | Tot: 5s707ms | Loss: 0.667 | Acc: 77.170% (7717/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6673074999451637 Acc: 77.17 Correct 7717 total 10000\n",
            "77.17\n",
            "\n",
            "Epoch: 7\n",
            " [================================================================>]  Step: 89ms | Tot: 50s292ms | Loss: 0.551 | Acc: 80.924% (40462/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.5509731552332563 Acc:  80.924 correct 40462 total 50000\n",
            " [================================================================>]  Step: 49ms | Tot: 5s61ms | Loss: 0.612 | Acc: 79.320% (7932/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6117288222908974 Acc: 79.32 Correct 7932 total 10000\n",
            "79.32\n",
            "\n",
            "Epoch: 8\n",
            " [================================================================>]  Step: 79ms | Tot: 50s498ms | Loss: 0.527 | Acc: 81.670% (40835/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.5273831974514915 Acc:  81.67 correct 40835 total 50000\n",
            " [================================================================>]  Step: 45ms | Tot: 5s888ms | Loss: 0.589 | Acc: 79.930% (7993/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.5885127264261246 Acc: 79.93 Correct 7993 total 10000\n",
            "79.93\n",
            "\n",
            "Epoch: 9\n",
            " [================================================================>]  Step: 88ms | Tot: 50s255ms | Loss: 0.509 | Acc: 82.392% (41196/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.5093543809240736 Acc:  82.392 correct 41196 total 50000\n",
            " [================================================================>]  Step: 41ms | Tot: 5s10ms | Loss: 0.568 | Acc: 80.670% (8067/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.5683570718765258 Acc: 80.67 Correct 8067 total 10000\n",
            "80.67\n",
            "\n",
            "Epoch: 10\n",
            " [================================================================>]  Step: 83ms | Tot: 50s478ms | Loss: 0.491 | Acc: 82.956% (41478/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.4908239402429527 Acc:  82.956 correct 41478 total 50000\n",
            " [================================================================>]  Step: 28ms | Tot: 5s331ms | Loss: 0.585 | Acc: 80.180% (8018/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.5848544955253601 Acc: 80.18 Correct 8018 total 10000\n",
            "80.67\n",
            "\n",
            "Epoch: 11\n",
            " [================================================================>]  Step: 82ms | Tot: 50s220ms | Loss: 0.476 | Acc: 83.558% (41779/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.4759052501767493 Acc:  83.558 correct 41779 total 50000\n",
            " [================================================================>]  Step: 50ms | Tot: 5s673ms | Loss: 0.510 | Acc: 82.600% (8260/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.5102730742096901 Acc: 82.6 Correct 8260 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 12\n",
            " [================================================================>]  Step: 81ms | Tot: 50s522ms | Loss: 0.459 | Acc: 84.168% (42084/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.45850652860253666 Acc:  84.168 correct 42084 total 50000\n",
            " [================================================================>]  Step: 46ms | Tot: 5s211ms | Loss: 0.681 | Acc: 77.830% (7783/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6810799732804298 Acc: 77.83 Correct 7783 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 13\n",
            " [================================================================>]  Step: 78ms | Tot: 50s560ms | Loss: 0.452 | Acc: 84.288% (42144/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.4522221396341348 Acc:  84.288 correct 42144 total 50000\n",
            " [================================================================>]  Step: 47ms | Tot: 5s681ms | Loss: 0.551 | Acc: 80.960% (8096/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.550827514231205 Acc: 80.96 Correct 8096 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 14\n",
            " [================================================================>]  Step: 80ms | Tot: 50s543ms | Loss: 0.436 | Acc: 84.914% (42457/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.4364969420920857 Acc:  84.914 correct 42457 total 50000\n",
            " [================================================================>]  Step: 43ms | Tot: 4s967ms | Loss: 0.621 | Acc: 78.470% (7847/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.620997468829155 Acc: 78.47 Correct 7847 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 15\n",
            " [================================================================>]  Step: 84ms | Tot: 50s372ms | Loss: 0.429 | Acc: 85.122% (42561/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.42914723782130826 Acc:  85.122 correct 42561 total 50000\n",
            " [================================================================>]  Step: 35ms | Tot: 5s519ms | Loss: 0.651 | Acc: 79.200% (7920/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6506189367175103 Acc: 79.2 Correct 7920 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 16\n",
            " [================================================================>]  Step: 79ms | Tot: 50s235ms | Loss: 0.427 | Acc: 85.270% (42635/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.42662651166129295 Acc:  85.27 correct 42635 total 50000\n",
            " [================================================================>]  Step: 45ms | Tot: 5s155ms | Loss: 0.588 | Acc: 80.170% (8017/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.5876236146688462 Acc: 80.17 Correct 8017 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 17\n",
            " [================================================================>]  Step: 83ms | Tot: 50s188ms | Loss: 0.415 | Acc: 85.846% (42923/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.4145812865946909 Acc:  85.846 correct 42923 total 50000\n",
            " [================================================================>]  Step: 43ms | Tot: 5s5ms | Loss: 0.606 | Acc: 80.220% (8022/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6060139563679695 Acc: 80.22 Correct 8022 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 18\n",
            " [================================================================>]  Step: 95ms | Tot: 50s600ms | Loss: 0.410 | Acc: 86.078% (43039/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.4095400737603302 Acc:  86.078 correct 43039 total 50000\n",
            " [================================================================>]  Step: 38ms | Tot: 5s961ms | Loss: 0.604 | Acc: 79.710% (7971/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.6039481276273727 Acc: 79.71 Correct 7971 total 10000\n",
            "82.6\n",
            "\n",
            "Epoch: 19\n",
            " [================================================================>]  Step: 88ms | Tot: 50s302ms | Loss: 0.407 | Acc: 85.954% (42977/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.40693415293608176 Acc:  85.954 correct 42977 total 50000\n",
            " [================================================================>]  Step: 51ms | Tot: 4s934ms | Loss: 0.484 | Acc: 83.300% (8330/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.48435930877923966 Acc: 83.3 Correct 8330 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 20\n",
            " [================================================================>]  Step: 87ms | Tot: 50s197ms | Loss: 0.396 | Acc: 86.450% (43225/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "Train Loss: 0.39639362471792705 Acc:  86.45 correct 43225 total 50000\n",
            " [================================================================>]  Step: 42ms | Tot: 5s744ms | Loss: 0.522 | Acc: 81.860% (8186/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Test Loss: 0.5217738151550293 Acc: 81.86 Correct 8186 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 21\n"
          ]
        }
      ],
      "source": [
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true',\n",
        "#                     help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "net = identity_resnet18()\n",
        "net = net.to(device)\n",
        "# if device == 'cuda':\n",
        "#     net = torch.nn.DataParallel(net)\n",
        "#     cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch,net)\n",
        "    test(epoch,net)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzy9pE6eCZF"
      },
      "source": [
        "# **Quantizer Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8i0o8YEyvwqm"
      },
      "outputs": [],
      "source": [
        "class Quantizer(nn.Module):\n",
        "    def __init__(self, m, e,gamma):\n",
        "        super().__init__()\n",
        "        self.m = m # number of mantissa bit \n",
        "        self.e = e # number of exponent bit\n",
        "        self.gamma=gamma\n",
        "    def forward(self, input):\n",
        "        sign=torch.sign(input) #sign bit\n",
        "        b= 2**(self.e-1) #bias\n",
        "        b=b-torch.log2(torch.tensor(self.gamma))     \n",
        "        b=b.to(device)\n",
        "        c=(2-2**(-self.m))*2**(2**self.e-b-1)  #maximum representable range i.e. dynamic range\n",
        "        temp=torch.floor(torch.log2(abs(input))+b)\n",
        "        temp_max=temp.max()\n",
        "        temp_max=temp_max.to(device)\n",
        "        temp_min=torch.tensor(1)\n",
        "        temp_min=temp_min.to(device)\n",
        "        if temp_max<temp_min:\n",
        "          temp_max=temp_min\n",
        "        temp=torch.clamp(temp,temp_min,temp_max)\n",
        "        p=temp-b-torch.round(torch.tensor(self.m))\n",
        "        s=2**p\n",
        "        output = torch.clamp(sign*s*torch.round(abs(input)/s),-c,c)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        self.quant = Quantizer(4,3,1)\n",
        "        self.model = model\n",
        "        # self.conv1_init_weight = self.model.conv1.weight.data\n",
        "        W = self.model.layer1[0].conv1.weight.data\n",
        "        print(W)\n",
        "        print(W.size())\n",
        "        W = self.model.layer2[0].shortcut[0].weight.data\n",
        "        print(W)\n",
        "        # W = self.model.layer2[0].shortcut[1].weight.data\n",
        "        # print(W)\n",
        "        W = self.model.linear.weight.data\n",
        "        print(W)\n",
        "        B=self.model.linear.bias.data\n",
        "        print(B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.quant(x)\n",
        "        W=self.model.conv1.weight.data\n",
        "        self.model.conv1.weight.data=self.quant(W)\n",
        "        x=self.model.conv1(x)+self.model.first_transform(x)\n",
        "        # x=self.quant(x)\n",
        "        W=self.model.bn1.weight.data\n",
        "        self.model.bn1.weight.data=self.quant(W)\n",
        "        B=self.model.bn1.bias.data\n",
        "        self.model.bn1.bias.data=self.quant(B)\n",
        "        x=self.model.bn1(x)\n",
        "        x10=F.relu(x)\n",
        "        # x=self.quant(x)\n",
        "        # #layer 1 \n",
        "        W=self.model.layer1[0].conv1.weight.data\n",
        "        self.model.layer1[0].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer1[0].conv1(x10)\n",
        "        W=self.model.layer1[0].bn1.weight.data\n",
        "        self.model.layer1[0].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer1[0].bn1.bias.data\n",
        "        self.model.layer1[0].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer1[0].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer1[0].conv2.weight.data\n",
        "        self.model.layer1[0].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer1[0].conv2(x)\n",
        "        W=self.model.layer1[0].bn2.weight.data\n",
        "        self.model.layer1[0].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer1[0].bn2.bias.data\n",
        "        self.model.layer1[0].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer1[0].bn2(x)\n",
        "        x11=F.relu(x)\n",
        "        W=self.model.layer1[1].conv1.weight.data\n",
        "        self.model.layer1[1].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer1[1].conv1(x11)\n",
        "        W=self.model.layer1[1].bn1.weight.data\n",
        "        self.model.layer1[1].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer1[1].bn1.bias.data\n",
        "        self.model.layer1[1].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer1[1].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer1[1].conv2.weight.data\n",
        "        self.model.layer1[1].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer1[1].conv2(x)\n",
        "        W=self.model.layer1[1].bn2.weight.data\n",
        "        self.model.layer1[1].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer1[1].bn2.bias.data\n",
        "        self.model.layer1[1].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer1[1].bn2(x)\n",
        "        x20=F.relu(x)\n",
        "        # # #layer 2\n",
        "        W=self.model.layer2[0].conv1.weight.data\n",
        "        self.model.layer2[0].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer2[0].conv1(x20)\n",
        "        W=self.model.layer2[0].bn1.weight.data\n",
        "        self.model.layer2[0].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer2[0].bn1.bias.data\n",
        "        self.model.layer2[0].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer2[0].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer2[0].conv2.weight.data\n",
        "        self.model.layer2[0].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer2[0].conv2(x)\n",
        "        W=self.model.layer2[0].bn2.weight.data\n",
        "        self.model.layer2[0].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer2[0].bn2.bias.data\n",
        "        self.model.layer2[0].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer2[0].bn2(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer2[0].shortcut[0].weight.data\n",
        "        self.model.layer2[0].shortcut[0].weight.data=self.quant(W)\n",
        "        xs=self.model.layer2[0].shortcut[0](x20)\n",
        "        W=self.model.layer2[0].shortcut[1].weight.data\n",
        "        self.model.layer2[0].shortcut[1].weight.data=self.quant(W)\n",
        "        B=self.model.layer2[0].shortcut[1].bias.data\n",
        "        self.model.layer2[0].shortcut[1].bias.data=self.quant(B)\n",
        "        xs=self.model.layer2[0].shortcut[1](xs)\n",
        "        x=x.clone()+self.model.layer2[0].shortcut[2](xs)\n",
        "        x21=F.relu(x)\n",
        "        W=self.model.layer2[1].conv1.weight.data\n",
        "        self.model.layer2[1].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer2[1].conv1(x21)\n",
        "        W=self.model.layer2[1].bn1.weight.data\n",
        "        self.model.layer2[1].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer2[1].bn1.bias.data\n",
        "        self.model.layer2[1].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer2[1].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer2[1].conv2.weight.data\n",
        "        self.model.layer2[1].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer2[1].conv2(x)\n",
        "        W=self.model.layer2[1].bn2.weight.data\n",
        "        self.model.layer2[1].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer2[1].bn2.bias.data\n",
        "        self.model.layer2[1].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer2[1].bn2(x)\n",
        "        x30=F.relu(x)\n",
        "\n",
        "        # # #layer 3\n",
        "        W=self.model.layer3[0].conv1.weight.data\n",
        "        self.model.layer3[0].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer3[0].conv1(x30)\n",
        "        W=self.model.layer3[0].bn1.weight.data\n",
        "        self.model.layer3[0].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer3[0].bn1.bias.data\n",
        "        self.model.layer3[0].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer3[0].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer3[0].conv2.weight.data\n",
        "        self.model.layer3[0].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer3[0].conv2(x)\n",
        "        W=self.model.layer3[0].bn2.weight.data\n",
        "        self.model.layer3[0].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer3[0].bn2.bias.data\n",
        "        self.model.layer3[0].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer3[0].bn2(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer3[0].shortcut[0].weight.data\n",
        "        self.model.layer3[0].shortcut[0].weight.data=self.quant(W)\n",
        "        xs=self.model.layer3[0].shortcut[0](x30)\n",
        "        W=self.model.layer3[0].shortcut[1].weight.data\n",
        "        self.model.layer3[0].shortcut[1].weight.data=self.quant(W)\n",
        "        B=self.model.layer3[0].shortcut[1].bias.data\n",
        "        self.model.layer3[0].shortcut[1].bias.data=self.quant(B)\n",
        "        xs=self.model.layer3[0].shortcut[1](xs)\n",
        "        x=x.clone()+self.model.layer3[0].shortcut[2](xs)\n",
        "        x31=F.relu(x)\n",
        "        W=self.model.layer3[1].conv1.weight.data\n",
        "        self.model.layer3[1].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer3[1].conv1(x31)\n",
        "        W=self.model.layer3[1].bn1.weight.data\n",
        "        self.model.layer3[1].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer3[1].bn1.bias.data\n",
        "        self.model.layer3[1].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer3[1].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer3[1].conv2.weight.data\n",
        "        self.model.layer3[1].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer3[1].conv2(x)\n",
        "        W=self.model.layer3[1].bn2.weight.data\n",
        "        self.model.layer3[1].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer3[1].bn2.bias.data\n",
        "        self.model.layer3[1].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer3[1].bn2(x)\n",
        "        x40=F.relu(x)\n",
        "        # # layer 4\n",
        "        W=self.model.layer4[0].conv1.weight.data\n",
        "        self.model.layer4[0].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer4[0].conv1(x40)\n",
        "        W=self.model.layer4[0].bn1.weight.data\n",
        "        self.model.layer4[0].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer4[0].bn1.bias.data\n",
        "        self.model.layer4[0].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer4[0].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer4[0].conv2.weight.data\n",
        "        self.model.layer4[0].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer4[0].conv2(x)\n",
        "        W=self.model.layer4[0].bn2.weight.data\n",
        "        self.model.layer4[0].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer4[0].bn2.bias.data\n",
        "        self.model.layer4[0].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer4[0].bn2(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer4[0].shortcut[0].weight.data\n",
        "        self.model.layer4[0].shortcut[0].weight.data=self.quant(W)\n",
        "        xs=self.model.layer4[0].shortcut[0](x40)\n",
        "        W=self.model.layer4[0].shortcut[1].weight.data\n",
        "        self.model.layer4[0].shortcut[1].weight.data=self.quant(W)\n",
        "        B=self.model.layer4[0].shortcut[1].bias.data\n",
        "        self.model.layer4[0].shortcut[1].bias.data=self.quant(B)\n",
        "        xs=self.model.layer4[0].shortcut[1](xs)\n",
        "        x=x.clone()+self.model.layer4[0].shortcut[2](xs)\n",
        "        x41=F.relu(x)\n",
        "        W=self.model.layer4[1].conv1.weight.data\n",
        "        self.model.layer4[1].conv1.weight.data=self.quant(W)\n",
        "        x=self.model.layer4[1].conv1(x41)\n",
        "        W=self.model.layer4[1].bn1.weight.data\n",
        "        self.model.layer4[1].bn1.weight.data=self.quant(W)\n",
        "        B=self.model.layer4[1].bn1.bias.data\n",
        "        self.model.layer4[1].bn1.bias.data=self.quant(B)\n",
        "        x=self.model.layer4[1].bn1(x)\n",
        "        x=F.relu(x)\n",
        "        W=self.model.layer4[1].conv2.weight.data\n",
        "        self.model.layer4[1].conv2.weight.data=self.quant(W)\n",
        "        x=self.model.layer4[1].conv2(x)\n",
        "        W=self.model.layer4[1].bn2.weight.data\n",
        "        self.model.layer4[1].bn2.weight.data=self.quant(W)\n",
        "        B=self.model.layer4[1].bn2.bias.data\n",
        "        self.model.layer4[1].bn2.bias.data=self.quant(B)\n",
        "        x=self.model.layer4[1].bn2(x)\n",
        "        x=F.relu(x)\n",
        "\n",
        "\n",
        "        x=F.avg_pool2d(x,4)\n",
        "        x=x.view(x.size(0),-1)\n",
        "        W=self.model.linear.weight.data\n",
        "        self.model.linear.weight.data=self.quant(W)\n",
        "        B=self.model.linear.bias.data\n",
        "        self.model.linear.bias.data=self.quant(B)\n",
        "        x=self.model.linear(x)\n",
        "        return x\n",
        "\n",
        "class PreAct(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(PreAct, self).__init__()\n",
        "        self.quant = Quantizer(4,3,0.2)\n",
        "        self.model = model\n",
        "        self.conv1_init_weight = self.model.conv1.weight.data\n",
        "        self.conv1_init_weight= self.conv1_init_weight.to(device)\n",
        "        # self.bn1_init_weight=self.model.bn1.weight.data\n",
        "        # self.bn1_init_weight=self.bn1_init_weight.to(device)\n",
        "        # self.bn1_init_bias=self.model.bn1.weight.data\n",
        "        # self.bn1_init_bias=self.bn1_init_bias.to(device)\n",
        "        #layer1\n",
        "        # self.layer1_0_bn1_init_weight=self.model.layer1[0].bn1.weight.data\n",
        "        # self.layer1_0_bn1_init_weight=self.layer1_0_bn1_init_weight.to(device)\n",
        "        # self.layer1_0_bn1_init_bias=self.model.layer1[0].bn1.bias.data\n",
        "        # self.layer1_0_bn1_init_bias=self.layer1_0_bn1_init_bias.to(device)\n",
        "        self.layer1_0_conv1_init_weight=self.model.layer1[0].conv1.weight.data\n",
        "        self.layer1_0_conv1_init_weight=self.layer1_0_conv1_init_weight.to(device)\n",
        "        # self.layer1_0_bn2_init_weight=self.model.layer1[0].bn2.weight.data\n",
        "        # self.layer1_0_bn2_init_weight=self.layer1_0_bn2_init_weight.to(device)\n",
        "        # self.layer1_0_bn2_init_bias=self.model.layer1[0].bn2.bias.data\n",
        "        # self.layer1_0_bn2_init_bias=self.layer1_0_bn2_init_bias.to(device)\n",
        "        self.layer1_0_conv2_init_weight=self.model.layer1[0].conv2.weight.data\n",
        "        self.layer1_0_conv2_init_weight=self.layer1_0_conv2_init_weight.to(device)\n",
        "        # self.layer1_1_bn1_init_weight=self.model.layer1[1].bn1.weight.data\n",
        "        # self.layer1_1_bn1_init_weight=self.layer1_1_bn1_init_weight.to(device)\n",
        "        # self.layer1_1_bn1_init_bias=self.model.layer1[1].bn1.bias.data\n",
        "        # self.layer1_1_bn1_init_bias=self.layer1_1_bn1_init_bias.to(device)\n",
        "        self.layer1_1_conv1_init_weight=self.model.layer1[1].conv1.weight.data\n",
        "        self.layer1_1_conv1_init_weight=self.layer1_1_conv1_init_weight.to(device)\n",
        "        # self.layer1_1_bn2_init_weight=self.model.layer1[1].bn2.weight.data\n",
        "        # self.layer1_1_bn2_init_weight= self.layer1_1_bn2_init_weight.to(device)\n",
        "        # self.layer1_1_bn2_init_bias=self.model.layer1[1].bn2.bias.data\n",
        "        # self.layer1_1_bn2_init_bias=self.layer1_1_bn2_init_bias.to(device)\n",
        "        self.layer1_1_conv2_init_weight=self.model.layer1[1].conv2.weight.data\n",
        "        self.layer1_1_conv2_init_weight=self.layer1_1_conv2_init_weight.to(device)\n",
        "        #layer2\n",
        "        # self.layer2_0_bn1_init_weight=self.model.layer2[0].bn1.weight.data\n",
        "        # self.layer2_0_bn1_init_weight=self.layer2_0_bn1_init_weight.to(device)\n",
        "        # self.layer2_0_bn1_init_bias=self.model.layer2[0].bn1.bias.data\n",
        "        # self.layer2_0_bn1_init_bias=self.layer2_0_bn1_init_bias.to(device)\n",
        "        self.layer2_0_shortcut_0_init_weight=self.model.layer2[0].shortcut[0].weight.data\n",
        "        self.layer2_0_shortcut_0_init_weight=self.layer2_0_shortcut_0_init_weight.to(device)\n",
        "        self.layer2_0_conv1_init_weight=self.model.layer2[0].conv1.weight.data\n",
        "        self.layer2_0_conv1_init_weight=self.layer2_0_conv1_init_weight.to(device)\n",
        "        # self.layer2_0_bn2_init_weight=self.model.layer2[0].bn2.weight.data\n",
        "        # self.layer2_0_bn2_init_weight=self.layer2_0_bn2_init_weight.to(device)\n",
        "        # self.layer2_0_bn2_init_bias=self.model.layer2[0].bn2.bias.data\n",
        "        # self.layer2_0_bn2_init_bias=self.layer2_0_bn2_init_bias.to(device)\n",
        "        self.layer2_0_conv2_init_weight=self.model.layer2[0].conv2.weight.data\n",
        "        self.layer2_0_conv2_init_weight=self.layer2_0_conv2_init_weight.to(device)\n",
        "        # self.layer2_1_bn1_init_weight=self.model.layer2[1].bn1.weight.data\n",
        "        # self.layer2_1_bn1_init_weight=self.layer2_1_bn1_init_weight.to(device)\n",
        "        # self.layer2_1_bn1_init_bias=self.model.layer2[1].bn1.bias.data\n",
        "        # self.layer2_1_bn1_init_bias=self.layer2_1_bn1_init_bias.to(device)\n",
        "        self.layer2_1_conv1_init_weight=self.model.layer2[1].conv1.weight.data\n",
        "        self.layer2_1_conv1_init_weight=self.layer2_1_conv1_init_weight.to(device)\n",
        "        # self.layer2_1_bn2_init_weight=self.model.layer2[1].bn2.weight.data\n",
        "        # self.layer2_1_bn2_init_weight=self.layer2_1_bn2_init_weight.to(device)\n",
        "        # self.layer2_1_bn2_init_bias=self.model.layer2[1].bn2.bias.data\n",
        "        # self.layer2_1_bn2_init_bias=self.layer2_1_bn2_init_bias.to(device)\n",
        "        self.layer2_1_conv2_init_weight=self.model.layer2[1].conv2.weight.data\n",
        "        self.layer2_1_conv2_init_weight=self.layer2_1_conv2_init_weight.to(device)\n",
        "        #layer3\n",
        "        # self.layer3_0_bn1_init_weight=self.model.layer3[0].bn1.weight.data\n",
        "        # self.layer3_0_bn1_init_weight=self.layer3_0_bn1_init_weight.to(device)\n",
        "        # self.layer3_0_bn1_init_bias=self.model.layer3[0].bn1.bias.data\n",
        "        # self.layer3_0_bn1_init_bias=self.layer3_0_bn1_init_bias.to(device)\n",
        "        self.layer3_0_shortcut_0_init_weight=self.model.layer3[0].shortcut[0].weight.data\n",
        "        self.layer3_0_shortcut_0_init_weight=self.layer3_0_shortcut_0_init_weight.to(device)\n",
        "        self.layer3_0_conv1_init_weight=self.model.layer3[0].conv1.weight.data\n",
        "        self.layer3_0_conv1_init_weight=self.layer3_0_conv1_init_weight.to(device)\n",
        "        # self.layer3_0_bn2_init_weight=self.model.layer3[0].bn2.weight.data\n",
        "        # self.layer3_0_bn2_init_weight=self.layer3_0_bn2_init_weight.to(device)\n",
        "        # self.layer3_0_bn2_init_bias=self.model.layer3[0].bn2.bias.data\n",
        "        # self.layer3_0_bn2_init_bias=self.layer3_0_bn2_init_bias.to(device)\n",
        "        self.layer3_0_conv2_init_weight=self.model.layer3[0].conv2.weight.data\n",
        "        self.layer3_0_conv2_init_weight=self.layer3_0_conv2_init_weight.to(device)\n",
        "        # self.layer3_1_bn1_init_weight=self.model.layer3[1].bn1.weight.data\n",
        "        # self.layer3_1_bn1_init_weight=self.layer3_1_bn1_init_weight.to(device)\n",
        "        # self.layer3_1_bn1_init_bias=self.model.layer3[1].bn1.bias.data\n",
        "        # self.layer3_1_bn1_init_bias=self.layer3_1_bn1_init_bias.to(device)\n",
        "        self.layer3_1_conv1_init_weight=self.model.layer3[1].conv1.weight.data\n",
        "        self.layer3_1_conv1_init_weight=self.layer3_1_conv1_init_weight.to(device)\n",
        "        # self.layer3_1_bn2_init_weight=self.model.layer3[1].bn2.weight.data\n",
        "        # self.layer3_1_bn2_init_weight=self.layer3_1_bn2_init_weight.to(device)\n",
        "        # self.layer3_1_bn2_init_bias=self.model.layer3[1].bn2.bias.data\n",
        "        # self.layer3_1_bn2_init_bias=self.layer3_1_bn2_init_bias.to(device)\n",
        "        self.layer3_1_conv2_init_weight=self.model.layer3[1].conv2.weight.data\n",
        "        self.layer3_1_conv2_init_weight=self.layer3_1_conv2_init_weight.to(device)\n",
        "        #layer4\n",
        "        # self.layer4_0_bn1_init_weight=self.model.layer4[0].bn1.weight.data\n",
        "        # self.layer4_0_bn1_init_weight=self.layer4_0_bn1_init_weight.to(device)\n",
        "        # self.layer4_0_bn1_init_bias=self.model.layer4[0].bn1.bias.data\n",
        "        # self.layer4_0_bn1_init_bias=self.layer4_0_bn1_init_bias.to(device)\n",
        "        self.layer4_0_shortcut_0_init_weight=self.model.layer4[0].shortcut[0].weight.data\n",
        "        self.layer4_0_shortcut_0_init_weight=self.layer4_0_shortcut_0_init_weight.to(device)\n",
        "        self.layer4_0_conv1_init_weight=self.model.layer4[0].conv1.weight.data\n",
        "        self.layer4_0_conv1_init_weight=self.layer4_0_conv1_init_weight.to(device)\n",
        "        # self.layer4_0_bn2_init_weight=self.model.layer4[0].bn2.weight.data\n",
        "        # self.layer4_0_bn2_init_weight=self.layer4_0_bn2_init_weight.to(device)\n",
        "        # self.layer4_0_bn2_init_bias=self.model.layer4[0].bn2.bias.data\n",
        "        # self.layer4_0_bn2_init_bias=self.layer4_0_bn2_init_bias.to(device)\n",
        "        self.layer4_0_conv2_init_weight=self.model.layer4[0].conv2.weight.data\n",
        "        self.layer4_0_conv2_init_weight=self.layer4_0_conv2_init_weight.to(device)\n",
        "        # self.layer4_1_bn1_init_weight=self.model.layer4[1].bn1.weight.data\n",
        "        # self.layer4_1_bn1_init_weight=self.layer4_1_bn1_init_weight.to(device)\n",
        "        # self.layer4_1_bn1_init_bias=self.model.layer4[1].bn1.bias.data\n",
        "        # self.layer4_1_bn1_init_bias=self.layer4_1_bn1_init_bias.to(device)\n",
        "        self.layer4_1_conv1_init_weight=self.model.layer4[1].conv1.weight.data\n",
        "        self.layer4_1_conv1_init_weight=self.layer4_1_conv1_init_weight.to(device)\n",
        "        # self.layer4_1_bn2_init_weight=self.model.layer4[1].bn2.weight.data\n",
        "        # self.layer4_1_bn2_init_weight=self.layer4_1_bn2_init_weight.to(device)\n",
        "        # self.layer4_1_bn2_init_bias=self.model.layer4[1].bn2.bias.data\n",
        "        # self.layer4_1_bn2_init_bias=self.layer4_1_bn2_init_bias.to(device)\n",
        "        self.layer4_1_conv2_init_weight=self.model.layer4[1].conv2.weight.data\n",
        "        self.layer4_1_conv2_init_weight=self.layer4_1_conv2_init_weight.to(device)\n",
        "\n",
        "        # self.bn2_init_weight=self.model.bn2.weight.data\n",
        "        # self.bn2_init_weight=self.bn2_init_weight.to(device)\n",
        "        # self.bn2_init_bias=self.model.bn2.bias.data\n",
        "        # self.bn2_init_bias=self.bn2_init_bias.to(device)\n",
        "        self.linear_init_weight=self.model.linear.weight.data\n",
        "        self.linear_init_weight=self.linear_init_weight.to(device)\n",
        "        self.linear_init_bias=self.model.linear.bias.data\n",
        "        self.linear_init_bias=self.linear_init_bias.to(device)\n",
        "       \n",
        "        W = self.model.layer1[0].conv1.weight.data\n",
        "        print(W)\n",
        "        W = self.model.layer2[0].shortcut[0].weight.data\n",
        "        print(W)\n",
        "        W = self.model.linear.weight.data\n",
        "        print(W)\n",
        "        B=self.model.linear.bias.data\n",
        "        print(B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.quant(x)\n",
        "        W=self.model.conv1.weight.data\n",
        "        # self.model.conv1.weight.data=self.quant(W)\n",
        "        self.model.conv1.weight.data=self.conv1_init_weight+self.quant(W-self.conv1_init_weight)\n",
        "        x=self.model.conv1(x)+self.model.first_transform(x)\n",
        "        # x=self.quant(x)\n",
        "        x=zero_relu(x)\n",
        "        # W=self.model.bn1.weight.data\n",
        "        # self.model.bn1.weight.data=self.quant(W)\n",
        "        # self.model.bn1.weight.data=self.bn1_init_weight+self.quant(W-self.bn1_init_weight)\n",
        "        # B=self.model.bn1.bias.data\n",
        "        # self.model.bn1.bias.data=self.quant(B)\n",
        "        # self.model.bn1.bias.data=self.bn1_init_bias+self.quant(B-self.bn1_init_bias)\n",
        "        x10=self.model.bn1(x)\n",
        "        \n",
        "        # #layer 1 \n",
        "        # W=self.model.layer1[0].bn1.weight.data\n",
        "        # self.model.layer1[0].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer1[0].bn1.weight.data=self.layer1_0_bn1_init_weight + self.quant(W-self.layer1_0_bn1_init_weight)\n",
        "        # B=self.model.layer1[0].bn1.bias.data\n",
        "        # self.model.layer1[0].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer1[0].bn1.bias.data=self.layer1_0_bn1_init_bias + self.quant(B-self.layer1_0_bn1_init_bias)\n",
        "        x=self.model.layer1[0].bn1(zero_relu(x10))\n",
        "        shortcut=x10\n",
        "        W=self.model.layer1[0].conv1.weight.data\n",
        "        # self.model.layer1[0].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer1[0].conv1.weight.data=self.layer1_0_conv1_init_weight + self.quant(W-self.layer1_0_conv1_init_weight)\n",
        "        x=self.model.layer1[0].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer1[0].bn2.weight.data\n",
        "        # self.model.layer1[0].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer1[0].bn2.weight.data=self.layer1_0_bn2_init_weight + self.quant(W-self.layer1_0_bn2_init_weight)\n",
        "        # B=self.model.layer1[0].bn2.bias.data\n",
        "        # self.model.layer1[0].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer1[0].bn2.bias.data=self.layer1_0_bn2_init_bias + self.quant(W-self.layer1_0_bn2_init_bias)\n",
        "        x=self.model.layer1[0].bn2(zero_relu(x))\n",
        "        W=self.model.layer1[0].conv2.weight.data\n",
        "        # self.model.layer1[0].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer1[0].conv2.weight.data=self.layer1_0_conv2_init_weight + self.quant(W-self.layer1_0_conv2_init_weight )\n",
        "        x=self.model.layer1[0].conv2(x)\n",
        "        x11=x+identity\n",
        "        # W=self.model.layer1[1].bn1.weight.data\n",
        "        # self.model.layer1[1].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer1[1].bn1.weight.data=self.layer1_1_bn1_init_weight + self.quant(W-self.layer1_1_bn1_init_weight)\n",
        "        # B=self.model.layer1[1].bn1.bias.data\n",
        "        # self.model.layer1[1].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer1[1].bn1.bias.data=self.layer1_1_bn1_init_bias + self.quant(B-self.layer1_1_bn1_init_bias)\n",
        "        x=self.model.layer1[1].bn1(zero_relu(x11))\n",
        "        shortcut=x11\n",
        "        W=self.model.layer1[1].conv1.weight.data\n",
        "        # self.model.layer1[1].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer1[1].conv1.weight.data=self.layer1_1_conv1_init_weight + self.quant(W-self.layer1_1_conv1_init_weight )\n",
        "        x=self.model.layer1[1].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer1[1].bn2.weight.data\n",
        "        # self.model.layer1[1].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer1[1].bn2.weight.data=self.layer1_1_bn2_init_weight + self.quant(W-self.layer1_1_bn2_init_weight)\n",
        "        # B=self.model.layer1[1].bn2.bias.data\n",
        "        # self.model.layer1[1].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer1[1].bn2.bias.data=self.layer1_1_bn2_init_bias + self.quant(W-self.layer1_1_bn2_init_bias)\n",
        "        x=self.model.layer1[1].bn2(zero_relu(x))\n",
        "        W=self.model.layer1[1].conv2.weight.data\n",
        "        # self.model.layer1[1].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer1[1].conv2.weight.data=self.layer1_1_conv2_init_weight + self.quant(W-self.layer1_1_conv2_init_weight )\n",
        "        x=self.model.layer1[1].conv2(x)\n",
        "        x20=x+identity\n",
        "    \n",
        "        # # # #layer 2\n",
        "        # W=self.model.layer2[0].bn1.weight.data\n",
        "        # self.model.layer2[0].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer2[0].bn1.weight.data=self.layer2_0_bn1_init_weight + self.quant(W-self.layer2_0_bn1_init_weight)\n",
        "        # B=self.model.layer2[0].bn1.bias.data\n",
        "        # self.model.layer2[0].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer2[0].bn1.bias.data=self.layer2_0_bn1_init_bias + self.quant(B-self.layer2_0_bn1_init_bias)\n",
        "        x=self.model.layer2[0].bn1(zero_relu(x20))\n",
        "        W=self.model.layer2[0].shortcut[0].weight.data\n",
        "        # self.model.layer2[0].shortcut[0].weight.data=self.quant(W)\n",
        "        self.model.layer2[0].shortcut[0].weight.data=self.layer2_0_shortcut_0_init_weight +  self.quant(W-self.layer2_0_shortcut_0_init_weight)\n",
        "        shortcut=self.model.layer2[0].shortcut[0](x)\n",
        "        shortcut=self.model.layer2[0].shortcut[1](shortcut)\n",
        "        W=self.model.layer2[0].conv1.weight.data\n",
        "        # self.model.layer2[0].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer2[0].conv1.weight.data=self.layer2_0_conv1_init_weight + self.quant(W-self.layer2_0_conv1_init_weight )\n",
        "        x=self.model.layer2[0].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer2[0].bn2.weight.data\n",
        "        # self.model.layer2[0].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer2[0].bn2.weight.data=self.layer2_0_bn2_init_weight + self.quant(W-self.layer2_0_bn2_init_weight)\n",
        "        # B=self.model.layer2[0].bn2.bias.data\n",
        "        # self.model.layer2[0].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer2[0].bn2.bias.data=self.layer2_0_bn2_init_bias + self.quant(W-self.layer2_0_bn2_init_bias)\n",
        "        x=self.model.layer2[0].bn2(zero_relu(x))\n",
        "        W=self.model.layer2[0].conv2.weight.data\n",
        "        # self.model.layer2[0].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer2[0].conv2.weight.data=self.layer2_0_conv2_init_weight + self.quant(W-self.layer2_0_conv2_init_weight )\n",
        "        x=self.model.layer2[0].conv2(x)\n",
        "        x21=x+identity\n",
        "        # W=self.model.layer2[1].bn1.weight.data\n",
        "        # self.model.layer2[1].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer2[1].bn1.weight.data=self.layer2_bn1_init_weight + self.quant(W-self.layer2_1_bn1_init_weight)\n",
        "        # B=self.model.layer2[1].bn1.bias.data\n",
        "        # self.model.layer2[1].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer2[1].bn1.bias.data=self.layer2_1_bn1_init_bias + self.quant(B-self.layer2_1_bn1_init_bias)\n",
        "        x=self.model.layer2[1].bn1(zero_relu(x21))\n",
        "        shortcut=x21\n",
        "        W=self.model.layer2[1].conv1.weight.data\n",
        "        # self.model.layer2[1].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer2[1].conv1.weight.data=self.layer2_1_conv1_init_weight + self.quant(W-self.layer2_1_conv1_init_weight )\n",
        "        x=self.model.layer2[1].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer2[1].bn2.weight.data\n",
        "        # self.model.layer2[1].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer2[1].bn2.weight.data=self.layer2_1_bn2_init_weight + self.quant(W-self.layer2_1_bn2_init_weight)\n",
        "        # B=self.model.layer2[1].bn2.bias.data\n",
        "        # self.model.layer2[1].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer2[1].bn2.bias.data=self.layer2_1_bn2_init_bias + self.quant(W-self.layer2_1_bn2_init_bias)\n",
        "        x=self.model.layer2[1].bn2(zero_relu(x))\n",
        "        W=self.model.layer2[1].conv2.weight.data\n",
        "        # self.model.layer2[1].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer2[1].conv2.weight.data=self.layer2_1_conv2_init_weight + self.quant(W-self.layer2_1_conv2_init_weight )\n",
        "        x=self.model.layer2[1].conv2(x)\n",
        "        x30=x+identity\n",
        "\n",
        "        # # #layer 3\n",
        "        # W=self.model.layer3[0].bn1.weight.data\n",
        "        # self.model.layer3[0].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer3[0].bn1.weight.data=self.layer3_0_bn1_init_weight + self.quant(W-self.layer3_0_bn1_init_weight)\n",
        "        # B=self.model.layer3[0].bn1.bias.data\n",
        "        # self.model.layer3[0].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer3[0].bn1.bias.data=self.layer3_0_bn1_init_bias + self.quant(B-self.layer3_0_bn1_init_bias)\n",
        "        x=self.model.layer3[0].bn1(zero_relu(x30))\n",
        "        W=self.model.layer3[0].shortcut[0].weight.data\n",
        "        # self.model.layer3[0].shortcut[0].weight.data=self.quant(W)\n",
        "        self.model.layer3[0].shortcut[0].weight.data=self.layer3_0_shortcut_0_init_weight +  self.quant(W-self.layer3_0_shortcut_0_init_weight)\n",
        "        shortcut=self.model.layer3[0].shortcut[0](x)\n",
        "        shortcut=self.model.layer3[0].shortcut[1](shortcut)\n",
        "        W=self.model.layer3[0].conv1.weight.data\n",
        "        # self.model.layer3[0].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer3[0].conv1.weight.data=self.layer3_0_conv1_init_weight + self.quant(W-self.layer3_0_conv1_init_weight )\n",
        "        x=self.model.layer3[0].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer3[0].bn2.weight.data\n",
        "        # self.model.layer3[0].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer3[0].bn2.weight.data=self.layer3_0_bn2_init_weight + self.quant(W-self.layer3_0_bn2_init_weight)\n",
        "        # B=self.model.layer3[0].bn2.bias.data\n",
        "        # self.model.layer3[0].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer3[0].bn2.bias.data=self.layer3_0_bn2_init_bias + self.quant(W-self.layer3_0_bn2_init_bias)\n",
        "        x=self.model.layer3[0].bn2(zero_relu(x))\n",
        "        W=self.model.layer3[0].conv2.weight.data\n",
        "        # self.model.layer3[0].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer3[0].conv2.weight.data=self.layer3_0_conv2_init_weight + self.quant(W-self.layer3_0_conv2_init_weight )\n",
        "        x=self.model.layer3[0].conv2(x)\n",
        "        x31=x+identity\n",
        "\n",
        "        # W=self.model.layer3[1].bn1.weight.data\n",
        "        # self.model.layer3[1].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer3[1].bn1.weight.data=self.layer3_bn1_init_weight + self.quant(W-self.layer3_1_bn1_init_weight)\n",
        "        # B=self.model.layer3[1].bn1.bias.data\n",
        "        # self.model.layer3[1].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer3[1].bn1.bias.data=self.layer3_1_bn1_init_bias + self.quant(B-self.layer3_1_bn1_init_bias)\n",
        "        x=self.model.layer3[1].bn1(zero_relu(x31))\n",
        "        shortcut=x31\n",
        "        W=self.model.layer3[1].conv1.weight.data\n",
        "        # self.model.layer3[1].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer3[1].conv1.weight.data=self.layer3_1_conv1_init_weight + self.quant(W-self.layer3_1_conv1_init_weight )\n",
        "        x=self.model.layer3[1].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer3[1].bn2.weight.data\n",
        "        # self.model.layer3[1].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer3[1].bn2.weight.data=self.layer3_1_bn2_init_weight + self.quant(W-self.layer3_1_bn2_init_weight)\n",
        "        # B=self.model.layer3[1].bn2.bias.data\n",
        "        # self.model.layer3[1].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer3[1].bn2.bias.data=self.layer3_1_bn2_init_bias + self.quant(W-self.layer3_1_bn2_init_bias)\n",
        "        x=self.model.layer3[1].bn2(zero_relu(x))\n",
        "        W=self.model.layer3[1].conv2.weight.data\n",
        "        # self.model.layer3[1].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer3[1].conv2.weight.data=self.layer3_1_conv2_init_weight + self.quant(W-self.layer3_1_conv2_init_weight )\n",
        "        x=self.model.layer3[1].conv2(x)\n",
        "        x40=x+identity\n",
        "\n",
        "        #layer 4\n",
        "        # W=self.model.layer4[0].bn1.weight.data\n",
        "        # self.model.layer4[0].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer4[0].bn1.weight.data=self.layer4_0_bn1_init_weight + self.quant(W-self.layer4_0_bn1_init_weight)\n",
        "        # B=self.model.layer4[0].bn1.bias.data\n",
        "        # self.model.layer4[0].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer4[0].bn1.bias.data=self.layer4_0_bn1_init_bias + self.quant(B-self.layer4_0_bn1_init_bias)\n",
        "        x=self.model.layer4[0].bn1(zero_relu(x40))\n",
        "        W=self.model.layer4[0].shortcut[0].weight.data\n",
        "        # self.model.layer4[0].shortcut[0].weight.data=self.quant(W)\n",
        "        self.model.layer4[0].shortcut[0].weight.data=self.layer4_0_shortcut_0_init_weight +  self.quant(W-self.layer4_0_shortcut_0_init_weight)\n",
        "        shortcut=self.model.layer4[0].shortcut[0](x)\n",
        "        shortcut=self.model.layer4[0].shortcut[1](shortcut)\n",
        "        W=self.model.layer4[0].conv1.weight.data\n",
        "        # self.model.layer4[0].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer4[0].conv1.weight.data=self.layer4_0_conv1_init_weight + self.quant(W-self.layer4_0_conv1_init_weight )\n",
        "        x=self.model.layer4[0].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer4[0].bn2.weight.data\n",
        "        # self.model.layer4[0].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer4[0].bn2.weight.data=self.layer4_0_bn2_init_weight + self.quant(W-self.layer4_0_bn2_init_weight)\n",
        "        # B=self.model.layer4[0].bn2.bias.data\n",
        "        # self.model.layer4[0].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer4[0].bn2.bias.data=self.layer4_0_bn2_init_bias + self.quant(W-self.layer4_0_bn2_init_bias)\n",
        "        x=self.model.layer4[0].bn2(zero_relu(x))\n",
        "        W=self.model.layer4[0].conv2.weight.data\n",
        "        # self.model.layer4[0].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer4[0].conv2.weight.data=self.layer4_0_conv2_init_weight + self.quant(W-self.layer4_0_conv2_init_weight )\n",
        "        x=self.model.layer4[0].conv2(x)\n",
        "        x41=x+identity\n",
        "\n",
        "        # W=self.model.layer4[1].bn1.weight.data\n",
        "        # self.model.layer4[1].bn1.weight.data=self.quant(W)\n",
        "        # self.model.layer4[1].bn1.weight.data=self.layer4_bn1_init_weight + self.quant(W-self.layer4_1_bn1_init_weight)\n",
        "        # B=self.model.layer4[1].bn1.bias.data\n",
        "        # self.model.layer4[1].bn1.bias.data=self.quant(B)\n",
        "        # self.model.layer4[1].bn1.bias.data=self.layer4_1_bn1_init_bias + self.quant(B-self.layer4_1_bn1_init_bias)\n",
        "        x=self.model.layer4[1].bn1(zero_relu(x41))\n",
        "        shortcut=x41\n",
        "        W=self.model.layer4[1].conv1.weight.data\n",
        "        # self.model.layer4[1].conv1.weight.data=self.quant(W)\n",
        "        self.model.layer4[1].conv1.weight.data=self.layer4_1_conv1_init_weight + self.quant(W-self.layer4_1_conv1_init_weight )\n",
        "        x=self.model.layer4[1].conv1(x)\n",
        "        x=x+shortcut\n",
        "        identity=x\n",
        "        # W=self.model.layer4[1].bn2.weight.data\n",
        "        # self.model.layer4[1].bn2.weight.data=self.quant(W)\n",
        "        # self.model.layer4[1].bn2.weight.data=self.layer4_1_bn2_init_weight + self.quant(W-self.layer4_1_bn2_init_weight)\n",
        "        # B=self.model.layer4[1].bn2.bias.data\n",
        "        # self.model.layer4[1].bn2.bias.data=self.quant(B)\n",
        "        # self.model.layer4[1].bn2.bias.data=self.layer4_1_bn2_init_bias + self.quant(W-self.layer4_1_bn2_init_bias)\n",
        "        x=self.model.layer4[1].bn2(zero_relu(x))\n",
        "        W=self.model.layer4[1].conv2.weight.data\n",
        "        # self.model.layer4[1].conv2.weight.data=self.quant(W)\n",
        "        self.model.layer4[1].conv2.weight.data=self.layer4_1_conv2_init_weight + self.quant(W-self.layer4_1_conv2_init_weight )\n",
        "        x=self.model.layer4[1].conv2(x)\n",
        "        x=x+identity\n",
        "\n",
        "\n",
        "        x=zero_relu(x)\n",
        "        # W=self.model.bn2.weight.data\n",
        "        # self.model.bn2.weight.data=self.quant(W)\n",
        "        # self.model.bn2.weight.data=self.bn2_init_weight+self.quant(W-self.bn2_init_weight)\n",
        "        # B=self.model.bn2.bias.data\n",
        "        # self.model.bn2.bias.data=self.quant(B)\n",
        "        # self.model.bn2.bias.data=self.bn2_init_bias+self.quant(B-self.bn2_init_bias)\n",
        "        x=self.model.bn2(x)\n",
        "\n",
        "        # x=self.quant(x)\n",
        "        x=F.avg_pool2d(x,4)\n",
        "        # x=self.quant(x)\n",
        "        x=x.view(x.size(0),-1)\n",
        "        W=self.model.linear.weight.data\n",
        "        # self.model.linear.weight.data=self.quant(W)\n",
        "        self.model.linear.weight.data=self.linear_init_weight+self.quant(W-self.linear_init_weight)\n",
        "        B=self.model.linear.bias.data\n",
        "        # self.model.linear.bias.data=self.quant(B) \n",
        "        self.model.linear.bias.data=self.linear_init_bias+self.quant(B-self.linear_init_bias)\n",
        "        x=self.model.linear(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsef0L-Yeu4Q"
      },
      "source": [
        "# **Building quantized resnet model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U_xIFHlpznQ",
        "outputId": "16048054-6293-49d6-ea3a-92461a8d9dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sub identity init a conv1x1\n",
            "sub identity init a conv1x1\n",
            "sub identity init a conv1x1\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "net_quant=copy.deepcopy(identity_resnet18())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM_ouz3YceH3",
        "outputId": "04c9a5c8-f4dd-46d4-927a-439408f0925e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Building quantized model..\n",
            "tensor([[[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]]])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "tensor([[[[1.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]]],\n",
            "\n",
            "\n",
            "        [[[0.]],\n",
            "\n",
            "         [[1.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]]],\n",
            "\n",
            "\n",
            "        [[[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]]],\n",
            "\n",
            "\n",
            "        [[[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]]],\n",
            "\n",
            "\n",
            "        [[[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]],\n",
            "\n",
            "         [[0.]]]])\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "\n",
            "Epoch: 0\n",
            " [================================================================>]  Step: 36ms | Tot: 13s526ms | Loss: 1.660 | Acc: 38.426% (19213/5000 391/391 ........................................................]  Step: 41ms | Tot: 497ms | Loss: 2.283 | Acc: 16.504% (169/102 8/391 ========================>........................................]  Step: 50ms | Tot: 5s380ms | Loss: 1.827 | Acc: 31.617% (6030/1907 149/391 ================================>................................]  Step: 39ms | Tot: 7s224ms | Loss: 1.816 | Acc: 32.473% (8230/2534 198/391 =============================================>...................]  Step: 30ms | Tot: 9s868ms | Loss: 1.753 | Acc: 34.832% (12261/3520 275/391 \n",
            "Train Loss: 1.6604007122766635 Acc:  38.426 correct 19213 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s821ms | Loss: 1.343 | Acc: 51.320% (5132/1000 100/100 =========================>.....................................]  Step: 28ms | Tot: 1s467ms | Loss: 1.346 | Acc: 51.636% (2272/440 44/100 \n",
            "Test Loss: 1.3427746844291688 Acc: 51.32 Correct 5132 total 10000\n",
            "51.32\n",
            "\n",
            "Epoch: 1\n",
            " [================================================================>]  Step: 29ms | Tot: 13s353ms | Loss: 1.249 | Acc: 54.216% (27108/5000 391/391 .........................................................]  Step: 31ms | Tot: 482ms | Loss: 1.271 | Acc: 52.246% (535/102 8/391 \n",
            "Train Loss: 1.2493497592104061 Acc:  54.216 correct 27108 total 50000\n",
            " [================================================================>]  Step: 16ms | Tot: 2s537ms | Loss: 1.212 | Acc: 56.410% (5641/1000 100/100 \n",
            "Test Loss: 1.2124376845359803 Acc: 56.41 Correct 5641 total 10000\n",
            "56.41\n",
            "\n",
            "Epoch: 2\n",
            " [================================================================>]  Step: 30ms | Tot: 13s98ms | Loss: 1.090 | Acc: 60.472% (30236/5000 391/391  ===================>...........................................]  Step: 34ms | Tot: 4s640ms | Loss: 1.132 | Acc: 58.528% (9739/1664 130/391 ==========================>......................................]  Step: 37ms | Tot: 5s670ms | Loss: 1.124 | Acc: 58.901% (12063/2048 160/391 =================================================>...............]  Step: 33ms | Tot: 10s85ms | Loss: 1.101 | Acc: 59.890% (22691/3788 296/391 \n",
            "Train Loss: 1.089716223194776 Acc:  60.472 correct 30236 total 50000\n",
            " [================================================================>]  Step: 23ms | Tot: 2s616ms | Loss: 1.023 | Acc: 63.020% (6302/1000 100/100 =======================================================>.......]  Step: 32ms | Tot: 2s239ms | Loss: 1.028 | Acc: 62.978% (5668/900 90/100 \n",
            "Test Loss: 1.0234071606397628 Acc: 63.02 Correct 6302 total 10000\n",
            "63.02\n",
            "\n",
            "Epoch: 3\n",
            " [================================================================>]  Step: 33ms | Tot: 13s161ms | Loss: 0.980 | Acc: 64.820% (32410/5000 391/391 \n",
            "Train Loss: 0.9801709106206284 Acc:  64.82 correct 32410 total 50000\n",
            " [================================================================>]  Step: 25ms | Tot: 2s690ms | Loss: 1.067 | Acc: 62.210% (6221/1000 100/100 \n",
            "Test Loss: 1.0667430806159972 Acc: 62.21 Correct 6221 total 10000\n",
            "63.02\n",
            "\n",
            "Epoch: 4\n",
            " [================================================================>]  Step: 35ms | Tot: 12s852ms | Loss: 0.910 | Acc: 67.466% (33733/5000 391/391 ============>.................................................]  Step: 33ms | Tot: 3s404ms | Loss: 0.945 | Acc: 65.914% (7762/1177 92/391 ===============>.................................................]  Step: 34ms | Tot: 3s438ms | Loss: 0.945 | Acc: 65.894% (7844/1190 93/391 \n",
            "Train Loss: 0.9100989089597522 Acc:  67.466 correct 33733 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s706ms | Loss: 0.880 | Acc: 68.480% (6848/1000 100/100 =========================================================>.....]  Step: 20ms | Tot: 2s557ms | Loss: 0.880 | Acc: 68.452% (6366/930 93/100 \n",
            "Test Loss: 0.8804268699884414 Acc: 68.48 Correct 6848 total 10000\n",
            "68.48\n",
            "\n",
            "Epoch: 5\n",
            " [================================================================>]  Step: 38ms | Tot: 13s327ms | Loss: 0.853 | Acc: 70.004% (35002/5000 391/391 ...........................................................]  Step: 32ms | Tot: 585ms | Loss: 0.860 | Acc: 70.373% (1171/166 13/391 ========>........................................................]  Step: 33ms | Tot: 1s890ms | Loss: 0.869 | Acc: 69.688% (4460/640 50/391 ===============>.................................................]  Step: 33ms | Tot: 3s284ms | Loss: 0.859 | Acc: 69.676% (8205/1177 92/391 ===========================>.....................................]  Step: 34ms | Tot: 5s766ms | Loss: 0.864 | Acc: 69.583% (14785/2124 166/391 ============================================>....................]  Step: 32ms | Tot: 9s234ms | Loss: 0.859 | Acc: 69.803% (24124/3456 270/391 ============================================================>....]  Step: 34ms | Tot: 12s327ms | Loss: 0.854 | Acc: 69.985% (32518/4646 363/391 ============================================================>....]  Step: 33ms | Tot: 12s361ms | Loss: 0.854 | Acc: 69.986% (32608/4659 364/391 \n",
            "Train Loss: 0.8527312844305697 Acc:  70.004 correct 35002 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s480ms | Loss: 1.020 | Acc: 66.360% (6636/1000 100/100 ==>.........................................................]  Step: 24ms | Tot: 501ms | Loss: 1.022 | Acc: 65.538% (852/130 13/100 =========>.......................................................]  Step: 27ms | Tot: 554ms | Loss: 1.012 | Acc: 65.533% (983/150 15/100 \n",
            "Test Loss: 1.0198120617866515 Acc: 66.36 Correct 6636 total 10000\n",
            "68.48\n",
            "\n",
            "Epoch: 6\n",
            " [================================================================>]  Step: 28ms | Tot: 13s295ms | Loss: 0.803 | Acc: 71.592% (35796/5000 391/391 ==>..........................................................]  Step: 34ms | Tot: 1s653ms | Loss: 0.821 | Acc: 70.846% (3718/524 41/391 ===================================>.............................]  Step: 33ms | Tot: 7s364ms | Loss: 0.820 | Acc: 70.799% (19212/2713 212/391 ======================================>..........................]  Step: 33ms | Tot: 8s26ms | Loss: 0.817 | Acc: 70.935% (21065/2969 232/391 \n",
            "Train Loss: 0.8027902443695556 Acc:  71.592 correct 35796 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s306ms | Loss: 0.958 | Acc: 68.080% (6808/1000 100/100 ==>.........................................................]  Step: 21ms | Tot: 483ms | Loss: 0.914 | Acc: 69.667% (836/120 12/100 ==========================>......................................]  Step: 20ms | Tot: 1s59ms | Loss: 0.949 | Acc: 67.857% (2850/420 42/100 \n",
            "Test Loss: 0.9584135138988494 Acc: 68.08 Correct 6808 total 10000\n",
            "68.48\n",
            "\n",
            "Epoch: 7\n",
            " [================================================================>]  Step: 27ms | Tot: 13s204ms | Loss: 0.737 | Acc: 74.086% (37043/5000 391/391 ===>.........................................................]  Step: 58ms | Tot: 1s838ms | Loss: 0.751 | Acc: 73.998% (4357/588 46/391 ================================>................................]  Step: 28ms | Tot: 7s302ms | Loss: 0.744 | Acc: 73.912% (18543/2508 196/391 \n",
            "Train Loss: 0.7367624927054891 Acc:  74.086 correct 37043 total 50000\n",
            " [================================================================>]  Step: 26ms | Tot: 2s277ms | Loss: 0.752 | Acc: 73.970% (7397/1000 100/100 ===========================================>...................]  Step: 20ms | Tot: 1s671ms | Loss: 0.759 | Acc: 73.690% (5232/710 71/100 \n",
            "Test Loss: 0.7518292486667633 Acc: 73.97 Correct 7397 total 10000\n",
            "73.97\n",
            "\n",
            "Epoch: 8\n",
            " [================================================================>]  Step: 34ms | Tot: 13s558ms | Loss: 0.689 | Acc: 75.944% (37972/5000 391/391 ============>.................................................]  Step: 34ms | Tot: 3s362ms | Loss: 0.693 | Acc: 75.739% (8919/1177 92/391 ================>................................................]  Step: 32ms | Tot: 3s596ms | Loss: 0.695 | Acc: 75.560% (9575/1267 99/391 =======================>.........................................]  Step: 32ms | Tot: 4s974ms | Loss: 0.696 | Acc: 75.513% (13532/1792 140/391 \n",
            "Train Loss: 0.6887085062006245 Acc:  75.944 correct 37972 total 50000\n",
            " [================================================================>]  Step: 16ms | Tot: 2s127ms | Loss: 0.933 | Acc: 69.130% (6913/1000 100/100 \n",
            "Test Loss: 0.9326690900325775 Acc: 69.13 Correct 6913 total 10000\n",
            "73.97\n",
            "\n",
            "Epoch: 9\n",
            " [================================================================>]  Step: 30ms | Tot: 13s | Loss: 0.654 | Acc: 77.452% (38726/5000 391/391 /391 ===================================================>.............]  Step: 32ms | Tot: 10s379ms | Loss: 0.656 | Acc: 77.404% (30912/3993 312/391 \n",
            "Train Loss: 0.6536042285544793 Acc:  77.452 correct 38726 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s195ms | Loss: 0.989 | Acc: 68.930% (6893/1000 100/100 ==================>..........................................]  Step: 17ms | Tot: 943ms | Loss: 1.007 | Acc: 68.371% (2393/350 35/100 \n",
            "Test Loss: 0.9887309259176255 Acc: 68.93 Correct 6893 total 10000\n",
            "73.97\n",
            "\n",
            "Epoch: 10\n",
            " [================================================================>]  Step: 31ms | Tot: 13s769ms | Loss: 0.626 | Acc: 78.546% (39273/5000 391/391 =============================>..................................]  Step: 34ms | Tot: 6s641ms | Loss: 0.636 | Acc: 78.041% (18580/2380 186/391 \n",
            "Train Loss: 0.6260547554096603 Acc:  78.546 correct 39273 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s542ms | Loss: 0.881 | Acc: 70.990% (7099/1000 100/100 \n",
            "Test Loss: 0.8814762860536576 Acc: 70.99 Correct 7099 total 10000\n",
            "73.97\n",
            "\n",
            "Epoch: 11\n",
            " [================================================================>]  Step: 33ms | Tot: 13s443ms | Loss: 0.606 | Acc: 78.900% (39450/5000 391/391 ...........................................................]  Step: 32ms | Tot: 730ms | Loss: 0.612 | Acc: 78.320% (1604/204 16/391 ==================>..............................................]  Step: 34ms | Tot: 4s139ms | Loss: 0.596 | Acc: 79.379% (11583/1459 114/391 ====================>............................................]  Step: 33ms | Tot: 4s480ms | Loss: 0.597 | Acc: 79.291% (12585/1587 124/391 =============================>...................................]  Step: 33ms | Tot: 6s252ms | Loss: 0.603 | Acc: 79.118% (17925/2265 177/391 =======================================>.........................]  Step: 34ms | Tot: 8s399ms | Loss: 0.600 | Acc: 79.214% (24233/3059 239/391 ================================================>................]  Step: 34ms | Tot: 10s297ms | Loss: 0.602 | Acc: 79.071% (29756/3763 294/391 ====================================================>............]  Step: 33ms | Tot: 11s94ms | Loss: 0.605 | Acc: 78.975% (32045/4057 317/391 =======================================================>.........]  Step: 32ms | Tot: 11s645ms | Loss: 0.606 | Acc: 78.927% (33642/4262 333/391 \n",
            "Train Loss: 0.6056251288070094 Acc:  78.9 correct 39450 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s474ms | Loss: 0.724 | Acc: 74.920% (7492/1000 100/100 \n",
            "Test Loss: 0.7243846762180328 Acc: 74.92 Correct 7492 total 10000\n",
            "74.92\n",
            "\n",
            "Epoch: 12\n",
            " [================================================================>]  Step: 25ms | Tot: 14s409ms | Loss: 0.583 | Acc: 79.892% (39946/5000 391/391 \n",
            "Train Loss: 0.5827516417978974 Acc:  79.892 correct 39946 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s186ms | Loss: 0.620 | Acc: 78.290% (7829/1000 100/100 \n",
            "Test Loss: 0.6202573871612549 Acc: 78.29 Correct 7829 total 10000\n",
            "78.29\n",
            "\n",
            "Epoch: 13\n",
            " [================================================================>]  Step: 24ms | Tot: 14s204ms | Loss: 0.563 | Acc: 80.812% (40406/5000 391/391 =======================================================>.........]  Step: 33ms | Tot: 12s284ms | Loss: 0.565 | Acc: 80.788% (34642/4288 335/391 ============================================================>....]  Step: 32ms | Tot: 13s286ms | Loss: 0.564 | Acc: 80.839% (37768/4672 365/391 \n",
            "Train Loss: 0.5626319514211181 Acc:  80.812 correct 40406 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s131ms | Loss: 0.693 | Acc: 76.920% (7692/1000 100/100 \n",
            "Test Loss: 0.6929243087768555 Acc: 76.92 Correct 7692 total 10000\n",
            "78.29\n",
            "\n",
            "Epoch: 14\n",
            " [================================================================>]  Step: 32ms | Tot: 15s217ms | Loss: 0.551 | Acc: 81.086% (40543/5000 391/391 =======================>........................................]  Step: 40ms | Tot: 5s608ms | Loss: 0.541 | Acc: 81.501% (15231/1868 146/391 ========================================>........................]  Step: 39ms | Tot: 9s491ms | Loss: 0.550 | Acc: 81.192% (25150/3097 242/391 ====================================================>............]  Step: 32ms | Tot: 12s467ms | Loss: 0.547 | Acc: 81.240% (33172/4083 319/391 \n",
            "Train Loss: 0.5505021944680177 Acc:  81.086 correct 40543 total 50000\n",
            " [================================================================>]  Step: 17ms | Tot: 2s439ms | Loss: 0.675 | Acc: 77.560% (7756/1000 100/100 ==================================================>............]  Step: 32ms | Tot: 2s35ms | Loss: 0.679 | Acc: 77.556% (6282/810 81/100 \n",
            "Test Loss: 0.6748928591609001 Acc: 77.56 Correct 7756 total 10000\n",
            "78.29\n",
            "\n",
            "Epoch: 15\n",
            " [================================================================>]  Step: 29ms | Tot: 14s280ms | Loss: 0.537 | Acc: 81.584% (40792/5000 391/391 =====>.......................................................]  Step: 39ms | Tot: 2s259ms | Loss: 0.514 | Acc: 82.296% (5899/716 56/391 =======================================================>.........]  Step: 35ms | Tot: 12s168ms | Loss: 0.538 | Acc: 81.556% (34867/4275 334/391 \n",
            "Train Loss: 0.5374312315450605 Acc:  81.584 correct 40792 total 50000\n",
            " [================================================================>]  Step: 26ms | Tot: 2s378ms | Loss: 0.779 | Acc: 74.430% (7443/1000 100/100 \n",
            "Test Loss: 0.7791624194383622 Acc: 74.43 Correct 7443 total 10000\n",
            "78.29\n",
            "\n",
            "Epoch: 16\n",
            " [================================================================>]  Step: 41ms | Tot: 15s497ms | Loss: 0.521 | Acc: 82.090% (41045/5000 391/391 ======>......................................................]  Step: 38ms | Tot: 2s899ms | Loss: 0.509 | Acc: 82.533% (7078/857 67/391 ====================================>............................]  Step: 32ms | Tot: 8s955ms | Loss: 0.520 | Acc: 82.028% (23414/2854 223/391 =====================================>...........................]  Step: 33ms | Tot: 8s989ms | Loss: 0.520 | Acc: 82.038% (23522/2867 224/391 ======================================>..........................]  Step: 34ms | Tot: 9s360ms | Loss: 0.520 | Acc: 82.038% (24677/3008 235/391 \n",
            "Train Loss: 0.521186471518958 Acc:  82.09 correct 41045 total 50000\n",
            " [================================================================>]  Step: 21ms | Tot: 2s606ms | Loss: 0.698 | Acc: 78.180% (7818/1000 100/100 \n",
            "Test Loss: 0.6978189790248871 Acc: 78.18 Correct 7818 total 10000\n",
            "78.29\n",
            "\n",
            "Epoch: 17\n",
            " [================================================================>]  Step: 32ms | Tot: 13s565ms | Loss: 0.511 | Acc: 82.530% (41265/5000 391/391 ...........................................................]  Step: 34ms | Tot: 879ms | Loss: 0.510 | Acc: 82.201% (2420/294 23/391 \n",
            "Train Loss: 0.5113761489805968 Acc:  82.53 correct 41265 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s615ms | Loss: 0.678 | Acc: 78.280% (7828/1000 100/100 ==================================>............................]  Step: 20ms | Tot: 1s700ms | Loss: 0.687 | Acc: 78.070% (4450/570 57/100 \n",
            "Test Loss: 0.677574722468853 Acc: 78.28 Correct 7828 total 10000\n",
            "78.29\n",
            "\n",
            "Epoch: 18\n",
            " [================================================================>]  Step: 25ms | Tot: 14s489ms | Loss: 0.510 | Acc: 82.412% (41206/5000 391/391 .........................................................]  Step: 39ms | Tot: 463ms | Loss: 0.460 | Acc: 83.496% (855/102 8/391 ==============>..................................................]  Step: 42ms | Tot: 3s243ms | Loss: 0.495 | Acc: 82.777% (9218/1113 87/391 ==================>..............................................]  Step: 34ms | Tot: 4s117ms | Loss: 0.497 | Acc: 82.714% (11752/1420 111/391 ===============================================>.................]  Step: 35ms | Tot: 10s478ms | Loss: 0.507 | Acc: 82.444% (30181/3660 286/391 \n",
            "Train Loss: 0.509796184956875 Acc:  82.412 correct 41206 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s580ms | Loss: 0.576 | Acc: 80.060% (8006/1000 100/100 ==================================================>............]  Step: 20ms | Tot: 2s147ms | Loss: 0.574 | Acc: 80.000% (6480/810 81/100 =========================================================>.......]  Step: 22ms | Tot: 2s328ms | Loss: 0.578 | Acc: 79.966% (7117/890 89/100 \n",
            "Test Loss: 0.5757688271999359 Acc: 80.06 Correct 8006 total 10000\n",
            "80.06\n",
            "\n",
            "Epoch: 19\n",
            " [================================================================>]  Step: 29ms | Tot: 14s737ms | Loss: 0.488 | Acc: 83.122% (41561/5000 391/391 \n",
            "Train Loss: 0.48815538648449247 Acc:  83.122 correct 41561 total 50000\n",
            " [================================================================>]  Step: 21ms | Tot: 2s677ms | Loss: 0.651 | Acc: 78.460% (7846/1000 100/100 ===========================>...................................]  Step: 22ms | Tot: 1s479ms | Loss: 0.645 | Acc: 78.935% (3631/460 46/100 \n",
            "Test Loss: 0.6506684717535972 Acc: 78.46 Correct 7846 total 10000\n",
            "80.06\n",
            "\n",
            "Epoch: 20\n",
            " [================================================================>]  Step: 31ms | Tot: 14s905ms | Loss: 0.486 | Acc: 83.292% (41646/5000 391/391 ====>........................................................]  Step: 35ms | Tot: 2s339ms | Loss: 0.455 | Acc: 84.345% (5614/665 52/391 ==============>..................................................]  Step: 39ms | Tot: 3s808ms | Loss: 0.472 | Acc: 83.854% (9338/1113 87/391 ===============================>.................................]  Step: 35ms | Tot: 7s811ms | Loss: 0.478 | Acc: 83.412% (20179/2419 189/391 ==================================>..............................]  Step: 43ms | Tot: 8s530ms | Loss: 0.482 | Acc: 83.325% (22291/2675 209/391 ============================================================>....]  Step: 35ms | Tot: 13s928ms | Loss: 0.485 | Acc: 83.323% (39035/4684 366/391 ==============================================================>..]  Step: 37ms | Tot: 14s299ms | Loss: 0.485 | Acc: 83.334% (40107/4812 376/391 \n",
            "Train Loss: 0.485798297483293 Acc:  83.292 correct 41646 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s853ms | Loss: 0.768 | Acc: 74.060% (7406/1000 100/100 ======================>........................................]  Step: 19ms | Tot: 1s374ms | Loss: 0.778 | Acc: 73.718% (2875/390 39/100 \n",
            "Test Loss: 0.7684425711631775 Acc: 74.06 Correct 7406 total 10000\n",
            "80.06\n",
            "\n",
            "Epoch: 21\n",
            " [================================================================>]  Step: 25ms | Tot: 14s366ms | Loss: 0.486 | Acc: 83.236% (41618/5000 391/391 .........................................................]  Step: 37ms | Tot: 481ms | Loss: 0.503 | Acc: 82.366% (738/89 7/391 ============>....................................................]  Step: 29ms | Tot: 3s22ms | Loss: 0.476 | Acc: 83.820% (8154/972 76/391 =========================>.......................................]  Step: 43ms | Tot: 6s303ms | Loss: 0.474 | Acc: 83.854% (16744/1996 156/391 ===============================>.................................]  Step: 33ms | Tot: 7s559ms | Loss: 0.479 | Acc: 83.586% (20328/2432 190/391 =====================================>...........................]  Step: 35ms | Tot: 8s712ms | Loss: 0.482 | Acc: 83.531% (23950/2867 224/391 =======================================>.........................]  Step: 36ms | Tot: 9s288ms | Loss: 0.481 | Acc: 83.587% (25785/3084 241/391 ========================================>........................]  Step: 34ms | Tot: 9s322ms | Loss: 0.481 | Acc: 83.578% (25889/3097 242/391 \n",
            "Train Loss: 0.4862383923414723 Acc:  83.236 correct 41618 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s118ms | Loss: 0.670 | Acc: 77.730% (7773/1000 100/100 \n",
            "Test Loss: 0.6699461787939072 Acc: 77.73 Correct 7773 total 10000\n",
            "80.06\n",
            "\n",
            "Epoch: 22\n",
            " [================================================================>]  Step: 33ms | Tot: 14s801ms | Loss: 0.473 | Acc: 83.704% (41852/5000 391/391 ===============>................................................]  Step: 34ms | Tot: 4s160ms | Loss: 0.471 | Acc: 83.847% (10947/1305 102/391 ===============================>.................................]  Step: 38ms | Tot: 7s441ms | Loss: 0.472 | Acc: 83.903% (20620/2457 192/391 ===========================================================>.....]  Step: 35ms | Tot: 13s552ms | Loss: 0.473 | Acc: 83.727% (38260/4569 357/391 \n",
            "Train Loss: 0.4726352481281056 Acc:  83.704 correct 41852 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s288ms | Loss: 0.578 | Acc: 80.090% (8009/1000 100/100 \n",
            "Test Loss: 0.5781084403395653 Acc: 80.09 Correct 8009 total 10000\n",
            "80.09\n",
            "\n",
            "Epoch: 23\n",
            " [================================================================>]  Step: 31ms | Tot: 12s763ms | Loss: 0.466 | Acc: 84.046% (42023/5000 391/391 \n",
            "Train Loss: 0.4656645311876331 Acc:  84.046 correct 42023 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s303ms | Loss: 0.618 | Acc: 79.100% (7910/1000 100/100 ===================================>...........................]  Step: 20ms | Tot: 1s467ms | Loss: 0.605 | Acc: 79.569% (4615/580 58/100 ========================================>........................]  Step: 18ms | Tot: 1s564ms | Loss: 0.606 | Acc: 79.429% (5004/630 63/100 \n",
            "Test Loss: 0.6183953496813774 Acc: 79.1 Correct 7910 total 10000\n",
            "80.09\n",
            "\n",
            "Epoch: 24\n",
            " [================================================================>]  Step: 31ms | Tot: 13s285ms | Loss: 0.459 | Acc: 84.110% (42055/5000 391/391 \n",
            "Train Loss: 0.45923534249100845 Acc:  84.11 correct 42055 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s345ms | Loss: 0.621 | Acc: 78.730% (7873/1000 100/100 \n",
            "Test Loss: 0.6211071738600731 Acc: 78.73 Correct 7873 total 10000\n",
            "80.09\n",
            "\n",
            "Epoch: 25\n",
            " [================================================================>]  Step: 26ms | Tot: 13s621ms | Loss: 0.450 | Acc: 84.520% (42260/5000 391/391 \n",
            "Train Loss: 0.45008476189030405 Acc:  84.52 correct 42260 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s527ms | Loss: 0.497 | Acc: 83.300% (8330/1000 100/100 ==========>..................................................]  Step: 24ms | Tot: 708ms | Loss: 0.495 | Acc: 82.652% (1901/230 23/100 ===========================================>.....................]  Step: 22ms | Tot: 1s843ms | Loss: 0.494 | Acc: 83.221% (5659/680 68/100 ===========================================================>.....]  Step: 18ms | Tot: 2s391ms | Loss: 0.494 | Acc: 83.398% (7756/930 93/100 \n",
            "Test Loss: 0.49737251430749896 Acc: 83.3 Correct 8330 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 26\n",
            " [================================================================>]  Step: 32ms | Tot: 13s19ms | Loss: 0.449 | Acc: 84.578% (42289/5000 391/391  ...........................................................]  Step: 44ms | Tot: 816ms | Loss: 0.418 | Acc: 86.121% (1874/217 17/391 ==========================>......................................]  Step: 32ms | Tot: 5s687ms | Loss: 0.457 | Acc: 84.360% (17385/2060 161/391 ===============================>.................................]  Step: 32ms | Tot: 6s626ms | Loss: 0.453 | Acc: 84.445% (20429/2419 189/391 ==========================================>......................]  Step: 33ms | Tot: 8s868ms | Loss: 0.451 | Acc: 84.487% (27901/3302 258/391 \n",
            "Train Loss: 0.44919100358053243 Acc:  84.578 correct 42289 total 50000\n",
            " [================================================================>]  Step: 30ms | Tot: 2s486ms | Loss: 0.639 | Acc: 77.840% (7784/1000 100/100 =========================================================>.....]  Step: 22ms | Tot: 2s279ms | Loss: 0.639 | Acc: 77.804% (7158/920 92/100 \n",
            "Test Loss: 0.6389111286401749 Acc: 77.84 Correct 7784 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 27\n",
            " [================================================================>]  Step: 26ms | Tot: 13s821ms | Loss: 0.440 | Acc: 84.880% (42440/5000 391/391 \n",
            "Train Loss: 0.440318337982268 Acc:  84.88 correct 42440 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s110ms | Loss: 0.542 | Acc: 82.040% (8204/1000 100/100 \n",
            "Test Loss: 0.5415399667620658 Acc: 82.04 Correct 8204 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 28\n",
            " [================================================================>]  Step: 31ms | Tot: 13s260ms | Loss: 0.433 | Acc: 85.016% (42508/5000 391/391 ================>...............................................]  Step: 34ms | Tot: 3s446ms | Loss: 0.410 | Acc: 85.922% (11438/1331 104/391 \n",
            "Train Loss: 0.43276180974815204 Acc:  85.016 correct 42508 total 50000\n",
            " [================================================================>]  Step: 25ms | Tot: 2s650ms | Loss: 0.641 | Acc: 79.290% (7929/1000 100/100 \n",
            "Test Loss: 0.641311887204647 Acc: 79.29 Correct 7929 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 29\n",
            " [================================================================>]  Step: 32ms | Tot: 13s919ms | Loss: 0.435 | Acc: 85.156% (42578/5000 391/391 ===================================>............................]  Step: 30ms | Tot: 8s187ms | Loss: 0.434 | Acc: 85.146% (24086/2828 221/391 \n",
            "Train Loss: 0.4347324130480247 Acc:  85.156 correct 42578 total 50000\n",
            " [================================================================>]  Step: 16ms | Tot: 2s197ms | Loss: 0.624 | Acc: 79.490% (7949/1000 100/100 \n",
            "Test Loss: 0.6236972495913505 Acc: 79.49 Correct 7949 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 30\n",
            " [================================================================>]  Step: 27ms | Tot: 13s5ms | Loss: 0.431 | Acc: 85.030% (42515/5000 391/391 1 =====================================>..........................]  Step: 39ms | Tot: 7s902ms | Loss: 0.436 | Acc: 84.735% (25163/2969 232/391 \n",
            "Train Loss: 0.4307391189248361 Acc:  85.03 correct 42515 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s80ms | Loss: 0.530 | Acc: 81.750% (8175/1000 100/100 =======>......................................................]  Step: 21ms | Tot: 502ms | Loss: 0.510 | Acc: 82.765% (1407/170 17/100 \n",
            "Test Loss: 0.5303212159872055 Acc: 81.75 Correct 8175 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 31\n",
            " [================================================================>]  Step: 27ms | Tot: 13s203ms | Loss: 0.415 | Acc: 85.868% (42934/5000 391/391 =======================>........................................]  Step: 35ms | Tot: 5s445ms | Loss: 0.413 | Acc: 85.870% (16597/1932 151/391 =========================>.......................................]  Step: 32ms | Tot: 5s477ms | Loss: 0.414 | Acc: 85.840% (16701/1945 152/391 \n",
            "Train Loss: 0.41505662643391156 Acc:  85.868 correct 42934 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s198ms | Loss: 0.511 | Acc: 82.850% (8285/1000 100/100 \n",
            "Test Loss: 0.5111330440640449 Acc: 82.85 Correct 8285 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 32\n",
            " [================================================================>]  Step: 26ms | Tot: 12s999ms | Loss: 0.417 | Acc: 85.796% (42898/5000 391/391 \n",
            "Train Loss: 0.4169282343076623 Acc:  85.796 correct 42898 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s72ms | Loss: 0.740 | Acc: 76.440% (7644/1000 100/100 \n",
            "Test Loss: 0.7398509347438812 Acc: 76.44 Correct 7644 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 33\n",
            " [================================================================>]  Step: 28ms | Tot: 13s63ms | Loss: 0.415 | Acc: 85.788% (42894/5000 391/391  ============================================================>....]  Step: 34ms | Tot: 12s225ms | Loss: 0.413 | Acc: 85.891% (40238/4684 366/391 =============================================================>...]  Step: 33ms | Tot: 12s293ms | Loss: 0.413 | Acc: 85.912% (40468/4710 368/391 \n",
            "Train Loss: 0.4149940147652955 Acc:  85.788 correct 42894 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s62ms | Loss: 0.694 | Acc: 76.580% (7658/1000 100/100 ===================>..........................................]  Step: 17ms | Tot: 908ms | Loss: 0.703 | Acc: 76.361% (2749/360 36/100 \n",
            "Test Loss: 0.6936242231726646 Acc: 76.58 Correct 7658 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 34\n",
            " [================================================================>]  Step: 41ms | Tot: 13s388ms | Loss: 0.406 | Acc: 85.972% (42986/5000 391/391 ...........................................................]  Step: 38ms | Tot: 808ms | Loss: 0.394 | Acc: 86.627% (1885/217 17/391 =================================================>...............]  Step: 34ms | Tot: 10s106ms | Loss: 0.404 | Acc: 86.082% (32725/3801 297/391 \n",
            "Train Loss: 0.40569570241376873 Acc:  85.972 correct 42986 total 50000\n",
            " [================================================================>]  Step: 17ms | Tot: 2s29ms | Loss: 0.598 | Acc: 79.730% (7973/1000 100/100 \n",
            "Test Loss: 0.5984970676898956 Acc: 79.73 Correct 7973 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 35\n",
            " [================================================================>]  Step: 30ms | Tot: 13s481ms | Loss: 0.410 | Acc: 85.882% (42941/5000 391/391 ======================>.........................................]  Step: 32ms | Tot: 4s939ms | Loss: 0.423 | Acc: 85.668% (15571/1817 142/391 =============================================================>...]  Step: 34ms | Tot: 12s735ms | Loss: 0.409 | Acc: 85.978% (40719/4736 370/391 \n",
            "Train Loss: 0.4098196206876384 Acc:  85.882 correct 42941 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s148ms | Loss: 0.704 | Acc: 76.040% (7604/1000 100/100 ======>......................................................]  Step: 17ms | Tot: 588ms | Loss: 0.679 | Acc: 76.353% (1298/170 17/100 ======================================================>..........]  Step: 18ms | Tot: 1s833ms | Loss: 0.701 | Acc: 76.035% (6463/850 85/100 \n",
            "Test Loss: 0.7042176681756973 Acc: 76.04 Correct 7604 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 36\n",
            " [================================================================>]  Step: 34ms | Tot: 15s100ms | Loss: 0.407 | Acc: 86.060% (43030/5000 391/391 .............................................................]  Step: 41ms | Tot: 1s212ms | Loss: 0.402 | Acc: 86.393% (2654/307 24/391 ====================================================>............]  Step: 32ms | Tot: 12s576ms | Loss: 0.405 | Acc: 86.171% (34634/4019 314/391 =====================================================>...........]  Step: 44ms | Tot: 12s777ms | Loss: 0.406 | Acc: 86.162% (35292/4096 320/391 \n",
            "Train Loss: 0.4065133891142238 Acc:  86.06 correct 43030 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s331ms | Loss: 0.528 | Acc: 82.440% (8244/1000 100/100 ........................................................]  Step: 202ms | Tot: 202ms | Loss: 0.461 | Acc: 86.000% (172/20 2/100 ======================>..........................................]  Step: 20ms | Tot: 1s22ms | Loss: 0.556 | Acc: 81.971% (2869/350 35/100 =======================================================>.........]  Step: 20ms | Tot: 2s65ms | Loss: 0.534 | Acc: 82.299% (7160/870 87/100 ============================================================>....]  Step: 24ms | Tot: 2s208ms | Loss: 0.531 | Acc: 82.383% (7744/940 94/100 \n",
            "Test Loss: 0.5275710114836692 Acc: 82.44 Correct 8244 total 10000\n",
            "83.3\n",
            "\n",
            "Epoch: 37\n",
            " [================================================================>]  Step: 33ms | Tot: 13s828ms | Loss: 0.398 | Acc: 86.330% (43165/5000 391/391 =====>.......................................................]  Step: 34ms | Tot: 2s338ms | Loss: 0.384 | Acc: 86.927% (6676/768 60/391 ==========>......................................................]  Step: 33ms | Tot: 2s437ms | Loss: 0.385 | Acc: 86.917% (7009/806 63/391 ===============>.................................................]  Step: 32ms | Tot: 3s475ms | Loss: 0.384 | Acc: 86.962% (10352/1190 93/391 ==================>..............................................]  Step: 34ms | Tot: 4s226ms | Loss: 0.387 | Acc: 86.760% (12771/1472 115/391 ======================>..........................................]  Step: 32ms | Tot: 5s7ms | Loss: 0.395 | Acc: 86.487% (15277/1766 138/391 ==============================>..................................]  Step: 35ms | Tot: 6s722ms | Loss: 0.396 | Acc: 86.376% (20675/2393 187/391 =================================================>...............]  Step: 33ms | Tot: 10s678ms | Loss: 0.394 | Acc: 86.484% (33210/3840 300/391 \n",
            "Train Loss: 0.39791243300413537 Acc:  86.33 correct 43165 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s358ms | Loss: 0.482 | Acc: 83.860% (8386/1000 100/100 \n",
            "Test Loss: 0.48152888625860213 Acc: 83.86 Correct 8386 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 38\n",
            " [================================================================>]  Step: 26ms | Tot: 14s2ms | Loss: 0.396 | Acc: 86.500% (43250/5000 391/391 1 =====================================================>...........]  Step: 34ms | Tot: 11s755ms | Loss: 0.392 | Acc: 86.627% (35926/4147 324/391 \n",
            "Train Loss: 0.3957264459575229 Acc:  86.5 correct 43250 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s590ms | Loss: 0.517 | Acc: 82.880% (8288/1000 100/100 \n",
            "Test Loss: 0.516968062222004 Acc: 82.88 Correct 8288 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 39\n",
            " [================================================================>]  Step: 44ms | Tot: 16s217ms | Loss: 0.386 | Acc: 86.696% (43348/5000 391/391 ==============================>.................................]  Step: 50ms | Tot: 8s254ms | Loss: 0.384 | Acc: 86.715% (20867/2406 188/391 =======================================================>.........]  Step: 40ms | Tot: 14s109ms | Loss: 0.385 | Acc: 86.756% (37312/4300 336/391 \n",
            "Train Loss: 0.3856477307160492 Acc:  86.696 correct 43348 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s885ms | Loss: 0.614 | Acc: 79.880% (7988/1000 100/100 \n",
            "Test Loss: 0.6142395514249802 Acc: 79.88 Correct 7988 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 40\n",
            " [================================================================>]  Step: 27ms | Tot: 15s20ms | Loss: 0.382 | Acc: 86.794% (43397/5000 391/391  =================================>..............................]  Step: 46ms | Tot: 8s447ms | Loss: 0.375 | Acc: 86.977% (23268/2675 209/391 ===============================================>.................]  Step: 33ms | Tot: 11s596ms | Loss: 0.381 | Acc: 86.773% (32099/3699 289/391 =====================================================>...........]  Step: 34ms | Tot: 12s739ms | Loss: 0.381 | Acc: 86.840% (35903/4134 323/391 =============================================================>...]  Step: 32ms | Tot: 14s303ms | Loss: 0.381 | Acc: 86.801% (41220/4748 371/391 \n",
            "Train Loss: 0.38223808737057247 Acc:  86.794 correct 43397 total 50000\n",
            " [================================================================>]  Step: 16ms | Tot: 2s320ms | Loss: 0.553 | Acc: 81.250% (8125/1000 100/100 ===========================>...................................]  Step: 27ms | Tot: 1s201ms | Loss: 0.569 | Acc: 80.739% (3714/460 46/100 \n",
            "Test Loss: 0.5533556208014488 Acc: 81.25 Correct 8125 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 41\n",
            " [================================================================>]  Step: 35ms | Tot: 13s286ms | Loss: 0.378 | Acc: 87.016% (43508/5000 391/391 ================================>...............................]  Step: 32ms | Tot: 7s25ms | Loss: 0.364 | Acc: 87.527% (22519/2572 201/391 ==========================================>......................]  Step: 34ms | Tot: 8s863ms | Loss: 0.367 | Acc: 87.455% (28881/3302 258/391 ===============================================>.................]  Step: 35ms | Tot: 9s750ms | Loss: 0.375 | Acc: 87.152% (31793/3648 285/391 ===================================================>.............]  Step: 34ms | Tot: 10s536ms | Loss: 0.377 | Acc: 87.116% (34456/3955 309/391 =======================================================>.........]  Step: 38ms | Tot: 11s360ms | Loss: 0.377 | Acc: 87.115% (37132/4262 333/391 \n",
            "Train Loss: 0.3782162628591518 Acc:  87.016 correct 43508 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s242ms | Loss: 0.485 | Acc: 83.120% (8312/1000 100/100 \n",
            "Test Loss: 0.4848091968894005 Acc: 83.12 Correct 8312 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 42\n",
            " [================================================================>]  Step: 36ms | Tot: 13s80ms | Loss: 0.369 | Acc: 87.306% (43653/5000 391/391  ====================>...........................................]  Step: 34ms | Tot: 4s653ms | Loss: 0.360 | Acc: 87.659% (14923/1702 133/391 \n",
            "Train Loss: 0.3694541667352247 Acc:  87.306 correct 43653 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s71ms | Loss: 0.528 | Acc: 82.680% (8268/1000 100/100 =======================================>........................]  Step: 20ms | Tot: 1s320ms | Loss: 0.529 | Acc: 82.594% (5286/640 64/100 \n",
            "Test Loss: 0.5279109144210815 Acc: 82.68 Correct 8268 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 43\n",
            " [================================================================>]  Step: 28ms | Tot: 13s420ms | Loss: 0.378 | Acc: 86.974% (43487/5000 391/391 ...........................................................]  Step: 34ms | Tot: 688ms | Loss: 0.399 | Acc: 86.760% (2110/243 19/391 ============================================================>....]  Step: 40ms | Tot: 12s436ms | Loss: 0.378 | Acc: 86.990% (40419/4646 363/391 \n",
            "Train Loss: 0.3780017888454525 Acc:  86.974 correct 43487 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s98ms | Loss: 0.530 | Acc: 82.300% (8230/1000 100/100 ============>.................................................]  Step: 20ms | Tot: 688ms | Loss: 0.527 | Acc: 82.280% (2057/250 25/100 \n",
            "Test Loss: 0.5302214604616166 Acc: 82.3 Correct 8230 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 44\n",
            " [================================================================>]  Step: 25ms | Tot: 12s763ms | Loss: 0.376 | Acc: 86.994% (43497/5000 391/391 ========>....................................................]  Step: 33ms | Tot: 2s732ms | Loss: 0.374 | Acc: 86.930% (8234/947 74/391 ============>....................................................]  Step: 33ms | Tot: 2s766ms | Loss: 0.373 | Acc: 86.969% (8349/960 75/391 ==================>..............................................]  Step: 32ms | Tot: 3s956ms | Loss: 0.363 | Acc: 87.444% (12424/1420 111/391 ==================================================>..............]  Step: 32ms | Tot: 9s941ms | Loss: 0.373 | Acc: 87.020% (34084/3916 306/391 \n",
            "Train Loss: 0.3760293612394796 Acc:  86.994 correct 43497 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s36ms | Loss: 0.491 | Acc: 83.510% (8351/1000 100/100 ==>..........................................................]  Step: 21ms | Tot: 279ms | Loss: 0.451 | Acc: 84.909% (934/110 11/100 \n",
            "Test Loss: 0.4907064062356949 Acc: 83.51 Correct 8351 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 45\n",
            " [================================================================>]  Step: 28ms | Tot: 12s693ms | Loss: 0.358 | Acc: 87.726% (43863/5000 391/391 \n",
            "Train Loss: 0.3578004765007502 Acc:  87.726 correct 43863 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s130ms | Loss: 0.538 | Acc: 82.430% (8243/1000 100/100 \n",
            "Test Loss: 0.5378450322151184 Acc: 82.43 Correct 8243 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 46\n",
            " [================================================================>]  Step: 28ms | Tot: 12s798ms | Loss: 0.361 | Acc: 87.512% (43756/5000 391/391 ========>....................................................]  Step: 35ms | Tot: 2s605ms | Loss: 0.357 | Acc: 87.622% (8636/985 77/391 =======================>.........................................]  Step: 33ms | Tot: 4s827ms | Loss: 0.350 | Acc: 87.850% (16080/1830 143/391 ==============================>..................................]  Step: 34ms | Tot: 6s257ms | Loss: 0.350 | Acc: 87.874% (20921/2380 186/391 \n",
            "Train Loss: 0.36107633348621065 Acc:  87.512 correct 43756 total 50000\n",
            " [================================================================>]  Step: 22ms | Tot: 2s328ms | Loss: 0.500 | Acc: 83.740% (8374/1000 100/100 \n",
            "Test Loss: 0.4998650512099266 Acc: 83.74 Correct 8374 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 47\n",
            " [================================================================>]  Step: 33ms | Tot: 13s349ms | Loss: 0.358 | Acc: 87.642% (43821/5000 391/391 ==========================================>.....................]  Step: 34ms | Tot: 9s63ms | Loss: 0.359 | Acc: 87.544% (29695/3392 265/391 ==============================================================>..]  Step: 33ms | Tot: 12s846ms | Loss: 0.357 | Acc: 87.684% (42313/4825 377/391 \n",
            "Train Loss: 0.35767232861055437 Acc:  87.642 correct 43821 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s284ms | Loss: 0.501 | Acc: 83.160% (8316/1000 100/100 \n",
            "Test Loss: 0.5014606875181198 Acc: 83.16 Correct 8316 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 48\n",
            " [================================================================>]  Step: 30ms | Tot: 14s233ms | Loss: 0.351 | Acc: 87.964% (43982/5000 391/391 ======================>.........................................]  Step: 42ms | Tot: 5s358ms | Loss: 0.341 | Acc: 88.152% (16361/1856 145/391 ===========================>.....................................]  Step: 33ms | Tot: 6s129ms | Loss: 0.343 | Acc: 88.111% (18609/2112 165/391 =============================================================>...]  Step: 35ms | Tot: 13s475ms | Loss: 0.349 | Acc: 88.057% (41704/4736 370/391 \n",
            "Train Loss: 0.35141240300424875 Acc:  87.964 correct 43982 total 50000\n",
            " [================================================================>]  Step: 33ms | Tot: 2s415ms | Loss: 0.510 | Acc: 83.460% (8346/1000 100/100 =====================================>.........................]  Step: 28ms | Tot: 1s470ms | Loss: 0.524 | Acc: 83.306% (5165/620 62/100 \n",
            "Test Loss: 0.510018295943737 Acc: 83.46 Correct 8346 total 10000\n",
            "83.86\n",
            "\n",
            "Epoch: 49\n",
            " [================================================================>]  Step: 35ms | Tot: 13s845ms | Loss: 0.349 | Acc: 87.976% (43988/5000 391/391 .........................................................]  Step: 44ms | Tot: 527ms | Loss: 0.331 | Acc: 87.500% (784/89 7/391 =>...............................................................]  Step: 36ms | Tot: 728ms | Loss: 0.329 | Acc: 87.695% (1347/153 12/391 ==================================================>..............]  Step: 34ms | Tot: 10s807ms | Loss: 0.344 | Acc: 88.085% (34163/3878 303/391 \n",
            "Train Loss: 0.34933428435831726 Acc:  87.976 correct 43988 total 50000\n",
            " [================================================================>]  Step: 17ms | Tot: 2s337ms | Loss: 0.460 | Acc: 84.620% (8462/1000 100/100 ===================================================>...........]  Step: 22ms | Tot: 2s44ms | Loss: 0.459 | Acc: 84.524% (7100/840 84/100 \n",
            "Test Loss: 0.45967971384525297 Acc: 84.62 Correct 8462 total 10000\n",
            "84.62\n",
            "\n",
            "Epoch: 50\n",
            " [================================================================>]  Step: 31ms | Tot: 13s886ms | Loss: 0.342 | Acc: 88.222% (44111/5000 391/391 ======>......................................................]  Step: 33ms | Tot: 2s494ms | Loss: 0.334 | Acc: 88.625% (7487/844 66/391 ==========>......................................................]  Step: 33ms | Tot: 2s527ms | Loss: 0.334 | Acc: 88.654% (7603/857 67/391 ====================================================>............]  Step: 34ms | Tot: 11s196ms | Loss: 0.337 | Acc: 88.392% (35866/4057 317/391 \n",
            "Train Loss: 0.3417769591216846 Acc:  88.222 correct 44111 total 50000\n",
            " [================================================================>]  Step: 24ms | Tot: 2s785ms | Loss: 0.548 | Acc: 81.230% (8123/1000 100/100 \n",
            "Test Loss: 0.5475935637950897 Acc: 81.23 Correct 8123 total 10000\n",
            "84.62\n",
            "\n",
            "Epoch: 51\n",
            " [================================================================>]  Step: 29ms | Tot: 15s243ms | Loss: 0.348 | Acc: 87.942% (43971/5000 391/391 =====>.......................................................]  Step: 41ms | Tot: 2s382ms | Loss: 0.338 | Acc: 88.228% (6663/755 59/391 ================================>................................]  Step: 51ms | Tot: 7s875ms | Loss: 0.337 | Acc: 88.281% (22261/2521 197/391 ==============================================>..................]  Step: 40ms | Tot: 11s177ms | Loss: 0.344 | Acc: 88.014% (31319/3558 278/391 ===============================================================>.]  Step: 32ms | Tot: 14s869ms | Loss: 0.348 | Acc: 87.924% (42766/4864 380/391 \n",
            "Train Loss: 0.34822448360188235 Acc:  87.942 correct 43971 total 50000\n",
            " [================================================================>]  Step: 15ms | Tot: 2s97ms | Loss: 0.463 | Acc: 84.340% (8434/1000 100/100 ============================================>...................]  Step: 18ms | Tot: 1s631ms | Loss: 0.475 | Acc: 83.944% (5960/710 71/100 \n",
            "Test Loss: 0.4633514240384102 Acc: 84.34 Correct 8434 total 10000\n",
            "84.62\n",
            "\n",
            "Epoch: 52\n",
            " [================================================================>]  Step: 28ms | Tot: 13s947ms | Loss: 0.352 | Acc: 87.860% (43930/5000 391/391 \n",
            "Train Loss: 0.3523111833102258 Acc:  87.86 correct 43930 total 50000\n",
            " [================================================================>]  Step: 17ms | Tot: 2s272ms | Loss: 0.478 | Acc: 84.120% (8412/1000 100/100 \n",
            "Test Loss: 0.4776739454269409 Acc: 84.12 Correct 8412 total 10000\n",
            "84.62\n",
            "\n",
            "Epoch: 53\n",
            " [================================================================>]  Step: 29ms | Tot: 13s994ms | Loss: 0.335 | Acc: 88.586% (44293/5000 391/391 =====>.......................................................]  Step: 39ms | Tot: 2s525ms | Loss: 0.331 | Acc: 88.776% (6818/768 60/391 ======================>..........................................]  Step: 35ms | Tot: 5s499ms | Loss: 0.330 | Acc: 88.876% (15699/1766 138/391 \n",
            "Train Loss: 0.3351449666501921 Acc:  88.586 correct 44293 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s347ms | Loss: 0.495 | Acc: 84.120% (8412/1000 100/100 =>..........................................................]  Step: 21ms | Tot: 470ms | Loss: 0.467 | Acc: 85.545% (941/110 11/100 \n",
            "Test Loss: 0.49469071716070173 Acc: 84.12 Correct 8412 total 10000\n",
            "84.62\n",
            "\n",
            "Epoch: 54\n",
            " [================================================================>]  Step: 37ms | Tot: 14s | Loss: 0.330 | Acc: 88.526% (44263/5000 391/391 /391 ...........................................................]  Step: 39ms | Tot: 738ms | Loss: 0.322 | Acc: 88.770% (1818/204 16/391 =========>.......................................................]  Step: 36ms | Tot: 2s427ms | Loss: 0.323 | Acc: 88.807% (6593/742 58/391 =========>.......................................................]  Step: 41ms | Tot: 2s469ms | Loss: 0.323 | Acc: 88.771% (6704/755 59/391 ==========>......................................................]  Step: 42ms | Tot: 2s592ms | Loss: 0.323 | Acc: 88.810% (7048/793 62/391 ==============================>..................................]  Step: 39ms | Tot: 7s243ms | Loss: 0.322 | Acc: 88.810% (21144/2380 186/391 ===============================>.................................]  Step: 40ms | Tot: 7s324ms | Loss: 0.322 | Acc: 88.809% (21371/2406 188/391 ===================================>.............................]  Step: 41ms | Tot: 8s284ms | Loss: 0.322 | Acc: 88.791% (24208/2726 213/391 \n",
            "Train Loss: 0.33013553993628764 Acc:  88.526 correct 44263 total 50000\n",
            " [================================================================>]  Step: 27ms | Tot: 2s730ms | Loss: 0.441 | Acc: 85.610% (8561/1000 100/100 =====================================================>.........]  Step: 20ms | Tot: 2s421ms | Loss: 0.440 | Acc: 85.506% (7439/870 87/100 \n",
            "Test Loss: 0.4409723077714443 Acc: 85.61 Correct 8561 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 55\n",
            " [================================================================>]  Step: 32ms | Tot: 14s120ms | Loss: 0.332 | Acc: 88.472% (44236/5000 391/391 ==================>.............................................]  Step: 38ms | Tot: 4s784ms | Loss: 0.318 | Acc: 89.017% (13559/1523 119/391 =======================================>.........................]  Step: 36ms | Tot: 8s752ms | Loss: 0.321 | Acc: 88.865% (27413/3084 241/391 \n",
            "Train Loss: 0.3324476318895969 Acc:  88.472 correct 44236 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s55ms | Loss: 0.556 | Acc: 81.090% (8109/1000 100/100 \n",
            "Test Loss: 0.5563279595971108 Acc: 81.09 Correct 8109 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 56\n",
            " [================================================================>]  Step: 41ms | Tot: 14s260ms | Loss: 0.328 | Acc: 88.768% (44384/5000 391/391 =====>.......................................................]  Step: 41ms | Tot: 2s389ms | Loss: 0.317 | Acc: 88.971% (6833/768 60/391 ================>................................................]  Step: 33ms | Tot: 3s854ms | Loss: 0.305 | Acc: 89.617% (11471/1280 100/391 ===========================>.....................................]  Step: 48ms | Tot: 6s344ms | Loss: 0.314 | Acc: 89.259% (19080/2137 167/391 \n",
            "Train Loss: 0.3277657786217492 Acc:  88.768 correct 44384 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s504ms | Loss: 0.469 | Acc: 84.470% (8447/1000 100/100 =>..........................................................]  Step: 36ms | Tot: 578ms | Loss: 0.451 | Acc: 84.727% (932/110 11/100 \n",
            "Test Loss: 0.46927895277738574 Acc: 84.47 Correct 8447 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 57\n",
            " [================================================================>]  Step: 29ms | Tot: 13s846ms | Loss: 0.326 | Acc: 88.896% (44448/5000 391/391 >............................................................]  Step: 58ms | Tot: 1s329ms | Loss: 0.310 | Acc: 89.754% (2987/332 26/391 ====>............................................................]  Step: 50ms | Tot: 1s426ms | Loss: 0.309 | Acc: 89.704% (3215/358 28/391 =================================================>...............]  Step: 36ms | Tot: 10s922ms | Loss: 0.325 | Acc: 88.928% (34262/3852 301/391 =============================================================>...]  Step: 33ms | Tot: 13s85ms | Loss: 0.324 | Acc: 88.927% (41888/4710 368/391 \n",
            "Train Loss: 0.32551739439177696 Acc:  88.896 correct 44448 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s186ms | Loss: 0.477 | Acc: 84.260% (8426/1000 100/100 \n",
            "Test Loss: 0.4771563085913658 Acc: 84.26 Correct 8426 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 58\n",
            " [================================================================>]  Step: 28ms | Tot: 12s875ms | Loss: 0.316 | Acc: 89.156% (44578/5000 391/391 ===>.........................................................]  Step: 34ms | Tot: 1s765ms | Loss: 0.330 | Acc: 88.621% (5218/588 46/391 ===========================>.....................................]  Step: 32ms | Tot: 5s635ms | Loss: 0.314 | Acc: 89.306% (19090/2137 167/391 ==========================================>......................]  Step: 31ms | Tot: 8s512ms | Loss: 0.314 | Acc: 89.271% (29138/3264 255/391 \n",
            "Train Loss: 0.3161148872902936 Acc:  89.156 correct 44578 total 50000\n",
            " [================================================================>]  Step: 35ms | Tot: 2s13ms | Loss: 0.495 | Acc: 83.190% (8319/1000 100/100 \n",
            "Test Loss: 0.49531136095523837 Acc: 83.19 Correct 8319 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 59\n",
            " [================================================================>]  Step: 31ms | Tot: 13s784ms | Loss: 0.329 | Acc: 88.646% (44323/5000 391/391 .........................................................]  Step: 37ms | Tot: 527ms | Loss: 0.336 | Acc: 88.281% (904/102 8/391 ===========>.....................................................]  Step: 33ms | Tot: 2s853ms | Loss: 0.343 | Acc: 87.977% (8108/921 72/391 ==============================>..................................]  Step: 33ms | Tot: 6s972ms | Loss: 0.325 | Acc: 88.789% (21139/2380 186/391 ============================================================>....]  Step: 32ms | Tot: 12s893ms | Loss: 0.329 | Acc: 88.632% (41409/4672 365/391 \n",
            "Train Loss: 0.32858832195744186 Acc:  88.646 correct 44323 total 50000\n",
            " [================================================================>]  Step: 17ms | Tot: 2s366ms | Loss: 0.521 | Acc: 83.060% (8306/1000 100/100 \n",
            "Test Loss: 0.5210860538482666 Acc: 83.06 Correct 8306 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 60\n",
            " [================================================================>]  Step: 33ms | Tot: 13s957ms | Loss: 0.322 | Acc: 88.840% (44420/5000 391/391 ...........................................................]  Step: 33ms | Tot: 760ms | Loss: 0.306 | Acc: 89.014% (1823/204 16/391 ======>..........................................................]  Step: 34ms | Tot: 1s748ms | Loss: 0.308 | Acc: 89.211% (4796/537 42/391 ==========>......................................................]  Step: 33ms | Tot: 2s454ms | Loss: 0.304 | Acc: 89.385% (7208/806 63/391 =========================================>.......................]  Step: 33ms | Tot: 9s99ms | Loss: 0.326 | Acc: 88.746% (28626/3225 252/391 ======================================================>..........]  Step: 34ms | Tot: 11s687ms | Loss: 0.324 | Acc: 88.801% (37055/4172 326/391 \n",
            "Train Loss: 0.32155128852333253 Acc:  88.84 correct 44420 total 50000\n",
            " [================================================================>]  Step: 16ms | Tot: 1s956ms | Loss: 0.548 | Acc: 82.690% (8269/1000 100/100 \n",
            "Test Loss: 0.5480736663937569 Acc: 82.69 Correct 8269 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 61\n",
            " [================================================================>]  Step: 30ms | Tot: 13s377ms | Loss: 0.312 | Acc: 89.278% (44639/5000 391/391 ===>.........................................................]  Step: 34ms | Tot: 1s954ms | Loss: 0.293 | Acc: 89.764% (5630/627 49/391 \n",
            "Train Loss: 0.3122474157520572 Acc:  89.278 correct 44639 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s202ms | Loss: 0.514 | Acc: 82.850% (8285/1000 100/100 ========================================>......................]  Step: 20ms | Tot: 1s545ms | Loss: 0.515 | Acc: 82.866% (5552/670 67/100 =======================================================>.........]  Step: 18ms | Tot: 1s934ms | Loss: 0.515 | Acc: 82.791% (7120/860 86/100 \n",
            "Test Loss: 0.5137880748510361 Acc: 82.85 Correct 8285 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 62\n",
            " [================================================================>]  Step: 30ms | Tot: 14s57ms | Loss: 0.301 | Acc: 89.672% (44836/5000 391/391  >............................................................]  Step: 39ms | Tot: 1s202ms | Loss: 0.297 | Acc: 89.873% (3106/345 27/391 ==========================>......................................]  Step: 31ms | Tot: 5s634ms | Loss: 0.298 | Acc: 89.913% (18184/2022 158/391 ==============================>..................................]  Step: 40ms | Tot: 6s649ms | Loss: 0.298 | Acc: 89.950% (21070/2342 183/391 =======================================>.........................]  Step: 46ms | Tot: 8s656ms | Loss: 0.297 | Acc: 89.824% (27249/3033 237/391 \n",
            "Train Loss: 0.3013491330625456 Acc:  89.672 correct 44836 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s430ms | Loss: 0.577 | Acc: 81.630% (8163/1000 100/100 \n",
            "Test Loss: 0.5774839705228806 Acc: 81.63 Correct 8163 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 63\n",
            " [================================================================>]  Step: 28ms | Tot: 13s798ms | Loss: 0.314 | Acc: 89.130% (44565/5000 391/391 =================================================>...............]  Step: 33ms | Tot: 10s693ms | Loss: 0.313 | Acc: 89.117% (34335/3852 301/391 =========================================================>.......]  Step: 33ms | Tot: 12s208ms | Loss: 0.314 | Acc: 89.108% (39236/4403 344/391 =========================================================>.......]  Step: 33ms | Tot: 12s376ms | Loss: 0.314 | Acc: 89.110% (39807/4467 349/391 ============================================================>....]  Step: 33ms | Tot: 12s954ms | Loss: 0.315 | Acc: 89.099% (41741/4684 366/391 \n",
            "Train Loss: 0.3140016528575317 Acc:  89.13 correct 44565 total 50000\n",
            " [================================================================>]  Step: 24ms | Tot: 2s410ms | Loss: 0.442 | Acc: 84.990% (8499/1000 100/100 ===============================>...............................]  Step: 21ms | Tot: 1s340ms | Loss: 0.442 | Acc: 84.887% (4499/530 53/100 \n",
            "Test Loss: 0.4415766516327858 Acc: 84.99 Correct 8499 total 10000\n",
            "85.61\n",
            "\n",
            "Epoch: 64\n",
            " [================================================================>]  Step: 29ms | Tot: 13s932ms | Loss: 0.306 | Acc: 89.484% (44742/5000 391/391 ====>........................................................]  Step: 36ms | Tot: 2s120ms | Loss: 0.287 | Acc: 90.668% (6383/704 55/391 \n",
            "Train Loss: 0.30577274920690395 Acc:  89.484 correct 44742 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s449ms | Loss: 0.404 | Acc: 86.240% (8624/1000 100/100 \n",
            "Test Loss: 0.40420502081513404 Acc: 86.24 Correct 8624 total 10000\n",
            "86.24\n",
            "\n",
            "Epoch: 65\n",
            " [================================================================>]  Step: 28ms | Tot: 13s552ms | Loss: 0.299 | Acc: 89.646% (44823/5000 391/391 ===========>..................................................]  Step: 33ms | Tot: 3s114ms | Loss: 0.279 | Acc: 90.554% (10200/1126 88/391 ==================>..............................................]  Step: 33ms | Tot: 4s14ms | Loss: 0.279 | Acc: 90.474% (13202/1459 114/391 ===================>.............................................]  Step: 33ms | Tot: 4s216ms | Loss: 0.282 | Acc: 90.339% (13876/1536 120/391 ============================================================>....]  Step: 32ms | Tot: 12s599ms | Loss: 0.300 | Acc: 89.603% (41748/4659 364/391 \n",
            "Train Loss: 0.2993000984344336 Acc:  89.646 correct 44823 total 50000\n",
            " [================================================================>]  Step: 32ms | Tot: 2s753ms | Loss: 0.397 | Acc: 87.040% (8704/1000 100/100 ==========>..................................................]  Step: 21ms | Tot: 799ms | Loss: 0.412 | Acc: 86.375% (2073/240 24/100 \n",
            "Test Loss: 0.3965093030035496 Acc: 87.04 Correct 8704 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 66\n",
            " [================================================================>]  Step: 41ms | Tot: 13s150ms | Loss: 0.312 | Acc: 89.120% (44560/5000 391/391 \n",
            "Train Loss: 0.3122773947541976 Acc:  89.12 correct 44560 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s264ms | Loss: 0.425 | Acc: 86.170% (8617/1000 100/100 =====================================>.........................]  Step: 21ms | Tot: 1s510ms | Loss: 0.430 | Acc: 85.984% (5331/620 62/100 \n",
            "Test Loss: 0.42473815083503724 Acc: 86.17 Correct 8617 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 67\n",
            " [================================================================>]  Step: 37ms | Tot: 14s593ms | Loss: 0.290 | Acc: 90.110% (45055/5000 391/391 ...........................................................]  Step: 32ms | Tot: 868ms | Loss: 0.279 | Acc: 90.234% (2541/281 22/391 171/391 \n",
            "Train Loss: 0.29009327932696816 Acc:  90.11 correct 45055 total 50000\n",
            " [================================================================>]  Step: 17ms | Tot: 2s294ms | Loss: 0.492 | Acc: 83.890% (8389/1000 100/100 ==>.........................................................]  Step: 22ms | Tot: 498ms | Loss: 0.493 | Acc: 83.917% (1007/120 12/100 \n",
            "Test Loss: 0.49198714464902876 Acc: 83.89 Correct 8389 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 68\n",
            " [================================================================>]  Step: 29ms | Tot: 14s91ms | Loss: 0.294 | Acc: 89.874% (44937/5000 391/391  ================>...............................................]  Step: 36ms | Tot: 3s833ms | Loss: 0.295 | Acc: 90.085% (12338/1369 107/391 ============================================>....................]  Step: 35ms | Tot: 9s842ms | Loss: 0.300 | Acc: 89.628% (31090/3468 271/391 ====================================================>............]  Step: 33ms | Tot: 11s340ms | Loss: 0.299 | Acc: 89.680% (36044/4019 314/391 \n",
            "Train Loss: 0.2939066289521544 Acc:  89.874 correct 44937 total 50000\n",
            " [================================================================>]  Step: 26ms | Tot: 2s559ms | Loss: 0.455 | Acc: 85.310% (8531/1000 100/100 \n",
            "Test Loss: 0.4551605577766895 Acc: 85.31 Correct 8531 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 69\n",
            " [================================================================>]  Step: 32ms | Tot: 14s376ms | Loss: 0.299 | Acc: 89.698% (44849/5000 391/391 =>...........................................................]  Step: 35ms | Tot: 1s578ms | Loss: 0.298 | Acc: 89.280% (4114/460 36/391 \n",
            "Train Loss: 0.2990568646841952 Acc:  89.698 correct 44849 total 50000\n",
            " [================================================================>]  Step: 26ms | Tot: 2s438ms | Loss: 0.411 | Acc: 86.110% (8611/1000 100/100 ==============>..............................................]  Step: 26ms | Tot: 817ms | Loss: 0.428 | Acc: 85.586% (2482/290 29/100 ====================================================>............]  Step: 25ms | Tot: 1s997ms | Loss: 0.413 | Acc: 86.049% (6970/810 81/100 \n",
            "Test Loss: 0.4111825196444988 Acc: 86.11 Correct 8611 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 70\n",
            " [================================================================>]  Step: 39ms | Tot: 14s927ms | Loss: 0.289 | Acc: 89.968% (44984/5000 391/391 =============>.................................................]  Step: 40ms | Tot: 3s539ms | Loss: 0.286 | Acc: 90.126% (10844/1203 94/391 =========================================>.......................]  Step: 51ms | Tot: 9s457ms | Loss: 0.286 | Acc: 90.027% (28924/3212 251/391 \n",
            "Train Loss: 0.28877595432883946 Acc:  89.968 correct 44984 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s428ms | Loss: 0.421 | Acc: 85.630% (8563/1000 100/100 \n",
            "Test Loss: 0.42128938615322115 Acc: 85.63 Correct 8563 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 71\n",
            " [================================================================>]  Step: 36ms | Tot: 14s609ms | Loss: 0.289 | Acc: 90.038% (45019/5000 391/391 ========>....................................................]  Step: 39ms | Tot: 2s996ms | Loss: 0.280 | Acc: 90.498% (8572/947 74/391 ===================>.............................................]  Step: 41ms | Tot: 4s789ms | Loss: 0.288 | Acc: 90.176% (13851/1536 120/391 =====================================================>...........]  Step: 35ms | Tot: 12s154ms | Loss: 0.289 | Acc: 90.119% (37374/4147 324/391 \n",
            "Train Loss: 0.2886982376656264 Acc:  90.038 correct 45019 total 50000\n",
            " [===============================================================>.]  Step: 23ms | Tot: 2s726ms | Loss: 0.406 | Acc: 86.455% (8559/990 99/100 ================================================================>]  Step: 23ms | Tot: 2s749ms | Loss: 0.406 | Acc: 86.440% (8644/1000 100/100 \n",
            "Test Loss: 0.4059070481359959 Acc: 86.44 Correct 8644 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 72\n",
            " [================================================================>]  Step: 25ms | Tot: 15s173ms | Loss: 0.277 | Acc: 90.582% (45291/5000 391/391 ================================================>................]  Step: 34ms | Tot: 11s312ms | Loss: 0.279 | Acc: 90.493% (33707/3724 291/391 ===================================================>.............]  Step: 46ms | Tot: 11s997ms | Loss: 0.278 | Acc: 90.547% (35929/3968 310/391 \n",
            "Train Loss: 0.27667585343046264 Acc:  90.582 correct 45291 total 50000\n",
            " [================================================================>]  Step: 21ms | Tot: 2s748ms | Loss: 0.441 | Acc: 85.570% (8557/1000 100/100 \n",
            "Test Loss: 0.4405327853560448 Acc: 85.57 Correct 8557 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 73\n",
            " [================================================================>]  Step: 31ms | Tot: 14s784ms | Loss: 0.279 | Acc: 90.292% (45146/5000 391/391 ================>...............................................]  Step: 35ms | Tot: 4s269ms | Loss: 0.261 | Acc: 90.888% (12448/1369 107/391 \n",
            "Train Loss: 0.2791841956965454 Acc:  90.292 correct 45146 total 50000\n",
            " [================================================================>]  Step: 37ms | Tot: 2s740ms | Loss: 0.449 | Acc: 85.160% (8516/1000 100/100 \n",
            "Test Loss: 0.44883603185415266 Acc: 85.16 Correct 8516 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 74\n",
            " [================================================================>]  Step: 30ms | Tot: 14s773ms | Loss: 0.274 | Acc: 90.618% (45309/5000 391/391 .........................................................]  Step: 36ms | Tot: 508ms | Loss: 0.246 | Acc: 91.493% (1054/115 9/391 =====>...........................................................]  Step: 39ms | Tot: 1s452ms | Loss: 0.263 | Acc: 90.838% (3837/422 33/391 =====================>...........................................]  Step: 38ms | Tot: 5s325ms | Loss: 0.265 | Acc: 90.915% (15361/1689 132/391 =======================================================>.........]  Step: 36ms | Tot: 12s856ms | Loss: 0.273 | Acc: 90.630% (38862/4288 335/391 \n",
            "Train Loss: 0.27425376316318123 Acc:  90.618 correct 45309 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s388ms | Loss: 0.459 | Acc: 85.010% (8501/1000 100/100 =============================>.................................]  Step: 21ms | Tot: 1s275ms | Loss: 0.472 | Acc: 84.700% (4235/500 50/100 \n",
            "Test Loss: 0.4589803430438042 Acc: 85.01 Correct 8501 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 75\n",
            " [================================================================>]  Step: 29ms | Tot: 13s710ms | Loss: 0.272 | Acc: 90.754% (45377/5000 391/391 =====================>..........................................]  Step: 33ms | Tot: 4s945ms | Loss: 0.266 | Acc: 90.934% (15597/1715 134/391 =============================>...................................]  Step: 34ms | Tot: 6s380ms | Loss: 0.266 | Acc: 90.909% (20480/2252 176/391 =============================>...................................]  Step: 33ms | Tot: 6s414ms | Loss: 0.266 | Acc: 90.872% (20588/2265 177/391 =============================================>...................]  Step: 34ms | Tot: 9s641ms | Loss: 0.268 | Acc: 90.815% (31618/3481 272/391 =============================================>...................]  Step: 33ms | Tot: 9s675ms | Loss: 0.268 | Acc: 90.822% (31737/3494 273/391 =============================================>...................]  Step: 32ms | Tot: 9s741ms | Loss: 0.268 | Acc: 90.801% (31962/3520 275/391 ================================================>................]  Step: 34ms | Tot: 10s268ms | Loss: 0.270 | Acc: 90.757% (33805/3724 291/391 =================================================>...............]  Step: 34ms | Tot: 10s537ms | Loss: 0.270 | Acc: 90.771% (34740/3827 299/391 ===================================================>.............]  Step: 34ms | Tot: 10s905ms | Loss: 0.269 | Acc: 90.801% (36030/3968 310/391 \n",
            "Train Loss: 0.2718971357168749 Acc:  90.754 correct 45377 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s376ms | Loss: 0.541 | Acc: 82.840% (8284/1000 100/100 =======================================================>.......]  Step: 20ms | Tot: 2s157ms | Loss: 0.545 | Acc: 82.867% (7458/900 90/100 \n",
            "Test Loss: 0.5410878691077232 Acc: 82.84 Correct 8284 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 76\n",
            " [================================================================>]  Step: 30ms | Tot: 13s502ms | Loss: 0.267 | Acc: 90.800% (45400/5000 391/391 ===========================================>....................]  Step: 33ms | Tot: 9s309ms | Loss: 0.265 | Acc: 91.002% (31101/3417 267/391 =========================================================>.......]  Step: 36ms | Tot: 11s966ms | Loss: 0.268 | Acc: 90.821% (40339/4441 347/391 \n",
            "Train Loss: 0.2674742681748422 Acc:  90.8 correct 45400 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s371ms | Loss: 0.414 | Acc: 86.290% (8629/1000 100/100 \n",
            "Test Loss: 0.413676984757185 Acc: 86.29 Correct 8629 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 77\n",
            " [================================================================>]  Step: 30ms | Tot: 13s680ms | Loss: 0.265 | Acc: 90.890% (45445/5000 391/391 ...........................................................]  Step: 34ms | Tot: 1s27ms | Loss: 0.252 | Acc: 91.088% (3148/345 27/391 ================================>................................]  Step: 33ms | Tot: 6s900ms | Loss: 0.266 | Acc: 90.848% (22792/2508 196/391 =================================>...............................]  Step: 34ms | Tot: 7s135ms | Loss: 0.268 | Acc: 90.748% (23580/2598 203/391 =========================================>.......................]  Step: 34ms | Tot: 8s747ms | Loss: 0.267 | Acc: 90.759% (29159/3212 251/391 \n",
            "Train Loss: 0.2645568277905969 Acc:  90.89 correct 45445 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s457ms | Loss: 0.497 | Acc: 84.260% (8426/1000 100/100 ===============================>...............................]  Step: 21ms | Tot: 1s471ms | Loss: 0.492 | Acc: 84.547% (4481/530 53/100 ==================================>..............................]  Step: 21ms | Tot: 1s493ms | Loss: 0.490 | Acc: 84.611% (4569/540 54/100 \n",
            "Test Loss: 0.4973547534644604 Acc: 84.26 Correct 8426 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 78\n",
            " [================================================================>]  Step: 28ms | Tot: 13s719ms | Loss: 0.261 | Acc: 91.082% (45541/5000 391/391 =============================>..................................]  Step: 32ms | Tot: 6s328ms | Loss: 0.251 | Acc: 91.466% (21542/2355 184/391 \n",
            "Train Loss: 0.260967165650919 Acc:  91.082 correct 45541 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s277ms | Loss: 0.493 | Acc: 84.210% (8421/1000 100/100 \n",
            "Test Loss: 0.4926241305470467 Acc: 84.21 Correct 8421 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 79\n",
            " [================================================================>]  Step: 30ms | Tot: 14s282ms | Loss: 0.262 | Acc: 91.158% (45579/5000 391/391 >............................................................]  Step: 31ms | Tot: 1s197ms | Loss: 0.253 | Acc: 90.972% (3144/345 27/391 ====>............................................................]  Step: 35ms | Tot: 1s232ms | Loss: 0.252 | Acc: 91.044% (3263/358 28/39 165/391 =============================>...................................]  Step: 39ms | Tot: 6s716ms | Loss: 0.262 | Acc: 91.029% (20507/2252 176/391 ==========================================>......................]  Step: 34ms | Tot: 9s631ms | Loss: 0.260 | Acc: 91.180% (29878/3276 256/391 ======================================================>..........]  Step: 36ms | Tot: 12s210ms | Loss: 0.261 | Acc: 91.196% (38638/4236 331/391 \n",
            "Train Loss: 0.26169412908956524 Acc:  91.158 correct 45579 total 50000\n",
            " [================================================================>]  Step: 25ms | Tot: 2s387ms | Loss: 0.524 | Acc: 83.890% (8389/1000 100/100 ========================================>......................]  Step: 18ms | Tot: 1s712ms | Loss: 0.535 | Acc: 83.597% (5601/670 67/100 \n",
            "Test Loss: 0.5243717569112778 Acc: 83.89 Correct 8389 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 80\n",
            " [================================================================>]  Step: 28ms | Tot: 15s67ms | Loss: 0.256 | Acc: 91.144% (45572/5000 391/391  =========================>......................................]  Step: 52ms | Tot: 6s313ms | Loss: 0.254 | Acc: 91.184% (18908/2073 162/391 ==================================>..............................]  Step: 34ms | Tot: 8s74ms | Loss: 0.255 | Acc: 91.187% (24511/2688 210/391 \n",
            "Train Loss: 0.25571903020448394 Acc:  91.144 correct 45572 total 50000\n",
            " [================================================================>]  Step: 23ms | Tot: 2s610ms | Loss: 0.435 | Acc: 85.720% (8572/1000 100/100 \n",
            "Test Loss: 0.43520730182528494 Acc: 85.72 Correct 8572 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 81\n",
            " [================================================================>]  Step: 37ms | Tot: 14s929ms | Loss: 0.255 | Acc: 91.326% (45663/5000 391/391 =====================>..........................................]  Step: 35ms | Tot: 5s355ms | Loss: 0.241 | Acc: 91.882% (16230/1766 138/391 \n",
            "Train Loss: 0.2551439924313284 Acc:  91.326 correct 45663 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s317ms | Loss: 0.444 | Acc: 85.750% (8575/1000 100/100 \n",
            "Test Loss: 0.44367174059152603 Acc: 85.75 Correct 8575 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 82\n",
            " [================================================================>]  Step: 36ms | Tot: 15s624ms | Loss: 0.249 | Acc: 91.426% (45713/5000 391/391 =======>.....................................................]  Step: 41ms | Tot: 2s749ms | Loss: 0.234 | Acc: 91.927% (8119/883 69/391 ===============>.................................................]  Step: 39ms | Tot: 3s860ms | Loss: 0.236 | Acc: 91.752% (11157/1216 95/391 ===========================>.....................................]  Step: 42ms | Tot: 6s652ms | Loss: 0.241 | Acc: 91.637% (19471/2124 166/391 =============================>...................................]  Step: 43ms | Tot: 7s158ms | Loss: 0.239 | Acc: 91.700% (20893/2278 178/391 ========================================>........................]  Step: 49ms | Tot: 9s574ms | Loss: 0.246 | Acc: 91.510% (28346/3097 242/391 \n",
            "Train Loss: 0.24851196931908504 Acc:  91.426 correct 45713 total 50000\n",
            " [================================================================>]  Step: 27ms | Tot: 3s412ms | Loss: 0.397 | Acc: 86.940% (8694/1000 100/100 ======================>........................................]  Step: 37ms | Tot: 1s480ms | Loss: 0.407 | Acc: 87.000% (3393/390 39/100 =======================================>.........................]  Step: 37ms | Tot: 2s137ms | Loss: 0.399 | Acc: 87.213% (5320/610 61/100 ==============================================>..................]  Step: 57ms | Tot: 2s561ms | Loss: 0.395 | Acc: 87.192% (6365/730 73/100 \n",
            "Test Loss: 0.3971844013035297 Acc: 86.94 Correct 8694 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 83\n",
            " [================================================================>]  Step: 31ms | Tot: 15s269ms | Loss: 0.248 | Acc: 91.496% (45748/5000 391/391 \n",
            "Train Loss: 0.24835931816521814 Acc:  91.496 correct 45748 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s557ms | Loss: 0.506 | Acc: 84.070% (8407/1000 100/100 \n",
            "Test Loss: 0.5062674373388291 Acc: 84.07 Correct 8407 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 84\n",
            " [================================================================>]  Step: 34ms | Tot: 13s755ms | Loss: 0.242 | Acc: 91.752% (45876/5000 391/391 =======================>........................................]  Step: 34ms | Tot: 5s142ms | Loss: 0.236 | Acc: 91.895% (17291/1881 147/391 ===================================>.............................]  Step: 36ms | Tot: 7s417ms | Loss: 0.236 | Acc: 91.899% (25173/2739 214/391 \n",
            "Train Loss: 0.24183696266406637 Acc:  91.752 correct 45876 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s432ms | Loss: 0.382 | Acc: 86.980% (8698/1000 100/100 ==========================================>....................]  Step: 21ms | Tot: 1s788ms | Loss: 0.389 | Acc: 86.826% (5991/690 69/100 \n",
            "Test Loss: 0.38238626897335054 Acc: 86.98 Correct 8698 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 85\n",
            " [================================================================>]  Step: 27ms | Tot: 14s840ms | Loss: 0.243 | Acc: 91.688% (45844/5000 391/391 ===========================>....................................]  Step: 34ms | Tot: 6s740ms | Loss: 0.240 | Acc: 91.685% (20068/2188 171/391 ========================================================>........]  Step: 44ms | Tot: 13s66ms | Loss: 0.243 | Acc: 91.642% (40000/4364 341/391 \n",
            "Train Loss: 0.24251866256794358 Acc:  91.688 correct 45844 total 50000\n",
            " [================================================================>]  Step: 23ms | Tot: 2s370ms | Loss: 0.439 | Acc: 85.820% (8582/1000 100/100 \n",
            "Test Loss: 0.43891415387392047 Acc: 85.82 Correct 8582 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 86\n",
            " [================================================================>]  Step: 28ms | Tot: 13s355ms | Loss: 0.237 | Acc: 91.760% (45880/5000 391/391  =============================>.................................]  Step: 34ms | Tot: 6s490ms | Loss: 0.226 | Acc: 92.307% (22331/2419 189/391 =============================================>...................]  Step: 30ms | Tot: 9s270ms | Loss: 0.229 | Acc: 92.084% (32060/3481 272/391 \n",
            "Train Loss: 0.23672967099244027 Acc:  91.76 correct 45880 total 50000\n",
            " [================================================================>]  Step: 15ms | Tot: 2s12ms | Loss: 0.433 | Acc: 85.890% (8589/1000 100/100 ============================================================>...]  Step: 16ms | Tot: 1s949ms | Loss: 0.433 | Acc: 85.833% (8240/960 96/100 \n",
            "Test Loss: 0.4327322372794151 Acc: 85.89 Correct 8589 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 87\n",
            " [================================================================>]  Step: 28ms | Tot: 12s927ms | Loss: 0.237 | Acc: 91.844% (45922/5000 391/391 ===============>................................................]  Step: 34ms | Tot: 3s637ms | Loss: 0.224 | Acc: 92.417% (12066/1305 102/391 =============================================>...................]  Step: 31ms | Tot: 9s8ms | Loss: 0.232 | Acc: 92.039% (32280/3507 274/391 ========================================================>........]  Step: 33ms | Tot: 11s192ms | Loss: 0.235 | Acc: 91.936% (39775/4326 338/391 \n",
            "Train Loss: 0.2368948190565914 Acc:  91.844 correct 45922 total 50000\n",
            " [================================================================>]  Step: 21ms | Tot: 2s251ms | Loss: 0.446 | Acc: 86.100% (8610/1000 100/100 \n",
            "Test Loss: 0.4464622813463211 Acc: 86.1 Correct 8610 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 88\n",
            " [================================================================>]  Step: 30ms | Tot: 12s969ms | Loss: 0.226 | Acc: 92.184% (46092/5000 391/391 =========================================>......................]  Step: 29ms | Tot: 8s671ms | Loss: 0.223 | Acc: 92.224% (30102/3264 255/391 \n",
            "Train Loss: 0.22606737736393423 Acc:  92.184 correct 46092 total 50000\n",
            " [================================================================>]  Step: 25ms | Tot: 2s438ms | Loss: 0.413 | Acc: 86.380% (8638/1000 100/100 \n",
            "Test Loss: 0.41308630123734474 Acc: 86.38 Correct 8638 total 10000\n",
            "87.04\n",
            "\n",
            "Epoch: 89\n",
            " [================================================================>]  Step: 30ms | Tot: 13s726ms | Loss: 0.230 | Acc: 92.076% (46038/5000 391/391 ===>.........................................................]  Step: 34ms | Tot: 1s763ms | Loss: 0.231 | Acc: 92.074% (5657/614 48/391 =======>.........................................................]  Step: 34ms | Tot: 1s797ms | Loss: 0.232 | Acc: 91.980% (5769/627 49/391 ===============>.................................................]  Step: 33ms | Tot: 3s283ms | Loss: 0.227 | Acc: 92.154% (10970/1190 93/391 =============================>...................................]  Step: 49ms | Tot: 6s415ms | Loss: 0.225 | Acc: 92.192% (21359/2316 181/391 ==================================>..............................]  Step: 32ms | Tot: 7s353ms | Loss: 0.225 | Acc: 92.225% (24672/2675 209/391 =======================================================>.........]  Step: 33ms | Tot: 11s861ms | Loss: 0.231 | Acc: 92.041% (39703/4313 337/391 \n",
            "Train Loss: 0.2295998699791596 Acc:  92.076 correct 46038 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s525ms | Loss: 0.381 | Acc: 87.320% (8732/1000 100/100 =====>.......................................................]  Step: 21ms | Tot: 614ms | Loss: 0.331 | Acc: 88.625% (1418/160 16/100 \n",
            "Test Loss: 0.38113092705607415 Acc: 87.32 Correct 8732 total 10000\n",
            "87.32\n",
            "\n",
            "Epoch: 90\n",
            " [================================================================>]  Step: 49ms | Tot: 14s146ms | Loss: 0.228 | Acc: 92.142% (46071/5000 391/391 \n",
            "Train Loss: 0.227507896911915 Acc:  92.142 correct 46071 total 50000\n",
            " [================================================================>]  Step: 16ms | Tot: 2s74ms | Loss: 0.457 | Acc: 85.230% (8523/1000 100/100 \n",
            "Test Loss: 0.4567037117481232 Acc: 85.23 Correct 8523 total 10000\n",
            "87.32\n",
            "\n",
            "Epoch: 91\n",
            " [================================================================>]  Step: 30ms | Tot: 13s629ms | Loss: 0.215 | Acc: 92.482% (46241/5000 391/391 ==========>...................................................]  Step: 34ms | Tot: 2s818ms | Loss: 0.224 | Acc: 92.461% (9468/1024 80/391 ==============================>..................................]  Step: 34ms | Tot: 6s367ms | Loss: 0.219 | Acc: 92.385% (21522/2329 182/391 ==============================>..................................]  Step: 33ms | Tot: 6s434ms | Loss: 0.219 | Acc: 92.404% (21763/2355 184/391 \n",
            "Train Loss: 0.2147932340726828 Acc:  92.482 correct 46241 total 50000\n",
            " [================================================================>]  Step: 20ms | Tot: 2s439ms | Loss: 0.449 | Acc: 85.440% (8544/1000 100/100 =========================================================>.....]  Step: 21ms | Tot: 2s227ms | Loss: 0.453 | Acc: 85.380% (7855/920 92/100 \n",
            "Test Loss: 0.4492775829136372 Acc: 85.44 Correct 8544 total 10000\n",
            "87.32\n",
            "\n",
            "Epoch: 92\n",
            " [================================================================>]  Step: 30ms | Tot: 14s598ms | Loss: 0.219 | Acc: 92.452% (46226/5000 391/391 ========================>.......................................]  Step: 34ms | Tot: 5s542ms | Loss: 0.209 | Acc: 92.918% (18197/1958 153/391 =========================>.......................................]  Step: 32ms | Tot: 5s575ms | Loss: 0.209 | Acc: 92.913% (18315/1971 154/391 \n",
            "Train Loss: 0.21933945476094172 Acc:  92.452 correct 46226 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s576ms | Loss: 0.409 | Acc: 86.940% (8694/1000 100/100 \n",
            "Test Loss: 0.40850581854581836 Acc: 86.94 Correct 8694 total 10000\n",
            "87.32\n",
            "\n",
            "Epoch: 93\n",
            " [================================================================>]  Step: 31ms | Tot: 14s333ms | Loss: 0.215 | Acc: 92.532% (46266/5000 391/391 =======>.....................................................]  Step: 47ms | Tot: 2s556ms | Loss: 0.206 | Acc: 92.812% (8316/896 70/391 \n",
            "Train Loss: 0.21528836711288413 Acc:  92.532 correct 46266 total 50000\n",
            " [================================================================>]  Step: 21ms | Tot: 2s976ms | Loss: 0.378 | Acc: 87.390% (8739/1000 100/100 =====================================>.........................]  Step: 28ms | Tot: 1s930ms | Loss: 0.390 | Acc: 87.210% (5407/620 62/100 ==============================================================>..]  Step: 22ms | Tot: 2s912ms | Loss: 0.376 | Acc: 87.412% (8479/970 97/100 \n",
            "Test Loss: 0.3775812771916389 Acc: 87.39 Correct 8739 total 10000\n",
            "87.39\n",
            "\n",
            "Epoch: 94\n",
            " [================================================================>]  Step: 31ms | Tot: 14s565ms | Loss: 0.209 | Acc: 92.780% (46390/5000 391/391 ================================================================>]  Step: 53ms | Tot: 14s488ms | Loss: 0.208 | Acc: 92.784% (46199/4979 389/391 \n",
            "Train Loss: 0.20880033556000352 Acc:  92.78 correct 46390 total 50000\n",
            " [================================================================>]  Step: 24ms | Tot: 2s342ms | Loss: 0.352 | Acc: 88.350% (8835/1000 100/100 ==================>..........................................]  Step: 17ms | Tot: 932ms | Loss: 0.359 | Acc: 88.286% (3090/350 35/100 \n",
            "Test Loss: 0.351695254445076 Acc: 88.35 Correct 8835 total 10000\n",
            "88.35\n",
            "\n",
            "Epoch: 95\n",
            " [================================================================>]  Step: 39ms | Tot: 14s799ms | Loss: 0.212 | Acc: 92.624% (46312/5000 391/391 =======>.....................................................]  Step: 32ms | Tot: 2s500ms | Loss: 0.204 | Acc: 92.771% (8431/908 71/391 \n",
            "Train Loss: 0.2119261453790433 Acc:  92.624 correct 46312 total 50000\n",
            " [================================================================>]  Step: 24ms | Tot: 2s408ms | Loss: 0.344 | Acc: 88.470% (8847/1000 100/100 \n",
            "Test Loss: 0.34351485341787336 Acc: 88.47 Correct 8847 total 10000\n",
            "88.47\n",
            "\n",
            "Epoch: 96\n",
            " [================================================================>]  Step: 29ms | Tot: 13s956ms | Loss: 0.208 | Acc: 92.900% (46450/5000 391/391 =========================>......................................]  Step: 33ms | Tot: 5s645ms | Loss: 0.200 | Acc: 93.191% (19324/2073 162/391 ====================================================>............]  Step: 43ms | Tot: 11s225ms | Loss: 0.206 | Acc: 92.961% (37482/4032 315/391 \n",
            "Train Loss: 0.20774197547941867 Acc:  92.9 correct 46450 total 50000\n",
            " [================================================================>]  Step: 21ms | Tot: 2s547ms | Loss: 0.336 | Acc: 88.550% (8855/1000 100/100 \n",
            "Test Loss: 0.335974495857954 Acc: 88.55 Correct 8855 total 10000\n",
            "88.55\n",
            "\n",
            "Epoch: 97\n",
            " [================================================================>]  Step: 30ms | Tot: 14s174ms | Loss: 0.203 | Acc: 92.968% (46484/5000 391/391 =>...........................................................]  Step: 40ms | Tot: 1s549ms | Loss: 0.185 | Acc: 93.527% (4190/448 35/391 ======>..........................................................]  Step: 31ms | Tot: 1s677ms | Loss: 0.189 | Acc: 93.510% (4668/499 39/391 \n",
            "Train Loss: 0.2031054052398028 Acc:  92.968 correct 46484 total 50000\n",
            " [================================================================>]  Step: 19ms | Tot: 2s692ms | Loss: 0.365 | Acc: 87.940% (8794/1000 100/100 ============================>..................................]  Step: 26ms | Tot: 1s508ms | Loss: 0.379 | Acc: 87.750% (4212/480 48/100 ==================================>..............................]  Step: 23ms | Tot: 1s657ms | Loss: 0.373 | Acc: 87.833% (4743/540 54/100 \n",
            "Test Loss: 0.3647636008262634 Acc: 87.94 Correct 8794 total 10000\n",
            "88.55\n",
            "\n",
            "Epoch: 98\n",
            " [================================================================>]  Step: 29ms | Tot: 13s753ms | Loss: 0.199 | Acc: 93.182% (46591/5000 391/391 .............................................................]  Step: 44ms | Tot: 1s246ms | Loss: 0.186 | Acc: 93.719% (2999/320 25/391 ========================>........................................]  Step: 34ms | Tot: 5s504ms | Loss: 0.191 | Acc: 93.568% (17486/1868 146/391 =========================================================>.......]  Step: 34ms | Tot: 12s255ms | Loss: 0.200 | Acc: 93.137% (41487/4454 348/391 \n",
            "Train Loss: 0.19898957076966 Acc:  93.182 correct 46591 total 50000\n",
            " [================================================================>]  Step: 15ms | Tot: 2s21ms | Loss: 0.336 | Acc: 89.110% (8911/1000 100/100 \n",
            "Test Loss: 0.336011653393507 Acc: 89.11 Correct 8911 total 10000\n",
            "89.11\n",
            "\n",
            "Epoch: 99\n",
            " [================================================================>]  Step: 24ms | Tot: 13s39ms | Loss: 0.205 | Acc: 92.990% (46495/5000 391/391  ================>...............................................]  Step: 36ms | Tot: 3s902ms | Loss: 0.201 | Acc: 93.013% (12739/1369 107/391 \n",
            "Train Loss: 0.20480392349269383 Acc:  92.99 correct 46495 total 50000\n",
            " [================================================================>]  Step: 18ms | Tot: 2s217ms | Loss: 0.336 | Acc: 88.650% (8865/1000 100/100 \n",
            "Test Loss: 0.33607946425676344 Acc: 88.65 Correct 8865 total 10000\n",
            "89.11\n"
          ]
        }
      ],
      "source": [
        "#Quantization aware traing of resnet18\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "print('==> Building quantized model..')\n",
        "\n",
        "quant_resnet=PreAct(model=net_quant)\n",
        "quant_resnet=quant_resnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(quant_resnet.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch,quant_resnet)\n",
        "    test(epoch,quant_resnet)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA A100-SXM4-80GB'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.get_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSrp987KJjbu"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "def count_parameters(model):\n",
        "  table=PrettyTable(['Modules','Parameters'])\n",
        "  total_params=0\n",
        "  for name,parameter in model.named_parameters():\n",
        "    if not parameter.requires_grad: continue\n",
        "    params=parameter.numel()\n",
        "    table.add_row([name,params])\n",
        "    total_params+=params\n",
        "  print(table)\n",
        "  print(f'Total Trainable Params:{total_params}')\n",
        "  return total_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dezv1gpVI3LC",
        "outputId": "3b608054-f3b7-4b73-c764-b368d9e6c325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+------------+\n",
            "|          Modules           | Parameters |\n",
            "+----------------------------+------------+\n",
            "|        conv1.weight        |    1728    |\n",
            "|         bn1.weight         |     64     |\n",
            "|          bn1.bias          |     64     |\n",
            "|         bn2.weight         |    512     |\n",
            "|          bn2.bias          |    512     |\n",
            "|    layer1.0.bn1.weight     |     64     |\n",
            "|     layer1.0.bn1.bias      |     64     |\n",
            "|   layer1.0.conv1.weight    |   36864    |\n",
            "|    layer1.0.bn2.weight     |     64     |\n",
            "|     layer1.0.bn2.bias      |     64     |\n",
            "|   layer1.0.conv2.weight    |   36864    |\n",
            "|    layer1.1.bn1.weight     |     64     |\n",
            "|     layer1.1.bn1.bias      |     64     |\n",
            "|   layer1.1.conv1.weight    |   36864    |\n",
            "|    layer1.1.bn2.weight     |     64     |\n",
            "|     layer1.1.bn2.bias      |     64     |\n",
            "|   layer1.1.conv2.weight    |   36864    |\n",
            "|    layer2.0.bn1.weight     |     64     |\n",
            "|     layer2.0.bn1.bias      |     64     |\n",
            "|   layer2.0.conv1.weight    |   73728    |\n",
            "|    layer2.0.bn2.weight     |    128     |\n",
            "|     layer2.0.bn2.bias      |    128     |\n",
            "|   layer2.0.conv2.weight    |   147456   |\n",
            "| layer2.0.shortcut.0.weight |    8192    |\n",
            "|    layer2.1.bn1.weight     |    128     |\n",
            "|     layer2.1.bn1.bias      |    128     |\n",
            "|   layer2.1.conv1.weight    |   147456   |\n",
            "|    layer2.1.bn2.weight     |    128     |\n",
            "|     layer2.1.bn2.bias      |    128     |\n",
            "|   layer2.1.conv2.weight    |   147456   |\n",
            "|    layer3.0.bn1.weight     |    128     |\n",
            "|     layer3.0.bn1.bias      |    128     |\n",
            "|   layer3.0.conv1.weight    |   294912   |\n",
            "|    layer3.0.bn2.weight     |    256     |\n",
            "|     layer3.0.bn2.bias      |    256     |\n",
            "|   layer3.0.conv2.weight    |   589824   |\n",
            "| layer3.0.shortcut.0.weight |   32768    |\n",
            "|    layer3.1.bn1.weight     |    256     |\n",
            "|     layer3.1.bn1.bias      |    256     |\n",
            "|   layer3.1.conv1.weight    |   589824   |\n",
            "|    layer3.1.bn2.weight     |    256     |\n",
            "|     layer3.1.bn2.bias      |    256     |\n",
            "|   layer3.1.conv2.weight    |   589824   |\n",
            "|    layer4.0.bn1.weight     |    256     |\n",
            "|     layer4.0.bn1.bias      |    256     |\n",
            "|   layer4.0.conv1.weight    |  1179648   |\n",
            "|    layer4.0.bn2.weight     |    512     |\n",
            "|     layer4.0.bn2.bias      |    512     |\n",
            "|   layer4.0.conv2.weight    |  2359296   |\n",
            "| layer4.0.shortcut.0.weight |   131072   |\n",
            "|    layer4.1.bn1.weight     |    512     |\n",
            "|     layer4.1.bn1.bias      |    512     |\n",
            "|   layer4.1.conv1.weight    |  2359296   |\n",
            "|    layer4.1.bn2.weight     |    512     |\n",
            "|     layer4.1.bn2.bias      |    512     |\n",
            "|   layer4.1.conv2.weight    |  2359296   |\n",
            "|       linear.weight        |    5120    |\n",
            "|        linear.bias         |     10     |\n",
            "+----------------------------+------------+\n",
            "Total Trainable Params:11172298\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "11172298"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_parameters(net_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKI51muiaC5j",
        "outputId": "3ad7fca1-6a23-41b4-8647-306c241d8068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install torchsummary "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pmEToMzaIdz",
        "outputId": "9d3922b8-1f97-4bdb-8aa2-bed18c132042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "ChannelPaddingSkip-2           [-1, 64, 32, 32]               0\n",
            "Hadamard_Transform-3           [-1, 64, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
            "       PreActBlock-9           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-10           [-1, 64, 32, 32]             128\n",
            "           Conv2d-11           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
            "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
            "      PreActBlock-14           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
            "           Conv2d-16          [-1, 128, 16, 16]           8,192\n",
            "Hadamard_Transform-17          [-1, 128, 16, 16]               0\n",
            "           Conv2d-18          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-19          [-1, 128, 16, 16]             256\n",
            "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
            "      PreActBlock-21          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
            "           Conv2d-23          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-24          [-1, 128, 16, 16]             256\n",
            "           Conv2d-25          [-1, 128, 16, 16]         147,456\n",
            "      PreActBlock-26          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-27          [-1, 128, 16, 16]             256\n",
            "           Conv2d-28            [-1, 256, 8, 8]          32,768\n",
            "Hadamard_Transform-29            [-1, 256, 8, 8]               0\n",
            "           Conv2d-30            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-31            [-1, 256, 8, 8]             512\n",
            "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
            "      PreActBlock-33            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "           Conv2d-35            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
            "           Conv2d-37            [-1, 256, 8, 8]         589,824\n",
            "      PreActBlock-38            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
            "           Conv2d-40            [-1, 512, 4, 4]         131,072\n",
            "Hadamard_Transform-41            [-1, 512, 4, 4]               0\n",
            "           Conv2d-42            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-43            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
            "      PreActBlock-45            [-1, 512, 4, 4]               0\n",
            "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-47            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-48            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-49            [-1, 512, 4, 4]       2,359,296\n",
            "      PreActBlock-50            [-1, 512, 4, 4]               0\n",
            "      BatchNorm2d-51            [-1, 512, 4, 4]           1,024\n",
            "           Linear-52                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,172,298\n",
            "Trainable params: 11,172,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 12.75\n",
            "Params size (MB): 42.62\n",
            "Estimated Total Size (MB): 55.38\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "# model=torchvision.models.net_quant.cuda()\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# net_quant=net_quant.to(device)\n",
        "summary(net_quant,input_size=(3,32,32),batch_size=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8WXrEDHaQXx",
        "outputId": "ad8d18a9-ff41-4c86-c261-f47f54d420f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATQvEs3_Pzdb",
        "outputId": "78eaa8ff-f9c9-447d-e6b9-6f66d7524eb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================================================================================================\n",
              "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
              "==========================================================================================================================================================================\n",
              "PreActResNet                                  [1, 3, 32, 32]            [1, 10]                   --                        --                        --\n",
              "Conv2d: 1-1                                 [1, 3, 32, 32]            [1, 64, 32, 32]           1,728                     [3, 3]                    1,769,472\n",
              "Sequential: 1-2                             [1, 3, 32, 32]            [1, 64, 32, 32]           --                        --                        --\n",
              "    ChannelPaddingSkip: 2-1                [1, 3, 32, 32]            [1, 64, 32, 32]           --                        --                        --\n",
              "    Hadamard_Transform: 2-2                [1, 64, 32, 32]           [1, 64, 32, 32]           (4,096)                   --                        --\n",
              "BatchNorm2d: 1-3                            [1, 64, 32, 32]           [1, 64, 32, 32]           128                       --                        128\n",
              "Sequential: 1-4                             [1, 64, 32, 32]           [1, 64, 32, 32]           --                        --                        --\n",
              "    PreActBlock: 2-3                       [1, 64, 32, 32]           [1, 64, 32, 32]           --                        --                        --\n",
              "        BatchNorm2d: 3-1                  [1, 64, 32, 32]           [1, 64, 32, 32]           128                       --                        128\n",
              "        Conv2d: 3-2                       [1, 64, 32, 32]           [1, 64, 32, 32]           36,864                    [3, 3]                    37,748,736\n",
              "        BatchNorm2d: 3-3                  [1, 64, 32, 32]           [1, 64, 32, 32]           128                       --                        128\n",
              "        Conv2d: 3-4                       [1, 64, 32, 32]           [1, 64, 32, 32]           36,864                    [3, 3]                    37,748,736\n",
              "    PreActBlock: 2-4                       [1, 64, 32, 32]           [1, 64, 32, 32]           --                        --                        --\n",
              "        BatchNorm2d: 3-5                  [1, 64, 32, 32]           [1, 64, 32, 32]           128                       --                        128\n",
              "        Conv2d: 3-6                       [1, 64, 32, 32]           [1, 64, 32, 32]           36,864                    [3, 3]                    37,748,736\n",
              "        BatchNorm2d: 3-7                  [1, 64, 32, 32]           [1, 64, 32, 32]           128                       --                        128\n",
              "        Conv2d: 3-8                       [1, 64, 32, 32]           [1, 64, 32, 32]           36,864                    [3, 3]                    37,748,736\n",
              "Sequential: 1-5                             [1, 64, 32, 32]           [1, 128, 16, 16]          --                        --                        --\n",
              "    PreActBlock: 2-5                       [1, 64, 32, 32]           [1, 128, 16, 16]          --                        --                        --\n",
              "        BatchNorm2d: 3-9                  [1, 64, 32, 32]           [1, 64, 32, 32]           128                       --                        128\n",
              "        Sequential: 3-10                  [1, 64, 32, 32]           [1, 128, 16, 16]          24,576                    --                        2,097,152\n",
              "        Conv2d: 3-11                      [1, 64, 32, 32]           [1, 128, 16, 16]          73,728                    [3, 3]                    18,874,368\n",
              "        BatchNorm2d: 3-12                 [1, 128, 16, 16]          [1, 128, 16, 16]          256                       --                        256\n",
              "        Conv2d: 3-13                      [1, 128, 16, 16]          [1, 128, 16, 16]          147,456                   [3, 3]                    37,748,736\n",
              "    PreActBlock: 2-6                       [1, 128, 16, 16]          [1, 128, 16, 16]          --                        --                        --\n",
              "        BatchNorm2d: 3-14                 [1, 128, 16, 16]          [1, 128, 16, 16]          256                       --                        256\n",
              "        Conv2d: 3-15                      [1, 128, 16, 16]          [1, 128, 16, 16]          147,456                   [3, 3]                    37,748,736\n",
              "        BatchNorm2d: 3-16                 [1, 128, 16, 16]          [1, 128, 16, 16]          256                       --                        256\n",
              "        Conv2d: 3-17                      [1, 128, 16, 16]          [1, 128, 16, 16]          147,456                   [3, 3]                    37,748,736\n",
              "Sequential: 1-6                             [1, 128, 16, 16]          [1, 256, 8, 8]            --                        --                        --\n",
              "    PreActBlock: 2-7                       [1, 128, 16, 16]          [1, 256, 8, 8]            --                        --                        --\n",
              "        BatchNorm2d: 3-18                 [1, 128, 16, 16]          [1, 128, 16, 16]          256                       --                        256\n",
              "        Sequential: 3-19                  [1, 128, 16, 16]          [1, 256, 8, 8]            98,304                    --                        2,097,152\n",
              "        Conv2d: 3-20                      [1, 128, 16, 16]          [1, 256, 8, 8]            294,912                   [3, 3]                    18,874,368\n",
              "        BatchNorm2d: 3-21                 [1, 256, 8, 8]            [1, 256, 8, 8]            512                       --                        512\n",
              "        Conv2d: 3-22                      [1, 256, 8, 8]            [1, 256, 8, 8]            589,824                   [3, 3]                    37,748,736\n",
              "    PreActBlock: 2-8                       [1, 256, 8, 8]            [1, 256, 8, 8]            --                        --                        --\n",
              "        BatchNorm2d: 3-23                 [1, 256, 8, 8]            [1, 256, 8, 8]            512                       --                        512\n",
              "        Conv2d: 3-24                      [1, 256, 8, 8]            [1, 256, 8, 8]            589,824                   [3, 3]                    37,748,736\n",
              "        BatchNorm2d: 3-25                 [1, 256, 8, 8]            [1, 256, 8, 8]            512                       --                        512\n",
              "        Conv2d: 3-26                      [1, 256, 8, 8]            [1, 256, 8, 8]            589,824                   [3, 3]                    37,748,736\n",
              "Sequential: 1-7                             [1, 256, 8, 8]            [1, 512, 4, 4]            --                        --                        --\n",
              "    PreActBlock: 2-9                       [1, 256, 8, 8]            [1, 512, 4, 4]            --                        --                        --\n",
              "        BatchNorm2d: 3-27                 [1, 256, 8, 8]            [1, 256, 8, 8]            512                       --                        512\n",
              "        Sequential: 3-28                  [1, 256, 8, 8]            [1, 512, 4, 4]            393,216                   --                        2,097,152\n",
              "        Conv2d: 3-29                      [1, 256, 8, 8]            [1, 512, 4, 4]            1,179,648                 [3, 3]                    18,874,368\n",
              "        BatchNorm2d: 3-30                 [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                     --                        1,024\n",
              "        Conv2d: 3-31                      [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                 [3, 3]                    37,748,736\n",
              "    PreActBlock: 2-10                      [1, 512, 4, 4]            [1, 512, 4, 4]            --                        --                        --\n",
              "        BatchNorm2d: 3-32                 [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                     --                        1,024\n",
              "        Conv2d: 3-33                      [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                 [3, 3]                    37,748,736\n",
              "        BatchNorm2d: 3-34                 [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                     --                        1,024\n",
              "        Conv2d: 3-35                      [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                 [3, 3]                    37,748,736\n",
              "BatchNorm2d: 1-8                            [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                     --                        1,024\n",
              "Linear: 1-9                                 [1, 512]                  [1, 10]                   5,130                     --                        5,130\n",
              "==========================================================================================================================================================================\n",
              "Total params: 11,520,458\n",
              "Trainable params: 11,172,298\n",
              "Non-trainable params: 348,160\n",
              "Total mult-adds (M): 555.43\n",
              "==========================================================================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 10.88\n",
              "Params size (MB): 46.08\n",
              "Estimated Total Size (MB): 56.97\n",
              "=========================================================================================================================================================================="
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchinfo\n",
        "# model=torchvision.models.resnet18().cuda()\n",
        "torchinfo.summary(net_quant,(3,32,32),batch_dim=0,col_names=('input_size','output_size','num_params','kernel_size','mult_adds'),verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr8ksWcXegn-"
      },
      "outputs": [],
      "source": [
        "num_eval_batches = 1000\n",
        "\n",
        "train_batch_size = 30\n",
        "eval_batch_size = 50\n",
        "\n",
        "saved_model_dir = '/content/drive/MyDrive/Extra/'\n",
        "scripted_float_model_file = 'quant_resnet18.pth'\n",
        "print(\"Size of baseline model\")\n",
        "print_size_of_model(quant_resnet)\n",
        "\n",
        "top1, top5 = evaluate(quant_resnet, criterion, testloader, neval_batches=num_eval_batches)\n",
        "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(quant_resnet), saved_model_dir + scripted_float_model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFLWft27u6Rz"
      },
      "source": [
        "# **Toy expeiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05-ATDpXG0SV"
      },
      "outputs": [],
      "source": [
        "# algorithm for learning mantissa and maximum range c\n",
        "def sgd(params, lr):\n",
        "    for param in params:  \n",
        "        param.data -= lr * param.grad.data\n",
        "        param.grad = None\n",
        "def mse_loss(predictions, targets):\n",
        "    return ((predictions - targets) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySvTCdfwwlON",
        "outputId": "acf23728-cdfa-4540-b133-eb77ae747521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 10: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 20: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 30: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 40: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 50: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 60: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 70: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 80: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 90: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 100: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 110: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 120: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 130: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 140: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 150: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 160: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 170: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 180: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 190: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 200: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 210: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 220: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 230: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 240: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 250: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 260: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 270: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 280: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 290: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 300: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 310: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 320: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 330: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 340: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 350: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 360: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 370: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 380: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 390: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 400: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 410: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 420: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 430: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 440: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 450: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 460: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 470: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 480: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n",
            "Epoch 490: loss=nan\n",
            "tensor([250.])\n",
            "tensor([nan])\n"
          ]
        }
      ],
      "source": [
        "# Input=torch.randn(10**5,1)\n",
        "Input=torch.zeros(10**5,1)\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.c = torch.nn.Parameter(torch.tensor([250.0]))\n",
        "        self.m = torch.nn.Parameter(torch.tensor([3.0]))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        sign=torch.sign(x)\n",
        "        e=7-self.m\n",
        "        # b= 2**(e-1) #bias\n",
        "        b=2**e-torch.log2(self.c)+torch.log2(2-2**(-self.m))-1 #bias\n",
        "        # self.c=(2-2**(-self.m))*2**(2**e-b-1)  #maximum representable range\n",
        "        p= torch.round(torch.log2(abs(x)))-self.m \n",
        "        p=torch.clamp(p,1-b-self.m,p.max())\n",
        "        s=2**p\n",
        "        output = torch.clamp(sign*s*torch.round(torch.abs(x)/s),-self.c,self.c)\n",
        "        return output\n",
        "\n",
        "        \n",
        "model = MyModel()\n",
        "loss_fn = mse_loss\n",
        "optimizer = lambda params, lr: sgd(params, lr=lr)\n",
        "\n",
        "lr = 0.1\n",
        "for epoch in range(500):\n",
        "    # Compute the predictions and loss\n",
        "    y_pred = model(Input)\n",
        "    loss = loss_fn(y_pred, Input)\n",
        "    \n",
        "    # Compute the gradients and update the model parameters\n",
        "    loss.backward()\n",
        "    optimizer(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Print the loss every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch %d: loss=%.4f' % (epoch, loss.item()))\n",
        "        # print(model.parameters())\n",
        "        for  param in model.parameters():\n",
        "          if param.requires_grad:\n",
        "             print( param.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS4u6YjYxmRK"
      },
      "source": [
        "# **Pertensor/Per channel quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cyFn_W16YHv"
      },
      "outputs": [],
      "source": [
        "     \n",
        "    def forward(input,m,e):\n",
        "        sign=torch.sign(input) #sign bit\n",
        "        print(sign)\n",
        "        b= 2**(e-1)     #bias\n",
        "        print(b)\n",
        "        c=(2-2**(-m))*2**(2**e-b-1)  #maximum representable range i.e. dynamic range\n",
        "        print(c)\n",
        "        p= torch.round(torch.log2(abs(input)))-m \n",
        "        print(p)\n",
        "        pmax=p.max()\n",
        "        print(torch.tensor(1-b-m),pmax)\n",
        "        if pmax<1-b-m:\n",
        "          pmax=1-b-m\n",
        "        p=torch.clamp(p,1-b-m,pmax)\n",
        "        # if p<1-b-m:\n",
        "        #   p=1-b-m\n",
        "        print(p)\n",
        "        s=2**p\n",
        "        print(s)\n",
        "        print(np.round(torch.abs(input)/s))\n",
        "        output = torch.clamp(sign*s*torch.round(torch.abs(input)/s),-c,c)\n",
        "        print(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kmPA22aYJxj",
        "outputId": "ba789583-ebb3-4134-abd1-e534274e5e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "4\n",
            "15.5\n",
            "tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])\n",
            "tensor(-7) tensor(-inf)\n",
            "tensor([-7., -7., -7., -7., -7., -7., -7., -7., -7., -7.])\n",
            "tensor([0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "        0.0078])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "x=torch.zeros(10)\n",
        "# x=torch.tensor(0)\n",
        "y=forward(x,4,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqPULHvaYVlO",
        "outputId": "01e5b0d1-d9e2-44bb-9d3a-075f9f01bac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFUWxnFVYgfW",
        "outputId": "1a947c2f-f9ca-4954-8170-c78c174b4b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1)\n"
          ]
        }
      ],
      "source": [
        "print(torch.sign(torch.tensor(5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPuUAnV7ZJPz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
