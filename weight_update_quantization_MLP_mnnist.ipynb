{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zseAMKmX5S3X"
      },
      "source": [
        "# Example on MNIST\n",
        "\n",
        "This example illustrates how ZerO works and avoids the training degeneracy (described by Thereom 1 in the paper).\n",
        "\n",
        "Link of the paper: https://arxiv.org/abs/2110.12661"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9s5aO0a5S3a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T10:38:03.332100Z",
          "iopub.status.busy": "2022-11-19T10:38:03.331834Z",
          "iopub.status.idle": "2022-11-19T10:38:04.725233Z",
          "shell.execute_reply": "2022-11-19T10:38:04.724455Z"
        },
        "id": "Jwj8mV-P5S3b"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.linalg import hadamard\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvhE8JTQ5S3c"
      },
      "source": [
        "### Model\n",
        "We consider a 4-layer multi-layer perceptron (MLP) where the hidden dimension is fixed. The models based on **random, partial identity, and ZerO** initialization are defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T10:38:04.728603Z",
          "iopub.status.busy": "2022-11-19T10:38:04.728183Z",
          "iopub.status.idle": "2022-11-19T10:38:04.735268Z",
          "shell.execute_reply": "2022-11-19T10:38:04.734660Z"
        },
        "id": "y9y560a-5S3d"
      },
      "outputs": [],
      "source": [
        "def ZerO_Init_on_matrix(matrix_tensor):\n",
        "    # Algorithm 1 in the paper.\n",
        "\n",
        "    m = matrix_tensor.size(0)\n",
        "    n = matrix_tensor.size(1)\n",
        "\n",
        "    if m <= n:\n",
        "        init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
        "    elif m > n:\n",
        "        clog_m = math.ceil(math.log2(m))\n",
        "        p = 2**(clog_m)\n",
        "        init_matrix = torch.nn.init.eye_(torch.empty(m, p)) @ (torch.tensor(hadamard(p)).float()/(2**(clog_m/2))) @ torch.nn.init.eye_(torch.empty(p, n))\n",
        "\n",
        "    return init_matrix\n",
        "\n",
        "def Identity_Init_on_matrix(matrix_tensor):\n",
        "    # Definition 1 in the paper\n",
        "    # See https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.eye_ for details. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible, the same as partial identity matrix.\n",
        "\n",
        "    m = matrix_tensor.size(0)\n",
        "    n = matrix_tensor.size(1)\n",
        "\n",
        "    init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
        "\n",
        "    return init_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T10:38:04.737789Z",
          "iopub.status.busy": "2022-11-19T10:38:04.737495Z",
          "iopub.status.idle": "2022-11-19T10:38:04.747357Z",
          "shell.execute_reply": "2022-11-19T10:38:04.746764Z"
        },
        "id": "idPAtf4k5S3d"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    '''\n",
        "    a standard model with 4 hidden layers\n",
        "    '''\n",
        "    def __init__(self, n_h=1024, init='ZerO'):\n",
        "        super(MLP, self).__init__()\n",
        "        self.init = init\n",
        "        self.n_h = n_h\n",
        "        self.l1 = nn.Linear(784, 784, bias=False)\n",
        "        self.l2 = nn.Linear(784, self.n_h, bias=False)\n",
        "        self.l3 = nn.Linear(self.n_h, self.n_h, bias=False)\n",
        "        self.l4 = nn.Linear(self.n_h, 10, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.l1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l4(x)\n",
        "\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "\n",
        "        if self.init == 'ZerO':\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = ZerO_Init_on_matrix(m.weight.data)\n",
        "\n",
        "        elif self.init == 'Partial_Identity':\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = Identity_Init_on_matrix(m.weight.data)\n",
        "\n",
        "        elif self.init == 'Random':\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP2gqhw55S3e"
      },
      "source": [
        "### Measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T10:38:04.750023Z",
          "iopub.status.busy": "2022-11-19T10:38:04.749728Z",
          "iopub.status.idle": "2022-11-19T10:38:04.753277Z",
          "shell.execute_reply": "2022-11-19T10:38:04.752707Z"
        },
        "id": "YTatHwpp5S3f"
      },
      "outputs": [],
      "source": [
        "def compute_rank(tensor):\n",
        "\n",
        "    tensor = tensor.detach().cpu()\n",
        "    rank = np.linalg.matrix_rank(tensor, tol=0.0001)\n",
        "\n",
        "    return rank\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **FP8 Quantizer Function**"
      ],
      "metadata": {
        "id": "HdgmL1UFVIhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_tensor=torch.randn(1024)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "temp_tensor=temp_tensor.to(device)\n",
        "print(temp_tensor)\n",
        "quant2=Quantizer(4,3)\n",
        "quant_temp_tensor=quant2(temp_tensor)\n",
        "print(quant_temp_tensor)"
      ],
      "metadata": {
        "id": "uTcsg6CoPCkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KWKE0eM5S3f"
      },
      "source": [
        "### Training Pipeline on MNIST\n",
        "\n",
        "from https://github.com/pytorch/examples/blob/main/mnist/main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T10:38:04.755972Z",
          "iopub.status.busy": "2022-11-19T10:38:04.755690Z",
          "iopub.status.idle": "2022-11-19T10:38:04.781129Z",
          "shell.execute_reply": "2022-11-19T10:38:04.780398Z"
        },
        "id": "uCOLWA085S3f"
      },
      "outputs": [],
      "source": [
        "class Optimizer(torch.optim.SGD):\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = p.grad.data\n",
        "                p.data.add_(d_p, alpha=-group['lr'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy({:.0f}%)'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(),\n",
        "                100. * correct / len(train_loader.dataset)))\n",
        "\n",
        "            # log metric\n",
        "            train_acc_list.append(100. * correct / len(train_loader.dataset))\n",
        "            train_loss_list.append(loss.item())\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    if 'l3' in name:\n",
        "                        if name not in rank_list_dict:\n",
        "                            rank_list_dict[name] = []\n",
        "\n",
        "                        # compute stable rank of the residual component\n",
        "                        print(param.data)\n",
        "                        rank_list_dict[name].append(compute_rank(param.data - torch.eye(param.data.size(0)).to(param.data.device)))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "def train_model(model, file_dir=None):\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='For Saving the current Model')\n",
        "\n",
        "    parser.add_argument('--name', type=str, default='test')\n",
        "\n",
        "    parser.add_argument('--init', type=str, default='ZerO')\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                       ])),\n",
        "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                       ])),\n",
        "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    model = model.to(device)\n",
        "    # debug\n",
        "    for name, params in model.named_parameters():\n",
        "        print(name, params.data)\n",
        "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # logging metric\n",
        "    train_acc_list = []\n",
        "    train_loss_list = []\n",
        "    rank_list_dict = {}\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=12, gamma=args.gamma)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), args.init + \"_mnist_cnn.pt\")\n",
        "\n",
        "    return rank_list_dict,model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUIPxsTZ5S3h"
      },
      "source": [
        "## Verification of Theorem 1 (Figure 3 in the paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSCDehT55S3h"
      },
      "source": [
        "### Figure 3 (left): identity initialization under different widths\n",
        "\n",
        "We show that the rank constraints (training degeneracy) happen no matter what the width is. The ranks are always smaller than the input dimension (784=28 * 28)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "FP8_quant_MLP = copy.deepcopy(MLP)"
      ],
      "metadata": {
        "id": "t0uFtDCdMtvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T10:38:04.784085Z",
          "iopub.status.busy": "2022-11-19T10:38:04.783771Z",
          "iopub.status.idle": "2022-11-19T11:29:48.772993Z",
          "shell.execute_reply": "2022-11-19T11:29:48.771995Z"
        },
        "id": "25fhvgje5S3i"
      },
      "outputs": [],
      "source": [
        "# n_h_256_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=256))\n",
        "\n",
        "# n_h_512_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=512))\n",
        "\n",
        "# n_h_1024_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
        "\n",
        "# n_h_2048_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=2048))\n",
        "\n",
        "# plotting\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
        "\n",
        "# x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
        "# x_axis = x_axis * 50\n",
        "\n",
        "# # generate a line of 784\n",
        "# input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
        "\n",
        "# ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
        "# ax.plot(x_axis, n_h_256_rank_list_dict['l3.weight'], label='n_h=256', linewidth='2')\n",
        "# ax.plot(x_axis, n_h_512_rank_list_dict['l3.weight'], label='n_h=512', linewidth='2')\n",
        "# ax.plot(x_axis, n_h_1024_rank_list_dict['l3.weight'], label='n_h=1024', linewidth='2')\n",
        "# ax.plot(x_axis, n_h_2048_rank_list_dict['l3.weight'], label='n_h=2048', linewidth='2')\n",
        "\n",
        "# ax.set_ylabel('Rank', fontsize=14)\n",
        "# ax.set_xlabel('Iterations', fontsize=14)\n",
        "# ax.legend(fontsize=12)\n",
        "\n",
        "# fig.tight_layout(w_pad=0.5)\n",
        "# plt.show()\n",
        "# fig.savefig('./figure_3_left.pdf', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yff5ACd5S3i"
      },
      "source": [
        "### Figure 3 (right): Hadamard transfrom breaks training degeneracy\n",
        "\n",
        "We show that when initializing dimension-increasing layer with Hadamard transform, the rank constraints (training degeneracy) not exsist any more. The rank can be greater than the input dimension during training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Quantizer(nn.Module):\n",
        "    def __init__(self, m, e,gamma):\n",
        "        super().__init__()\n",
        "        self.m = m # number of mantissa bit\n",
        "        self.e = e # number of exponent bit\n",
        "        self.gamma=gamma\n",
        "    def forward(self, input):\n",
        "        sign=torch.sign(input) #sign bit\n",
        "        b= 2**(self.e-1) #bias\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        b=b-torch.log2(torch.tensor(self.gamma))\n",
        "        b=b.to(device)\n",
        "        c=(2-2**(-self.m))*2**(2**self.e-b-1)  #maximum representable range i.e. dynamic range\n",
        "        # p= torch.floor(torch.log2(abs(input)))-self.m\n",
        "        temp=torch.floor(torch.log2(abs(input))+b)\n",
        "        temp_max=temp.max()\n",
        "        temp_max=temp_max.to(device)\n",
        "        temp_min=torch.tensor(1)\n",
        "        temp_min=temp_min.to(device)\n",
        "        if temp_max<temp_min:\n",
        "          temp_max=temp_min\n",
        "        temp=torch.clamp(temp,temp_min,temp_max)\n",
        "        # if torch.floor(torch.log2(abs(input))+b)>1:\n",
        "        #  p=torch.floor(torch.log2(abs(input))+b)-b-torch.round(self.m)\n",
        "        # else :\n",
        "        #  p=1-b-torch.round(self.m)\n",
        "        p=temp-b-torch.round(torch.tensor(self.m))\n",
        "        # p_min=torch.tensor(1-b-self.m)\n",
        "        # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # p_min=p_min.to(device)\n",
        "        # p_max=p.max()\n",
        "        # p_max=p_max.to(device)\n",
        "        # if p_max<p_min:\n",
        "          # p_max=p_min\n",
        "        # p=torch.clamp(p,p_min,p_max)\n",
        "        s=2**p\n",
        "        output = torch.clamp(sign*s*torch.round(abs(input)/s),-c,c)\n",
        "        # output=input\n",
        "\n",
        "        return output\n",
        "\n",
        "class QuantizedMLP(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(QuantizedMLP, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = Quantizer(4,3,1)\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        # self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model = model\n",
        "        # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # self.l1_init_weight=model.l1.weight.data\n",
        "        # self.l1_init_weight=self.l1_init_weight.to(device)\n",
        "        # self.l2_init_weight=model.l2.weight.data\n",
        "        # self.l2_init_weight=self.l2_init_weight.to(device)\n",
        "        # self.l3_init_weight=model.l3.weight.data\n",
        "        # self.l3_init_weight=self.l3_init_weight.to(device)\n",
        "        # self.l4_init_weight=model.l4.weight.data\n",
        "        # self.l4_init_weight=self.l4_init_weight.to(device)\n",
        "        # print(self.l1_init_weight)\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x=x.view(-1,28*28)\n",
        "        x=self.quant(x)\n",
        "        # W=self.model.l1.weight.data\n",
        "        # self.model.l1.weight.data=self.l1_init_weight+self.quant(W-self.l1_init_weight)\n",
        "        # self.model.l1.weight.data=self.quant(W)\n",
        "        x=self.model.l1(x)\n",
        "        # x=self.quant(x)\n",
        "        x=F.relu(x)\n",
        "        # x=self.quant(x)\n",
        "        # W=self.model.l2.weight.data\n",
        "        # self.model.l2.weight.data=self.l2_init_weight+self.quant(W-self.l2_init_weight)\n",
        "        # self.model.l2.weight.data=self.quant(W)\n",
        "        x=self.model.l2(x)\n",
        "        # x=self.quant(x)\n",
        "        x=F.relu(x)\n",
        "        # x=self.quant(x)\n",
        "        # W=self.model.l3.weight.data\n",
        "        # self.model.l3.weight.data=self.l3_init_weight+self.quant(W-self.l3_init_weight)\n",
        "        # self.model.l3.weight.data=self.quant(W)\n",
        "        x=self.model.l3(x)\n",
        "        # x=self.quant(x)\n",
        "        x=F.relu(x)\n",
        "        # x=self.quant(x)\n",
        "        # W=self.model.l4.weight.data\n",
        "        # self.model.l4.weight.data=self.l4_init_weight+self.quant(W-self.l4_init_weight)\n",
        "        # self.model.l4.weight.data=self.quant(W)\n",
        "        x=self.model.l4(x)\n",
        "        # x=self.quant(x)\n",
        "        # x = self.model(x)\n",
        "        # x = self.quant(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        # x = self.dequant(x)\n",
        "        # x=F.log_softmax(x)\n",
        "        # x=self.quant(x)\n",
        "        return F.log_softmax(x)\n",
        "        # return x"
      ],
      "metadata": {
        "id": "TICu5y0oUeL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-19T11:29:48.779294Z",
          "iopub.status.busy": "2022-11-19T11:29:48.778112Z",
          "iopub.status.idle": "2022-11-19T13:40:38.489364Z",
          "shell.execute_reply": "2022-11-19T13:40:38.488486Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BZAhl6zq5S3i",
        "outputId": "cf746b5f-bf58-44e1-b322-c0f8c4dda216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_rand_quant\n",
            "model.l1.weight tensor([[-2.7448e-02, -3.2748e-02,  2.3963e-02,  ..., -1.7463e-02,\n",
            "         -3.3231e-02, -2.4177e-02],\n",
            "        [-1.3858e-02, -5.1563e-02,  6.0303e-02,  ...,  7.6206e-02,\n",
            "         -1.6132e-02, -2.4891e-02],\n",
            "        [ 4.3044e-02,  7.7571e-02, -1.4022e-02,  ...,  4.1556e-02,\n",
            "         -6.9273e-02,  2.3381e-05],\n",
            "        ...,\n",
            "        [-3.1329e-02, -3.6133e-02, -1.6154e-02,  ..., -5.3324e-03,\n",
            "         -5.0084e-02,  3.6675e-02],\n",
            "        [ 4.1213e-02,  6.0945e-02,  1.3815e-02,  ...,  3.1495e-02,\n",
            "         -1.2127e-02, -3.3966e-02],\n",
            "        [ 3.3884e-02, -3.7613e-03, -1.1464e-01,  ..., -2.1665e-02,\n",
            "         -6.2421e-02, -6.3243e-02]], device='cuda:0')\n",
            "model.l2.weight tensor([[ 0.0425, -0.0548, -0.0692,  ..., -0.0220,  0.0378,  0.0619],\n",
            "        [-0.0586,  0.0157,  0.0197,  ..., -0.0205,  0.0455, -0.0078],\n",
            "        [ 0.0145, -0.0108,  0.0769,  ..., -0.0172,  0.0232,  0.0170],\n",
            "        ...,\n",
            "        [ 0.0584,  0.0537, -0.0550,  ...,  0.0871,  0.0258, -0.0399],\n",
            "        [ 0.0029, -0.0391,  0.0019,  ..., -0.0315,  0.0005,  0.0097],\n",
            "        [-0.0353, -0.0142, -0.0514,  ..., -0.1180,  0.0626,  0.1154]],\n",
            "       device='cuda:0')\n",
            "model.l3.weight tensor([[ 0.0025, -0.0036,  0.0317,  ...,  0.0064, -0.0008, -0.0027],\n",
            "        [-0.0027,  0.0277, -0.0129,  ..., -0.0101, -0.0130,  0.0028],\n",
            "        [-0.0350,  0.0511, -0.0196,  ...,  0.0621,  0.0331, -0.0299],\n",
            "        ...,\n",
            "        [ 0.0262, -0.0151, -0.0642,  ..., -0.0089, -0.0280,  0.0177],\n",
            "        [ 0.0229,  0.0326, -0.0656,  ..., -0.0003, -0.0188,  0.0080],\n",
            "        [ 0.0618, -0.0456,  0.0363,  ..., -0.0367,  0.0155, -0.0160]],\n",
            "       device='cuda:0')\n",
            "model.l4.weight tensor([[ 0.0330,  0.0029, -0.0299,  ..., -0.0188, -0.0518, -0.0177],\n",
            "        [ 0.0049,  0.0101,  0.0460,  ..., -0.0526, -0.0153,  0.0154],\n",
            "        [-0.0065, -0.0154, -0.0457,  ..., -0.0047,  0.0030,  0.0145],\n",
            "        ...,\n",
            "        [-0.0410,  0.0229, -0.0306,  ...,  0.0111,  0.0368,  0.0155],\n",
            "        [ 0.0343, -0.0423, -0.0147,  ...,  0.0012,  0.0279,  0.0151],\n",
            "        [-0.0048,  0.0226, -0.0441,  ..., -0.0299, -0.0081, -0.0022]],\n",
            "       device='cuda:0')\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.274615\tAccuracy(0%)\n",
            "tensor([[ 0.0025, -0.0036,  0.0317,  ...,  0.0064, -0.0008, -0.0027],\n",
            "        [-0.0027,  0.0277, -0.0129,  ..., -0.0101, -0.0130,  0.0028],\n",
            "        [-0.0350,  0.0511, -0.0196,  ...,  0.0621,  0.0331, -0.0299],\n",
            "        ...,\n",
            "        [ 0.0262, -0.0151, -0.0642,  ..., -0.0089, -0.0280,  0.0177],\n",
            "        [ 0.0229,  0.0326, -0.0656,  ..., -0.0003, -0.0188,  0.0080],\n",
            "        [ 0.0618, -0.0456,  0.0363,  ..., -0.0367,  0.0155, -0.0160]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-99ae8562d033>:101: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.680358\tAccuracy(3%)\n",
            "tensor([[ 0.0025, -0.0036,  0.0317,  ...,  0.0064, -0.0008, -0.0027],\n",
            "        [-0.0027,  0.0277, -0.0129,  ..., -0.0101, -0.0130,  0.0028],\n",
            "        [-0.0350,  0.0511, -0.0196,  ...,  0.0621,  0.0331, -0.0299],\n",
            "        ...,\n",
            "        [ 0.0262, -0.0151, -0.0642,  ..., -0.0089, -0.0280,  0.0177],\n",
            "        [ 0.0229,  0.0326, -0.0656,  ..., -0.0003, -0.0188,  0.0080],\n",
            "        [ 0.0618, -0.0456,  0.0363,  ..., -0.0367,  0.0155, -0.0160]],\n",
            "       device='cuda:0')\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: nan\tAccuracy(5%)\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c14c99697d1a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# random_init_rank_list_dict,model_rand = train_model(MLP(init='Random', n_h=2048))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_rand_quant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrandom_init_rank_list_dict_FP8_quant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_rand_quant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuantizedMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFP8_quant_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Random'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print('model_partial')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# partial_identity_init_rank_list_dict,model_partial = train_model(MLP(init='Partial_Identity', n_h=2048))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fbcec5588a6b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, file_dir)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_list_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fbcec5588a6b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0;31m# compute stable rank of the residual component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \u001b[0mrank_list_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0e0f349bddf4>\u001b[0m in \u001b[0;36mcompute_rank\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mmatrix_rank\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mmatrix_rank\u001b[0;34m(A, tol, hermitian)\u001b[0m\n\u001b[1;32m   1887\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_uv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1890\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m         \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->d'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_svd_nonconvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_lstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
          ]
        }
      ],
      "source": [
        "# print('model_rand')\n",
        "# random_init_rank_list_dict,model_rand = train_model(MLP(init='Random', n_h=2048))\n",
        "print('model_rand_quant')\n",
        "random_init_rank_list_dict_FP8_quant,model_rand_quant = train_model(QuantizedMLP(FP8_quant_MLP(init='Random', n_h=2048)))\n",
        "# print('model_partial')\n",
        "# partial_identity_init_rank_list_dict,model_partial = train_model(MLP(init='Partial_Identity', n_h=2048))\n",
        "# print('model_partial_quant')\n",
        "# partial_identity_init_rank_list_dict_FP8_quant,model_partial_FP8_quant = train_model(QuantizedMLP(FP8_quant_MLP(init='Partial_Identity', n_h=2048)))\n",
        "# print('model_ZerO')\n",
        "# ZerO_init_rank_list_dict,model_ZerO = train_model(MLP(init='ZerO', n_h=2048))\n",
        "print('model_ZerO_quant')\n",
        "ZerO_init_rank_list_dict_FP8_quant,model_ZerO_FP8_quant = train_model(QuantizedMLP(FP8_quant_MLP(init='ZerO', n_h=2048)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization of only activations\n",
        "applying quantization after relu\n",
        "random init = 93\n",
        "ZerO init = 92"
      ],
      "metadata": {
        "id": "5lqX7nmTZwDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xfig, ax = plt.subplots(1,1, figsize=(10,8), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
        "\n",
        "x_axis = np.arange(1, len(random_init_rank_list_dict['l3.weight'])+1)\n",
        "x_axis = x_axis * 50\n",
        "\n",
        "# generate a line of 784\n",
        "input_dim_line = np.ones(len(random_init_rank_list_dict['l3.weight'])) * 784\n",
        "\n",
        "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
        "ax.plot(x_axis, random_init_rank_list_dict['l3.weight'], label='Random Init', linewidth='2')\n",
        "ax.plot(x_axis, random_init_rank_list_dict_FP8_quant['model.l3.weight'], label='Random Init FP8 quant', linewidth='2')\n",
        "ax.plot(x_axis, partial_identity_init_rank_list_dict['l3.weight'], label='Partial Identity Init', linewidth='2')\n",
        "ax.plot(x_axis, partial_identity_init_rank_list_dict_FP8_quant['model.l3.weight'], label='Partial Identity Init FP8 quant', linewidth='2')\n",
        "ax.plot(x_axis, ZerO_init_rank_list_dict['l3.weight'], label='ZerO Init ', linewidth='2')\n",
        "ax.plot(x_axis, ZerO_init_rank_list_dict_FP8_quant['model.l3.weight'], label='ZerO Init FP8 quant', linewidth='2')\n",
        "\n",
        "\n",
        "ax.set_ylabel('Rank', fontsize=14)\n",
        "\n",
        "ax.set_xlabel('Iterations', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "\n",
        "fig.tight_layout(w_pad=0.5)\n",
        "plt.show()\n",
        "fig.savefig('./figure_3_right.pdf', bbox_inches='tight')\n"
      ],
      "metadata": {
        "id": "lmkt_wtJ6yVS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "outputId": "ecf5ca4b-662b-4507-842f-eda20a456960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-89d24e9dedc5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridspec_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'wspace'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hspace'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_init_rank_list_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l3.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random_init_rank_list_dict' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INT8 quantization**"
      ],
      "metadata": {
        "id": "0IFTUCGlAAIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quantization Function**"
      ],
      "metadata": {
        "id": "itcr-B9MAgT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
        "\n",
        "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
        "  # Calc Scale and zero point of next\n",
        "  qmin = 0.\n",
        "  qmax = 2.**num_bits - 1.\n",
        "\n",
        "  scale = (max_val - min_val) / (qmax - qmin)\n",
        "\n",
        "  initial_zero_point = qmin - min_val / scale\n",
        "\n",
        "  zero_point = 0\n",
        "  if initial_zero_point < qmin:\n",
        "      zero_point = qmin\n",
        "  elif initial_zero_point > qmax:\n",
        "      zero_point = qmax\n",
        "  else:\n",
        "      zero_point = initial_zero_point\n",
        "\n",
        "  zero_point = int(zero_point)\n",
        "\n",
        "  return scale, zero_point\n",
        "\n",
        "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
        "\n",
        "    if not min_val and not max_val:\n",
        "      min_val, max_val = x.min(), x.max()\n",
        "\n",
        "    qmin = 0.\n",
        "    qmax = 2.**num_bits - 1.\n",
        "\n",
        "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
        "    q_x = zero_point + x / scale\n",
        "    q_x.clamp_(qmin, qmax).round_()\n",
        "    q_x = q_x.round().byte()\n",
        "\n",
        "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
        "\n",
        "def dequantize_tensor(q_x):\n",
        "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)"
      ],
      "metadata": {
        "id": "csLMgc6ruJJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantizeLayer(x, layer, stat, scale_x, zp_x):\n",
        "  # for both conv and linear layers\n",
        "\n",
        "  # cache old values\n",
        "  W = layer.weight.data\n",
        "  # B = layer.bias.data\n",
        "\n",
        "  # quantise weights, activations are already quantised\n",
        "  w = quantize_tensor(layer.weight.data)\n",
        "  # b = quantize_tensor(layer.bias.data)\n",
        "\n",
        "  layer.weight.data = w.tensor.float()\n",
        "  # layer.bias.data = b.tensor.float()\n",
        "\n",
        "  # This is Quantisation Artihmetic\n",
        "  scale_w = w.scale\n",
        "  zp_w = w.zero_point\n",
        "  # scale_b = b.scale\n",
        "  # zp_b = b.zero_point\n",
        "\n",
        "  scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
        "\n",
        "  # Preparing input by shifting\n",
        "  X = x.float() - zp_x\n",
        "  layer.weight.data = scale_x * scale_w*(layer.weight.data - zp_w)\n",
        "  # layer.bias.data = scale_b*(layer.bias.data + zp_b)\n",
        "\n",
        "  # All int computation\n",
        "  x = (layer(X)/ scale_next) + zero_point_next\n",
        "\n",
        "  # Perform relu too\n",
        "  x = F.relu(x)\n",
        "\n",
        "  # Reset weights for next forward pass\n",
        "  layer.weight.data = W\n",
        "  # layer.bias.data = B\n",
        "\n",
        "  return x, scale_next, zero_point_next\n",
        "\n"
      ],
      "metadata": {
        "id": "1JJ5QMsiBbFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Min and max of x tensor, and stores it\n",
        "def updateStats(x, stats, key):\n",
        "  max_val, _ = torch.max(x, dim=1)\n",
        "  min_val, _ = torch.min(x, dim=1)\n",
        "\n",
        "\n",
        "  if key not in stats:\n",
        "    stats[key] = {\"max\": max_val.sum(), \"min\": min_val.sum(), \"total\": 1}\n",
        "  else:\n",
        "    stats[key]['max'] += max_val.sum().item()\n",
        "    stats[key]['min'] += min_val.sum().item()\n",
        "    stats[key]['total'] += 1\n",
        "\n",
        "  return stats\n",
        "\n",
        "# Reworked Forward Pass to access activation Stats through updateStats function\n",
        "def gatherActivationStats(model, x, stats):\n",
        "\n",
        "  x = x.view(-1, 28*28)\n",
        "\n",
        "  stats = updateStats(x, stats, 'l1')\n",
        "\n",
        "  x = F.relu(model.l1(x))\n",
        "\n",
        "  stats = updateStats(x, stats, 'l2')\n",
        "\n",
        "  x = F.relu(model.l2(x))\n",
        "\n",
        "  stats = updateStats(x, stats, 'l3')\n",
        "\n",
        "  x = F.relu(model.l3(x))\n",
        "\n",
        "  stats = updateStats(x, stats, 'l4')\n",
        "\n",
        "  x = model.l4(x)\n",
        "\n",
        "  return stats\n",
        "\n",
        "# Entry function to get stats of all functions.\n",
        "def gatherStats(model, test_loader):\n",
        "    device = 'cuda'\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    stats = {}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            stats = gatherActivationStats(model, data, stats)\n",
        "\n",
        "    final_stats = {}\n",
        "    for key, value in stats.items():\n",
        "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"] }\n",
        "    return final_stats"
      ],
      "metadata": {
        "id": "755fdsP-BrWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantForward(model, x, stats):\n",
        "\n",
        "  # Quantise before inputting into incoming layers\n",
        "  x = x.view(-1, 28*28)\n",
        "\n",
        "  x = quantize_tensor(x, min_val=stats['l1']['min'], max_val=stats['l1']['max'])\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.l1, stats['l2'], x.scale, x.zero_point)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.l2, stats['l3'], scale_next, zero_point_next)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.l3, stats['l4'], scale_next, zero_point_next)\n",
        "\n",
        "  # Back to dequant for final layer\n",
        "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
        "\n",
        "  x = model.l4(x)\n",
        "\n",
        "  return F.log_softmax(x,dim=1)"
      ],
      "metadata": {
        "id": "S-qhmioVD5-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testQuant(model, test_loader, quant=False, stats=None):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            if quant:\n",
        "              output = quantForward(model, data, stats)\n",
        "            else:\n",
        "              output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "-fT_KghnIQQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INT8_quant_MLP = copy.deepcopy(model_ZerO)"
      ],
      "metadata": {
        "id": "_iME0QC8IYwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "      datasets.MNIST('../data', train=True, download=True,\n",
        "                      transform=transforms.Compose([\n",
        "                          transforms.ToTensor(),\n",
        "                      ])),\n",
        "      batch_size=1000, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                       ])),\n",
        "        batch_size=1000, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "PaH21dyKIfMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testQuant(INT8_quant_MLP, test_loader, quant=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fswaMaGVIiV2",
        "outputId": "ee8ec60c-ed1e-4657-a9b5-65c49540cd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1ba25c7416bf>:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0990, Accuracy: 9700/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats = gatherStats(INT8_quant_MLP, test_loader)\n",
        "print(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqN6Q1fgIlOl",
        "outputId": "722560e9-c41e-41ea-aa60-96167f837481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'l1': {'max': tensor(999.5996, device='cuda:0'), 'min': tensor(0., device='cuda:0')}, 'l2': {'max': tensor(1607.0243, device='cuda:0'), 'min': tensor(0., device='cuda:0')}, 'l3': {'max': tensor(3174.3877, device='cuda:0'), 'min': tensor(0., device='cuda:0')}, 'l4': {'max': tensor(5651.7241, device='cuda:0'), 'min': tensor(0., device='cuda:0')}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testQuant(INT8_quant_MLP, test_loader, quant=True, stats=stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo3nNWnVIsj8",
        "outputId": "2c87ee5e-4dfb-4aaf-ddb6-36d40a80e58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 980/10000 (10%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Alternate method for INT8 quantization**"
      ],
      "metadata": {
        "id": "X1MwBzJzNoLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP2(nn.Module):\n",
        "    '''\n",
        "    a standard model with 4 hidden layers\n",
        "    '''\n",
        "    def __init__(self, n_h=1024, init='ZerO',q=False):\n",
        "        super(MLP, self).__init__()\n",
        "        self.init = init\n",
        "        self.n_h = n_h\n",
        "        self.l1 = nn.Linear(784, 784, bias=False)\n",
        "        self.l2 = nn.Linear(784, self.n_h, bias=False)\n",
        "        self.l3 = nn.Linear(self.n_h, self.n_h, bias=False)\n",
        "        self.l4 = nn.Linear(self.n_h, 10, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.q=q\n",
        "        if q:\n",
        "          self.qaunt=QuantStub()\n",
        "          self.dequant=DeQuantStub()\n",
        "    def forward(self, x):\n",
        "        if self.q:\n",
        "          x=self.quant(x)\n",
        "\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.l1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l4(x)\n",
        "\n",
        "        if self.q:\n",
        "          x=self.dequant(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "\n",
        "        if self.init == 'ZerO':\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = ZerO_Init_on_matrix(m.weight.data)\n",
        "\n",
        "        elif self.init == 'Partial_Identity':\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = Identity_Init_on_matrix(m.weight.data)\n",
        "\n",
        "        elif self.init == 'Random':\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "F-648tWlPMSW",
        "outputId": "a2de9f32-acfb-42d0-f6de-22b0c4a2b311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-259633715b25>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMLP2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     '''\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZerO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    \"\"\" Computes the top 1 accuracy \"\"\"\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
        "        return correct_one.mul_(100.0 / batch_size).item()\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    \"\"\" Prints the real size of the model \"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def load_model(quantized_model, model):\n",
        "    \"\"\" Loads in the weights into an object meant for quantization \"\"\"\n",
        "    state_dict = model.state_dict()\n",
        "    model = model.to('cpu')\n",
        "    quantized_model.load_state_dict(state_dict)\n",
        "\n",
        "def fuse_modules(model):\n",
        "    \"\"\" Fuse together convolutions/linear layers and ReLU \"\"\"\n",
        "    torch.quantization.fuse_modules(model, [['l1'],\n",
        "                                            ['l2'],\n",
        "                                            ['l3'],\n",
        "                                            ['l4']], inplace=True)"
      ],
      "metadata": {
        "id": "0K-XRA7NRNzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP2(q=False).cuda()\n",
        "print_size_of_model(net)"
      ],
      "metadata": {
        "id": "ZTSHzjOBMtTL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}